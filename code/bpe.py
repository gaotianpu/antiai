"""
bpe is short for Byte Pair Encoder. It translates arbitrary utf-8 strings into
sequences of integers, where each integer represents small chunks of commonly
occuring characters. This implementation is based on openai's gpt2 encoder.py:
https://github.com/openai/gpt-2/blob/master/src/encoder.py
but was mildly modified because the original implementation is a bit confusing.
I also tried to add as many comments as possible, my own understanding of what's
going on.

bpe æ˜¯å­—èŠ‚å¯¹ç¼–ç å™¨çš„ç¼©å†™ã€‚ å®ƒå°†ä»»æ„ utf-8 å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°åºåˆ—ï¼Œå…¶ä¸­æ¯ä¸ªæ•´æ•°ä»£è¡¨ä¸€å°å—ç»å¸¸å‡ºç°çš„å­—ç¬¦ã€‚ 
è¿™ä¸ªå®ç°æ˜¯åŸºäºopenaiçš„gpt2 encoder.py: https://github.com/openai/gpt-2/blob/master/src/encoder.py
ä½†æ˜¯ç¨å¾®ä¿®æ”¹äº†ä¸€ä¸‹ï¼Œå› ä¸ºåŸæ¥çš„å®ç°æœ‰ç‚¹æ··ä¹±ã€‚æˆ‘ä¹Ÿè¯•è¿‡ æ·»åŠ å°½å¯èƒ½å¤šçš„è¯„è®ºï¼Œæˆ‘è‡ªå·±å¯¹å‘ç”Ÿçš„äº‹æƒ…çš„ç†è§£ã€‚

openAIçš„rustå®ç°ï¼šhttps://github.com/openai/tiktoken

ä»£ç æ³¨é‡Šï¼š
https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py
"""

import os
import json
import regex as re
import requests

import torch

# -----------------------------------------------------------------------------

def bytes_to_unicode():
    """
    Every possible byte (really an integer 0..255) gets mapped by OpenAI to a unicode character that represents it visually. 
    Some bytes have their appearance preserved because they don't cause any trouble. 
    These are defined in list bs. For example:
    chr(33) returns "!", so in the returned dictionary we simply have d[33] -> "!".
    However, chr(0), for example, is '\x00', which looks ugly. So OpenAI maps these bytes, into new characters in a range where chr() returns a single nice character.
    So in the final dictionary we have d[0] -> 'Ä€' instead, which is just chr(0 + 2**8).
    In particular, the space character is 32, which we can see by ord(' '). 
    Instead, this function will shift space (32) by 256 to 288, so d[32] -> 'Ä '.
    So this is just a simple one-to-one mapping of bytes 0..255 into unicode characters that "look nice", either in their original form, or a funny shifted character like 'Ä€', or 'Ä ', etc.

    æ¯ä¸ªå¯èƒ½çš„å­—èŠ‚ï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ªæ•´æ•° 2çš„8æ¬¡æ–¹, 0..255ï¼‰éƒ½è¢« OpenAI æ˜ å°„åˆ°ä¸€ä¸ª unicode å­—ç¬¦ï¼Œä»¥ç›´è§‚åœ°è¡¨ç¤ºå®ƒã€‚
    æœ‰äº›å­—èŠ‚ä¿ç•™äº†å®ƒä»¬çš„å¤–è§‚ï¼Œå› ä¸ºå®ƒä»¬ä¸ä¼šé€ æˆä»»ä½•éº»çƒ¦ã€‚ è¿™äº›åœ¨åˆ—è¡¨ bs ä¸­å®šä¹‰ã€‚ ä¾‹å¦‚ï¼š chr(33) è¿”å›â€œ!â€ï¼Œæ‰€ä»¥åœ¨è¿”å›çš„å­—å…¸ä¸­æˆ‘ä»¬åªæœ‰ d[33] -> â€œ!â€ã€‚
    ä½†æ˜¯ï¼Œä¾‹å¦‚chr(0)æ˜¯'\x00'ï¼Œçœ‹èµ·æ¥å¾ˆéš¾çœ‹ã€‚ æ‰€ä»¥ OpenAI å°†è¿™äº›å­—èŠ‚æ˜ å°„åˆ°ä¸€ä¸ªèŒƒå›´å†…çš„æ–°å­—ç¬¦ï¼Œå…¶ä¸­ chr() è¿”å›ä¸€ä¸ªå¾ˆå¥½çš„å­—ç¬¦ã€‚
    æ‰€ä»¥åœ¨æœ€ç»ˆçš„å­—å…¸ä¸­ï¼Œæˆ‘ä»¬æœ‰ d[0] -> 'Ä€'ï¼Œå®ƒåªæ˜¯ chr(0 + 2**8)ã€‚
    ç‰¹åˆ«æ˜¯ç©ºæ ¼å­—ç¬¦æ˜¯32ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ord(' ')çœ‹åˆ°ã€‚
    ç›¸åï¼Œæ­¤å‡½æ•°ä¼šå°†ç©ºé—´ (32) ç§»åŠ¨ 256 åˆ° 288ï¼Œå› æ­¤ d[32] -> 'Ä 'ã€‚
    æ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªç®€å•çš„ä¸€å¯¹ä¸€æ˜ å°„ï¼Œå°†å­—èŠ‚ 0..255 æ˜ å°„åˆ°â€œçœ‹èµ·æ¥ä¸é”™â€çš„ unicode å­—ç¬¦ï¼Œæ— è®ºæ˜¯åŸå§‹å½¢å¼ï¼Œè¿˜æ˜¯æœ‰è¶£çš„ç§»ä½å­—ç¬¦ï¼Œå¦‚â€œÄ€â€æˆ–â€œÄ â€ç­‰ã€‚
    """
    # the 188 integers that render fine in their original form and need no shifting
    # 188 ä¸ªæ•´æ•°ä»¥å…¶åŸå§‹å½¢å¼å‘ˆç°è‰¯å¥½å¹¶ä¸”ä¸éœ€è¦ç§»ä½
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("Â¡"), ord("Â¬")+1))+list(range(ord("Â®"), ord("Ã¿")+1))
    cs = bs[:] #?ä¸ºå•¥è¦æ¥è¿™ä¹ˆä¸€ä¸‹,å¤åˆ¶ä¸€ä»½ï¼Ÿ
    # print(sorted(bs))
    # print(sorted(cs))
    # all integers b in bs will simply map to chr(b) in the output dict
    # now get the representations of the other 68 integers that do need shifting
    # each will get mapped chr(256 + n), where n will grow from 0...67 in the loop
    # bs ä¸­çš„æ‰€æœ‰æ•´æ•° b å°†ç®€å•åœ°æ˜ å°„åˆ°è¾“å‡ºå­—å…¸ä¸­çš„ chr(b) 
    # ç°åœ¨è·å–å…¶ä»– 68 ä¸ªéœ€è¦ç§»ä½çš„æ•´æ•°çš„è¡¨ç¤º 
    # æ¯ä¸ªéƒ½å°†å¾—åˆ°æ˜ å°„çš„ chr(256 + n)ï¼Œå…¶ä¸­ n å°†åœ¨å¾ªç¯ä¸­ä» 0...67 å¢é•¿
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            # if this byte is "ugly" then map it to the next available "nice" character
            # å¦‚æœè¿™ä¸ªå­—èŠ‚æ˜¯â€œä¸‘é™‹çš„â€ï¼Œåˆ™å°†å…¶æ˜ å°„åˆ°ä¸‹ä¸€ä¸ªå¯ç”¨çš„â€œæ¼‚äº®â€å­—ç¬¦
            cs.append(2**8+n)
            # print(n, 2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    # ord å­—ç¬¦->ASCIIæ•°å€¼, chr ASCIIæ•°å€¼->å­—ç¬¦
    # print(cs)
    d = dict(zip(bs, cs)) #key:idx, value:char(nice char)

    # tmp = {k: v for k, v in sorted(d.items(), key=lambda item: item[0])}
    # print(tmp,ord('a'),ord('b'),ord("A"))

    print(ord("ä¸­") , [v for v in "ä¸­".encode('utf-8')] ) #,tuple(ord("ä¸­")) #ä¸­æ–‡éœ€è¦è½¬asciiç ï¼Ÿ

    return d

def get_pairs(word):
    """
    Return all bigrams as a set of tuples, of consecutive elements in the iterable word.
    å°†æ‰€æœ‰äºŒå…ƒç»„ä½œä¸ºå¯è¿­ä»£å•è¯ä¸­è¿ç»­å…ƒç´ çš„ä¸€ç»„å…ƒç»„è¿”å›ã€‚
    """
    pairs = set() #ä¸èƒ½é‡å¤ï¼Ÿ
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs

class Encoder:

    def __init__(self, encoder, bpe_merges):
        # byte encoder/decoder
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}
        # bpe token encoder/decoder
        self.encoder = encoder
        self.decoder = {v:k for k,v in self.encoder.items()}
        # bpe merge list that defines the bpe "tree", of tuples (a,b) that are to merge to token ab
        # bpe åˆå¹¶åˆ—è¡¨ï¼Œå®šä¹‰è¦åˆå¹¶åˆ°æ ‡è®° ab çš„å…ƒç»„ (a,b) çš„ bpeâ€œæ ‘â€
        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        # the splitting pattern used for pre-tokenization
        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions <-- original openai comment
        # ç”¨äºé¢„æ ‡è®°åŒ–çš„æ‹†åˆ†æ¨¡å¼ åº”è¯¥æ·»åŠ  re.IGNORECASE ä»¥ä¾¿ BPE åˆå¹¶å¯ä»¥å‘ç”Ÿåœ¨å¤§å†™ç‰ˆæœ¬çš„æ”¶ç¼© <-- åŸå§‹ openai è¯„è®º
        """
        ok so what is this regex looking for, exactly?
        python re reference: https://docs.python.org/3/library/re.html
        - the vertical bars | is OR, so re.findall will chunkate text as the pieces match, from left to right
        - '\'s' would split up things like Andrej's -> (Andrej, 's)
        - ' ?\p{L}': optional space followed by 1+ unicode code points in the category "letter"
        - ' ?\p{N}': optional space followed by 1+ unicode code points in the category "number"
        - ' ?[^\s\p{L}\p{N}]+': optional space, then 1+ things that are NOT a whitespace, letter or number
        - '\s+(?!\S)': 1+ whitespace characters (e.g. space or tab or etc) UNLESS they are followed by non-whitespace
                       so this will consume whitespace characters in a sequence but exclude the last whitespace in
                       that sequence. that last whitespace has the opportunity to then match the optional ' ?' in
                       earlier patterns.
        - '\s+': 1+ whitespace characters, intended probably to catch a full trailing sequence of whitespaces at end of string
        So TLDR:
        - we are special casing a few common apostrophe constructs ('s, 't, 're, ...) and making those into separate tokens
        - we then separate out strings into consecutive chunks of 1) letters, 2) numbers, 3) non-letter-numbers, 4) whitespaces

        å¥½å§ï¼Œè¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼åˆ°åº•åœ¨å¯»æ‰¾ä»€ä¹ˆï¼Ÿ
         pythoné‡æ–°å‚è€ƒï¼šhttps://docs.python.org/3/library/re.html
         - ç«–çº¿ | æ˜¯ ORï¼Œå› æ­¤ re.findall å°†åœ¨ç‰‡æ®µåŒ¹é…æ—¶ä»å·¦åˆ°å³å¯¹æ–‡æœ¬è¿›è¡Œåˆ†å—
         - '\'s' ä¼šæ‹†åˆ† Andrej's -> (Andrej, 's)
         - ' ?\p{L}'ï¼šå¯é€‰ç©ºæ ¼åè·Ÿç±»åˆ«â€œå­—æ¯â€ä¸­çš„ 1+ ä¸ª unicode ä»£ç ç‚¹
         - ' ?\p{N}'ï¼šå¯é€‰ç©ºæ ¼åè·Ÿç±»åˆ«â€œæ•°å­—â€ä¸­çš„ 1+ unicode ä»£ç ç‚¹
         - ' ?[^\s\p{L}\p{N}]+'ï¼šå¯é€‰ç©ºæ ¼ï¼Œç„¶åæ˜¯ 1+ ä¸ªä¸æ˜¯ç©ºæ ¼ã€å­—æ¯æˆ–æ•°å­—çš„ä¸œè¥¿
         - '\s+(?!\S)'ï¼š1+ ä¸ªç©ºç™½å­—ç¬¦ï¼ˆä¾‹å¦‚ç©ºæ ¼æˆ–åˆ¶è¡¨ç¬¦ç­‰ï¼‰ï¼Œé™¤éå®ƒä»¬åè·Ÿéç©ºç™½å­—ç¬¦
                        æ‰€ä»¥è¿™å°†æ¶ˆè€—åºåˆ—ä¸­çš„ç©ºç™½å­—ç¬¦ä½†æ’é™¤æœ€åä¸€ä¸ªç©ºç™½å­—ç¬¦
                        é‚£ä¸ªåºåˆ—ã€‚ æœ€åä¸€ä¸ªç©ºæ ¼æœ‰æœºä¼šåŒ¹é…å¯é€‰çš„â€œï¼Ÿâ€ åœ¨
                        è¾ƒæ—©çš„æ¨¡å¼ã€‚
         - '\s+'ï¼š1+ ä¸ªç©ºç™½å­—ç¬¦ï¼Œå¯èƒ½ç”¨äºæ•è·å­—ç¬¦ä¸²æœ«å°¾çš„å®Œæ•´å°¾éšç©ºç™½åºåˆ—
         æ‰€ä»¥ TLDRï¼š
         - æˆ‘ä»¬å¯¹ä¸€äº›å¸¸è§çš„æ’‡å·ç»“æ„ï¼ˆ'sã€'tã€'reã€...ï¼‰è¿›è¡Œç‰¹æ®Šå°è£…ï¼Œå¹¶å°†å®ƒä»¬åˆ¶æˆå•ç‹¬çš„æ ‡è®°
         - ç„¶åæˆ‘ä»¬å°†å­—ç¬¦ä¸²åˆ†æˆè¿ç»­çš„å—ï¼ŒåŒ…æ‹¬ 1) å­—æ¯ã€2) æ•°å­—ã€3) éå­—æ¯æ•°å­—ã€4) ç©ºæ ¼

        pythonå®˜æ–¹reåº“ä¼šå‡ºç°ä¸‹é¢çš„é”™è¯¯ï¼Œéœ€è¦ç”¨åˆ°ç¬¬ä¸‰æ–¹regåº“ï¼š import regex as re
        re.error: bad escape \p at position 26
        
        """
        self.pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+""")
        self.cache = {}

    def bpe(self, token):
        """
        this function uses self.bpe_ranks to iteratively merge all the possible bpe tokens up the tree. 
        token is a string of one individual 'word' (after regex tokenization) and after byte encoding, e.g. 'Ä there'.
        æ­¤å‡½æ•°ä½¿ç”¨ self.bpe_ranks è¿­ä»£åœ°å°†æ‰€æœ‰å¯èƒ½çš„ bpe æ ‡è®°åˆå¹¶åˆ°æ ‘ä¸­ã€‚ 
        ä»¤ç‰Œæ˜¯ä¸€ä¸ªå•ç‹¬çš„â€œå•è¯â€ï¼ˆåœ¨æ­£åˆ™è¡¨è¾¾å¼æ ‡è®°åŒ–ä¹‹åï¼‰å’Œå­—èŠ‚ç¼–ç ä¹‹åçš„å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ 'Ä there'ã€‚
        """
        # token is a string of one individual 'word', after byte encoding, e.g. 'Ä there'
        # ä»¤ç‰Œæ˜¯ä¸€ä¸ªå•ç‹¬çš„â€œå•è¯â€çš„å­—ç¬¦ä¸²ï¼Œç»è¿‡å­—èŠ‚ç¼–ç ï¼Œä¾‹å¦‚ 'Ä there'

        # memoization, for efficiency è®°å¿†ï¼Œä¸ºäº†æ•ˆç‡, ç¼“å­˜ç²’åº¦ï¼Ÿ
        if token in self.cache:
            return self.cache[token]

        word = tuple(token) # individual characters that make up the token, in a tuple ç»„æˆä»¤ç‰Œçš„å•ä¸ªå­—ç¬¦ï¼Œåœ¨ä¸€ä¸ªå…ƒç»„ä¸­
        pairs = get_pairs(word) # get all bigrams å¾—åˆ°æ‰€æœ‰äºŒå…ƒç»„
        print("pairs",pairs)

        if not pairs:
            return token

        while True:

            # find the next lowest rank bigram that can be merged
            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break # no more bigrams are eligible to be merged
            first, second = bigram

            # we will now replace all occurences of (first, second) in the list of current
            # words into one merged token first_second, in the output list new_words
            # æˆ‘ä»¬ç°åœ¨å°†å½“å‰å•è¯åˆ—è¡¨ä¸­æ‰€æœ‰å‡ºç°çš„ (first, second) æ›¿æ¢ä¸ºè¾“å‡ºåˆ—è¡¨ new_words ä¸­çš„ä¸€ä¸ªåˆå¹¶æ ‡è®° first_second
            new_word = []
            i = 0
            while i < len(word):
                # find the next occurence of first in the sequence of current words
                # åœ¨å½“å‰å•è¯çš„åºåˆ—ä¸­æ‰¾åˆ°ä¸‹ä¸€æ¬¡å‡ºç°çš„ first
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break

                # if this occurence is also followed by second, then merge them into one
                # å¦‚æœæ­¤äº‹ä»¶ä¹‹åä¹Ÿæœ‰ç¬¬äºŒä¸ªï¼Œåˆ™å°†å®ƒä»¬åˆå¹¶ä¸ºä¸€ä¸ª
                if word[i] == first and i < len(word)-1 and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1

            # all occurences of (first, second) have been merged to first_second
            # æ‰€æœ‰å‡ºç°çš„ (first, second) éƒ½å·²åˆå¹¶åˆ° first second
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)

        # concat all words into a string, and use ' ' as the separator. 
        # Note that by now all characters have been byte encoded, 
        # guaranteeing that ' ' is not used in the actual data and is a 'special' delimiter character
        # å°†æ‰€æœ‰å•è¯è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶ä½¿ç”¨ ' ' ä½œä¸ºåˆ†éš”ç¬¦ã€‚ è¯·æ³¨æ„ï¼Œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæ‰€æœ‰å­—ç¬¦éƒ½å·²è¿›è¡Œå­—èŠ‚ç¼–ç ï¼Œ
        # ä¿è¯ ' ' æœªåœ¨å®é™…æ•°æ®ä¸­ä½¿ç”¨ï¼Œå¹¶ä¸”æ˜¯ä¸€ä¸ªâ€œç‰¹æ®Šâ€åˆ†éš”ç¬¦
        word = ' '.join(word)

        # cache the result and return
        self.cache[token] = word
        return word

    def encode(self, text):
        """ string goes in, list of integers comes out
        å­—ç¬¦ä¸²è¿›å…¥ï¼Œæ•´æ•°åˆ—è¡¨å‡ºæ¥ """
        bpe_idx = []
        # pre-tokenize the input text into string tokens (words, roughly speaking)
        # å°†è¾“å…¥æ–‡æœ¬é¢„æ ‡è®°ä¸ºå­—ç¬¦ä¸²æ ‡è®°ï¼ˆå•è¯ï¼Œç²—ç•¥åœ°è¯´ï¼‰
        tokens = re.findall(self.pat, text)
        
        # process each token into BPE integers å°†æ¯ä¸ªæ ‡è®°å¤„ç†æˆ BPE æ•´æ•°
        for token in tokens:
            # encode the token as a bytes (b'') object  å°†ä»¤ç‰Œç¼–ç ä¸ºå­—èŠ‚ (b'') å¯¹è±¡
            token_bytes = token.encode('utf-8') #
            # translate all bytes to their unicode string representation and flatten
            # å°†æ‰€æœ‰å­—èŠ‚è½¬æ¢ä¸ºå®ƒä»¬çš„ unicode å­—ç¬¦ä¸²è¡¨ç¤ºå¹¶å±•å¹³
            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)
            
            # perform all the applicable bpe merges according to self.bpe_ranks
            # æ ¹æ® self.bpe_ranks æ‰§è¡Œæ‰€æœ‰é€‚ç”¨çš„ bpe åˆå¹¶
            token_merged = self.bpe(token_translated).split(' ')
            # translate all bpe tokens to integers å°†æ‰€æœ‰ bpe æ ‡è®°è½¬æ¢ä¸ºæ•´æ•°
            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]
            # extend our running list of all output integers æ‰©å±•æˆ‘ä»¬æ‰€æœ‰è¾“å‡ºæ•´æ•°çš„è¿è¡Œåˆ—è¡¨
            bpe_idx.extend(token_ix)
        return bpe_idx

    def encode_and_show_work(self, text):
        """ debugging function, same as encode but returns all intermediate work 
        è°ƒè¯•åŠŸèƒ½ï¼Œä¸ç¼–ç ç›¸åŒï¼Œä½†è¿”å›æ‰€æœ‰ä¸­é—´å·¥ä½œ """
        bpe_idx = []
        parts = []
        tokens = re.findall(self.pat, text)
        print("tokens:",tokens)
        for token in tokens:
            token_bytes = token.encode('utf-8')
            # print("token_bytes:",token_bytes)
            token_translated = ''.join(self.byte_encoder[b] for b in token_bytes)
            
            token_merged = self.bpe(token_translated).split(' ')
            print(token,token_translated,token_merged)
            token_ix = [self.encoder[bpe_token] for bpe_token in token_merged]
            bpe_idx.extend(token_ix)
            parts.append({
                'token': token,
                'token_bytes': token_bytes,
                'token_translated': token_translated,
                'token_merged': token_merged,
                'token_ix': token_ix,
            })
        out = {
            'bpe_idx': bpe_idx, # the actual output sequence
            'tokens': tokens, # result of pre-tokenization
            'parts': parts, # intermediates for each token part
        }
        return out

    def decode(self, bpe_idx):
        """ list of integers comes in, string comes out
        æ•´æ•°åˆ—è¡¨è¿›æ¥ï¼Œå­—ç¬¦ä¸²å‡ºæ¥ """
        # inverse map the integers to get the tokens åå‘æ˜ å°„æ•´æ•°ä»¥è·å–æ ‡è®°
        tokens_merged = [self.decoder[token] for token in bpe_idx]
        # inverse the byte encoder, e.g. recovering 'Ä ' -> ' ', and get the bytes 
        # åè½¬å­—èŠ‚ç¼–ç å™¨ï¼Œä¾‹å¦‚ æ¢å¤ 'Ä ' -> ' 'ï¼Œå¹¶è·å–å­—èŠ‚
        tokens_flat = ''.join(tokens_merged)
        tokens_bytes = bytearray([self.byte_decoder[c] for c in tokens_flat])
        # recover the full utf-8 string æ¢å¤å®Œæ•´çš„ utf-8 å­—ç¬¦ä¸²
        text = tokens_bytes.decode('utf-8', errors='replace')
        return text

def get_file(local_file, remote_file):
    """ downloads remote_file to local_file if necessary """
    if not os.path.isfile(local_file):
        print(f"downloading {remote_file} to {local_file}")
        response = requests.get(remote_file)
        open(local_file, "wb").write(response.content)

def get_encoder():
    """
    Returns an instance of the GPT BPE Encoder/Decoder and handles caching of "database" files.
    """
    home_dir = os.path.expanduser('~')
    cache_dir = os.path.join(home_dir, '.cache', 'mingpt')
    os.makedirs(cache_dir, exist_ok=True)

    # load encoder.json that has the raw mappings from token -> bpe index
    encoder_local_file = os.path.join(cache_dir, 'encoder.json')
    encoder_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json'
    get_file(encoder_local_file, encoder_remote_file)
    with open(encoder_local_file, 'r') as f:
        encoder = json.load(f)
    # 256 individual byte tokens, 50,000 merged tokens, and 1 special <|endoftext|> token
    assert len(encoder) == 50257 

    # load vocab.bpe that contains the bpe merges, i.e. the bpe tree structure in the form tuples (a, b), that indicate that (a, b) is to be merged to one token ab
    # åŠ è½½åŒ…å« bpe åˆå¹¶çš„ vocab.bpeï¼Œå³å…ƒç»„ (a, b) å½¢å¼çš„ bpe æ ‘ç»“æ„ï¼Œè¡¨ç¤º (a, b) å°†åˆå¹¶ä¸ºä¸€ä¸ªæ ‡è®° ab
    vocab_local_file = os.path.join(cache_dir, 'vocab.bpe')
    vocab_remote_file = 'https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe'
    get_file(vocab_local_file, vocab_remote_file)
    with open(vocab_local_file, 'r', encoding="utf-8") as f:
        bpe_data = f.read()
    # light postprocessing: strip the version on first line and the last line is a blank
    # è½»åº¦åå¤„ç†ï¼šå»æ‰ç¬¬ä¸€è¡Œçš„ç‰ˆæœ¬ï¼Œæœ€åä¸€è¡Œæ˜¯ç©ºç™½
    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\n')[1:-1]]
    assert len(bpe_merges) == 50000 # 50,000 merged tokens

    # construct the Encoder object and return
    enc = Encoder(encoder, bpe_merges)
    return enc

# -----------------------------------------------------------------------------

class BPETokenizer:
    """ PyTorch-aware class that wraps the Encoder above 
    åŒ…è£…ä¸Šé¢ç¼–ç å™¨çš„ PyTorch æ„ŸçŸ¥ç±» """

    def __init__(self):
        self.encoder = get_encoder()

    def __call__(self, text, return_tensors='pt'):
        # PyTorch only; here because we want to match huggingface/transformers interface
        assert return_tensors == 'pt'
        # single string input for now, in the future potentially a list of strings
        assert isinstance(text, str)
        # encode and create a "batch dimension" of 1
        idx = [self.encoder.encode(text)]
        # wrap into PyTorch tensor
        out = torch.tensor(idx, dtype=torch.long)
        return out

    def decode(self, idx):
        # ensure a simple 1D tensor for now
        assert idx.ndim == 1
        # decode indices to text
        text = self.encoder.decode(idx.tolist())
        return text


# bpe paper codes begin
import collections

def get_stats(vocab):
    pairs = collections.defaultdict(int)
    for word, freq in vocab.items():
        symbols = word.split()
        for i in range(len(symbols)-1):
            pairs[symbols[i],symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out

def process():
    vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,
    'n e w e s t </w>':6, 'w i d e s t </w>':3}

    num_merges = 12
    for i in range(num_merges):
        pairs = get_stats(vocab)
        best = max(pairs, key=pairs.get)
        vocab = merge_vocab(best, vocab)
        print(best)
        print(vocab)
        print('~~~')

# bpe paper codes end

if __name__ == '__main__':
    # process()  ##bpe paper codes
    # bytes_to_unicode()
    # print(get_pairs("hell,world"))

    # here is an encoding example
    text = "Hello!! I'm Andrej Karpathy. It's 2022. w00t :D ğŸ¤— ä¸­å›½åŠ æ²¹ï¼"
    e = get_encoder()
    r = e.encode_and_show_work(text)
    # print(r)

    # print("Original text is:")
    # print(text)
    # print("First the text gets pre-tokenized, broken up into chunks, the outcome is:")
    # print(r['tokens'])
    # # ['Hello', '!!', ' I', "'m", ' Andrej', ' Karpathy', '.', ' It', "'s", ' 2022', '.', ' w', '00', 't', ' :', 'D', ' ğŸ¤—']
    # print("Then we iterate over each chunk and process them in turn...")
    # for part in r['parts']:
    #     print(part)
    # # {'token': 'Hello', 'token_bytes': b'Hello', 'token_translated': 'Hello', 'token_merged': ['Hello'], 'token_ix': [15496]}
    # # {'token': '!!', 'token_bytes': b'!!', 'token_translated': '!!', 'token_merged': ['!!'], 'token_ix': [3228]}
    # # {'token': ' I', 'token_bytes': b' I', 'token_translated': 'Ä I', 'token_merged': ['Ä I'], 'token_ix': [314]}
    # # {'token': "'m", 'token_bytes': b"'m", 'token_translated': "'m", 'token_merged': ["'m"], 'token_ix': [1101]}
    # # {'token': ' Andrej', 'token_bytes': b' Andrej', 'token_translated': 'Ä Andrej', 'token_merged': ['Ä Andre', 'j'], 'token_ix': [10948, 73]}
    # # {'token': ' Karpathy', 'token_bytes': b' Karpathy', 'token_translated': 'Ä Karpathy', 'token_merged': ['Ä K', 'arp', 'athy'], 'token_ix': [509, 5117, 10036]}
    # # {'token': '.', 'token_bytes': b'.', 'token_translated': '.', 'token_merged': ['.'], 'token_ix': [13]}
    # # {'token': ' It', 'token_bytes': b' It', 'token_translated': 'Ä It', 'token_merged': ['Ä It'], 'token_ix': [632]}
    # # {'token': "'s", 'token_bytes': b"'s", 'token_translated': "'s", 'token_merged': ["'s"], 'token_ix': [338]}
    # # {'token': ' 2022', 'token_bytes': b' 2022', 'token_translated': 'Ä 2022', 'token_merged': ['Ä 2022'], 'token_ix': [33160]}
    # # {'token': '.', 'token_bytes': b'.', 'token_translated': '.', 'token_merged': ['.'], 'token_ix': [13]}
    # # {'token': ' w', 'token_bytes': b' w', 'token_translated': 'Ä w', 'token_merged': ['Ä w'], 'token_ix': [266]}
    # # {'token': '00', 'token_bytes': b'00', 'token_translated': '00', 'token_merged': ['00'], 'token_ix': [405]}
    # # {'token': 't', 'token_bytes': b't', 'token_translated': 't', 'token_merged': ['t'], 'token_ix': [83]}
    # # {'token': ' :', 'token_bytes': b' :', 'token_translated': 'Ä :', 'token_merged': ['Ä :'], 'token_ix': [1058]}
    # # {'token': 'D', 'token_bytes': b'D', 'token_translated': 'D', 'token_merged': ['D'], 'token_ix': [35]}
    # # {'token': ' ğŸ¤—', 'token_bytes': b' \xf0\x9f\xa4\x97', 'token_translated': 'Ä Ã°ÅÂ¤Ä¹', 'token_merged': ['Ä Ã°Å', 'Â¤', 'Ä¹'], 'token_ix': [12520, 97, 245]}
    # # (refer to the code inside Encoder.encode for what these intermediates are)
    # print("and the final outcome is concatenating and flattening all the token_ix:")
    # print(r['bpe_idx'])
    # # [15496, 3228, 314, 1101, 10948, 73, 509, 5117, 10036, 13, 632, 338, 33160, 13, 266, 405, 83, 1058, 35, 12520, 97, 245]
    # # this would then become the integer input sequence to the transformer
    # print("ready to feed into a Transformer!")
