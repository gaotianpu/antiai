A GLOBAL BUDGET SCHEDULE
As mentioned in Section 3.3, we propose a global budget scheduler to gradually decrease the budget
b
(t)
following a cubic schedule. The detailed equation is given as follows:
b
(t) =



b
(0) 0 ≤ t < ti
b
(T) +
￾ b
(0) − b
(T)

 1 −
t−ti−tf
T −ti−tf

3
ti ≤ t < T − tf
b
(T) o.w.
. (12)
B GLUE DATASET STATISTICS
We present the dataset statistics of GLUE (Wang et al., 2019) in the following table.
Table 6: Summary of the GLUE benchmark.
Corpus Task #Train #Dev #Test #Label Metrics
Single-Sentence Classification (GLUE)
CoLA Acceptability 8.5k 1k 1k 2 Matthews corr
SST Sentiment 67k 872 1.8k 2 Accuracy
Pairwise Text Classification (GLUE)
MNLI NLI 393k 20k 20k 3 Accuracy
RTE NLI 2.5k 276 3k 2 Accuracy
QQP Paraphrase 364k 40k 391k 2 Accuracy/F1
MRPC Paraphrase 3.7k 408 1.7k 2 Accuracy/F1
QNLI QA/NLI 108k 5.7k 5.7k 2 Accuracy
Text Similarity (GLUE)
STS-B Similarity 7k 1.5k 1.4k 1 Pearson/Spearman corr
C NATURAL LANGUAGE UNDERSTANDING
C.1 BUDGET CONFIGURATION
For each budget level, we tune the final budget b
(T)
for AdaLoRA, the rank r for LoRA, the hidden
dimension d for two adapters to match the budget requirements.
Table 7: Detailed budget setup for GLUE benchmark.
# Params Houlsby Adapter (d) Pfeiffer Adapter (d) LoRA (r) AdaLoRA (b
(T )
)
1.2M 32 64 8 576
0.6M 16 32 4 288
0.3M 8 16 2 144
Alternatively, we can also set the final average rank r¯
(T) = b
(T)/n for AdaLoRA to control the
budget, which is set as 2, 4, and 8 given the final budget as 144, 288, and 576 respectively. Then we
select the initial rank r from {4, 6, 12} for the final average rank {2, 4, 8} respectively.
C.2 TRAINING DETAILS
We tune the learning rate from {8 × 10−5
, 5 × 10−5
, 3 × 10−5
, 1 × 10−4
, 3 × 10−4
, 5 × 10−4
, 8 ×
10−4
, 1 × 10−3} and pick the best learning rate for every method. For each dataset, the batch size is
set as identical for every method.
14
Published as a conference paper at ICLR 2023
Table 8: Hyper-parameter setup of AdaLoRA for GLUE benchmark.
Dataset learning rate batch size # epochs γ ti ∆T tf
MNLI 5 × 10−4
32 7 0.1 8000 100 50000
RTE 1.2 × 10−3
32 50 0.3 600 1 1800
QNLI 1.2 × 10−3
32 5 0.1 2000 100 8000
MRPC 1 × 10−3
32 30 0.1 600 1 1800
QQP 5 × 10−4
32 5 0.1 8000 100 25000
SST-2 8 × 10−4
32 24 0.1 6000 100 22000
CoLA 5 × 10−4
32 25 0.5 800 10 3500
STS-B 2.2 × 10−3
32 25 0.1 800 10 2000
D QUESTION ANSWERING
D.1 BUDGET CONFIGURATION
Given the budget, we control the trainable parameters for each method as the following table.
Table 9: Detailed budget setup for question answering.
# Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA
d d r b(T )
/r¯
(T )
/r
0.65% 32 64 8 576 / 8 / 12
0.32% 16 32 4 288 / 4 / 6
0.16% 8 16 2 144 / 2 / 4
0.08% 4 8 1 72 / 1 / 2
D.2 TRAINING DETAILS
We set the batch size as 16. We select the learning rate from {8 × 10−5
, 5 × 10−5
, 3 × 10−5
, 1 ×
10−4
, 3 × 10−4
, 5 × 10−4
, 8 × 10−4
, 1 × 10−3} and pick the best-performing learning rate for every
method. The configuration of AdaLoRA is listed in the following table.
Table 10: Hyper-parameter setup of AdaLoRA for question answering tasks.
Dataset learning rate batch size # epochs γ ti ∆T tf
SQuADv1.1 1 × 10−3
16 10 0.1 5000 100 25000
SQuADv2.0 1 × 10−3
16 12 0.1 5000 100 50000
D.3 DATASET
The statistics of question answering datasets are summarized in Table 11.
Table 11: Statistics of the SQuAD dataset.
# Train # Validation
SQuAD v1.1 87,599 10,570
SQuAD v2.0 130,319 11,873
E NATURAL LANGUAGE GENERATION
E.1 BUDGET CONFIGURATION
Given the budget, we control the trainable parameters for each method as the following table.
15
Published as a conference paper at ICLR 2023
Table 12: Detailed budget setup for summarization tasks.
# Params Houlsby Adapter Pfeiffer Adapter LoRA AdaLoRA
d d r b(T )
/r¯
(T )
/r
0.65% 32 64 8 576 / 8 / 12
0.32% 16 32 4 288 / 4 / 6
0.16% 8 16 2 144 / 2 / 4
0.08% 4 8 1 72 / 1 / 2
E.2 TRAINING DETAILS
We set the batch size as 16. We select the learning rate from {8 × 10−5
, 5 × 10−5
, 3 × 10−5
, 1 ×
10−4
, 3 × 10−4
, 5 × 10−4
, 8 × 10−4
, 1 × 10−3} and pick the best-performing learning rate for every
method. The configuration of AdaLoRA is listed in the following table.
Table 13: Hyper-parameter setup of AdaLoRA for summarization tasks.
Dataset learning rate batch size # epochs γ ti ∆T tf
XSum 5 × 10−4
64 25 0.1 6000 100 50000
CNN/DailyMail 5 × 10−4
32 15 0.1 5000 100 85000
F ABLATION STUDY FOR LORA
As mentioned in Section 4, we find that the performance of LoRA can be further improved when
applying it to every weight matrix, compared to fine-tuning Wq and Wv only (Hu et al., 2022). This
observation aligns with the empirical results of He et al. (2022). In Table 14, we follow the same
training configuration as Section 4.1 and present an ablation study to illustrate this point.
Table 14: We compare the fine-tuning performance when apply LoRA to every weight matrix or
Wq, Wv only. The parameter budget is fixed as 0.3M. We report accuracy for QQP and MRPC,
accuracy(m) for MNLI, and average correlation for STS-B.
MNLI QQP CoLA RTE QNLI SST-2 MRPC STS-B
LoRA (Wq, Wk) 89.80 90.48 67.04 83.75 93.69 94.84 90.20 91.05
LoRA (all) 90.30 91.61 68.71 85.56 94.31 94.95 90.44 91.68
G ORTHOGONAL REGULARIZATION
To verify the effectiveness of (4), we plot k P
> P − Ik
2
F
and k QQ> − Ik
2
F
to show whether P and Q
are regularized to be orthogonal. We fine-tune a DeBERTaV3-base model on SST-2 with AdaLoRA
and follow the same training configuration as Section 4.1. We set γ as 0.1 and plot the two terms
along the training horizon. From Figure 4, we can see that two regularization terms can be optimized
to a very small value (e.g., 0.001) at the beginning of training. Therefore, both P and Q can be
enforced to be orthogonal quickly during the initial warm-up of AdaLoRA. It ensures that the triplets
are not dependent with each other.
H COMPARISON OF TRAINING COST
We compare the training cost between AdaLoRA and LoRA in the following table. We use two
methods to fine-tune DeBERTaV3-base on a single NVIDIA V100 GPU. We do training only and set
hyperparameters, e.g., batch size and training epochs, the same as in Section 4.
Table 15 shows that MARVEL incurs 11% additional training time on MNLI and 16% on SQuADv2
under different budgets. The memory footprint of two methods are quite close. Such results
demonstrate that MARVEL does not incur significant training overheads. The reason behind is that
16
Published as a conference paper at ICLR 2023
0 20000 40000
Iterations
10−4
10−3
10−2
10−1
100
(a) P of Wo at the first layer.
0 20000 40000
Iterations
10−4
10−3
10−2
10−1
100
(b) Q of Wo at the first layer.
0 20000 40000
Iterations
10−4
10−3
10−2
10−1
100
(c) P of Wf2
at the first layer.
0 20000 40000
Iterations
10−4
10−3
10−2
10−1
(d) Q of Wf2
at the first layer
Figure 4: We plot the k P
> P − Ik
2
F
and k QQ> − Ik
2
F when fine-tuning DeBERTaV3-base on SST-2.
Table 15: Comparison of practical training cost between AdaLoRA and LoRA.
Dataset # Param Method GPU Mem Time/epoch
MNLI
0.08% LoRA 11.094 GB 105 min
AdaLoRA 11.104 GB 116 min
0.16% LoRA 11.098 GB 105 min
AdaLoRA 11.110 GB 117 min
0.65% LoRA 11.128 GB 105 min
AdaLoRA 11.188 GB 117 min
SST-2
0.08% LoRA 13.138 GB 60 min
AdaLoRA 13.148 GB 71 min
0.16% LoRA 13.142 GB 61 min
AdaLoRA 13.164 GB 71 min
0.65% LoRA 13.170 GB 61 min
AdaLoRA 13.226 GB 71 min
we only evaluate the importance score for small incremental matrices PΛQ. Their total number of
parameters is usually less than 1% of pre-trained weights. Therefore, it does not lead to significant
computational cost to update the importance scores of these well-structured small matrices, compared
to forward-backward pass of full model.
17
> ||P
P
−
I||2F
> ||QQ −
I||2F
> ||P
P
−
I||2F
> ||QQ −
I||2F