* Agrawal, H., Anderson, P., Desai, K., Wang, Y., Chen, X.,
Jain, R., Johnson, M., Batra, D., Parikh, D., and Lee, S.
nocaps: novel object captioning at scale. In ICCV, pp.
8947–8956, 2019.
* Anaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A.,
Kour, G., Shlomov, S., Tepper, N., and Zwerdling, N. Do
not have enough data? deep learning to the rescue! In
AAAI, pp. 7383–7390, 2020.
* Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D.,
Zitnick, C. L., and Parikh, D. VQA: visual question
answering. In ICCV, pp. 2425–2433, 2015.
* Bain, M., Nagrani, A., Varol, G., and Zisserman, A. Frozen
in time: A joint video and image encoder for end-to-end
retrieval. In ICCV, 2021.
* Bertasius, G., Wang, H., and Torresani, L. Is space-time
attention all you need for video understanding? In ICML,
2021.
* Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Con￾ceptual 12M: Pushing web-scale image-text pre-training
to recognize long-tail visual concepts. In CVPR, 2021.
* Chen, Y., Li, L., Yu, L., Kholy, A. E., Ahmed, F., Gan, Z.,
Cheng, Y., and Liu, J. UNITER: universal image-text
representation learning. In ECCV, volume 12375, pp. 104–120, 2020.
* Cho, J., Lei, J., Tan, H., and Bansal, M. Unifying vision￾and-language tasks via text generation. arXiv preprint
arXiv:2102.02779, 2021.
* Das, A., Kottur, S., Gupta, K., Singh, A., Yadav, D., Moura,
J. M. F., Parikh, D., and Batra, D. Visual dialog. In CVPR,
pp. 1080–1089, 2017.
* Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan￾guage understanding. In Burstein, J., Doran, C., and
Solorio, T. (eds.), NAACL, pp. 4171–4186, 2019.
* Do, V., Camburu, O.-M., Akata, Z., and Lukasiewicz, T. e￾snli-ve: Corrected visual-textual entailment with natural
language explanations. arXiv preprint arXiv:2004.03744,
2020.
* Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image
recognition at scale. In ICLR, 2021.
Fan, C., Zhang, X., Zhang, S., Wang, W., Zhang, C., and
Huang, H. Heterogeneous memory enhanced multimodal
attention model for video question answering. In CVPR,
pp. 1999–2007, 2019.
* Gan, Z., Cheng, Y., Kholy, A. E., Li, L., Liu, J., and Gao, J. Multi-step reasoning via recurrent dual attention for vi￾sual dialog. In Korhonen, A., Traum, D. R., and M`arquez,
L. (eds.), ACL, pp. 6463–6474, 2019.
* Gan, Z., Chen, Y., Li, L., Zhu, C., Cheng, Y., and Liu, J.  Large-scale adversarial training for vision-and-language
representation learning. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H. (eds.), NeurIPS,
2020.
* Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and
Parikh, D. Making the V in VQA matter: Elevating the
role of image understanding in visual question answering. In CVPR, pp. 6325–6334, 2017.
* Hinton, G., Vinyals, O., and Dean, J. Distilling
the knowledge in a neural network. arXiv preprint
arXiv:1503.02531, 2015.
* Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y. The curious case of neural text degeneration. In ICLR,
2020.
* Hu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and
Wang, L. Scaling up vision-language pre-training for
image captioning, 2021.
* Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham,
H., Le, Q. V., Sung, Y., Li, Z., and Duerig, T. Scaling up
visual and vision-language representation learning with
noisy text supervision. arXiv preprint arXiv:2102.05918,
2021.
* Karpathy, A. and Li, F. Deep visual-semantic alignments for
generating image descriptions. In CVPR, pp. 3128–3137,
2015.
* Kim, J., Jun, J., and Zhang, B. Bilinear attention networks. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman,
K., Cesa-Bianchi, N., and Garnett, R. (eds.), NIPS, pp. 1571–1581, 2018.
* Kim, W., Son, B., and Kim, I. Vilt: Vision-and-language
transformer without convolution or region supervision. arXiv preprint arXiv:2102.03334, 2021.
* Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
Kravitz, J., Chen, S., Kalantidis, Y., Li, L., Shamma,
D. A., Bernstein, M. S., and Fei-Fei, L. Visual genome:
Connecting language and vision using crowdsourced
dense image annotations. IJCV, 123(1):32–73, 2017.
* Kumar, V., Choudhary, A., and Cho, E. Data augmentation
using pre-trained transformer models. arXiv preprint
arXiv:2003.02245, 2020.
* Le, T. M., Le, V., Venkatesh, S., and Tran, T. Hierarchical
conditional relation networks for video question answer￾ing. In CVPR, pp. 9972–9981, 2020.
* Lei, J., Li, L., Zhou, L., Gan, Z., Berg, T. L., Bansal, M.,
and Liu, J. Less is more: Clipbert for video-and-language
learning via sparse sampling. In CVPR, pp. 7331–7341,
2021.
* Li, J., Selvaraju, R. R., Gotmare, A. D., Joty, S., Xiong,
C., and Hoi, S. Align before fuse: Vision and language
representation learning with momentum distillation. In
NeurIPS, 2021a.
* Li, W., Gao, C., Niu, G., Xiao, X., Liu, H., Liu, J., Wu,
H., and Wang, H. UNIMO: towards unified-modal un￾derstanding and generation via cross-modal contrastive
learning. In Zong, C., Xia, F., Li, W., and Navigli, R.(eds.), ACL, pp. 2592–2607, 2021b.
* Li, X., Yin, X., Li, C., Zhang, P., Hu, X., Zhang, L., Wang,
L., Hu, H., Dong, L., Wei, F., Choi, Y., and Gao, J. Oscar:
Object-semantics aligned pre-training for vision-language
tasks. In ECCV, pp. 121–137, 2020.
* Lin, T., Maire, M., Belongie, S. J., Hays, J., Perona, P.,
Ramanan, D., Doll´ar, P., and Zitnick, C. L. Microsoft
COCO: common objects in context. In Fleet, D. J., Pajdla,
T., Schiele, B., and Tuytelaars, T. (eds.), ECCV, volume
8693, pp. 740–755, 2014.
* Loshchilov, I. and Hutter, F. Decoupled weight decay regu￾larization. arXiv preprint arXiv:1711.05101, 2017.
* Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pretraining
task-agnostic visiolinguistic representations for vision￾and-language tasks. In Wallach, H. M., Larochelle, H.,
Beygelzimer, A., d’Alch´e-Buc, F., Fox, E. B., and Garnett,
R. (eds.), NeurIPS, pp. 13–23, 2019.
* Miech, A., Alayrac, J.-B., Smaira, L., Laptev, I., Sivic, J.,
and Zisserman, A. End-to-end learning of visual repre￾sentations from uncurated instructional videos. In CVPR,
pp. 9879–9889, 2020.
* Murahari, V., Batra, D., Parikh, D., and Das, A. Large-scale
pretraining for visual dialog: A simple state-of-the-art
baseline. In Vedaldi, A., Bischof, H., Brox, T., and Frahm,
J. (eds.), ECCV, pp. 336–352, 2020.
* Ordonez, V., Kulkarni, G., and Berg, T. L. Im2text: Describ￾ing images using 1 million captioned photographs. In
Shawe-Taylor, J., Zemel, R. S., Bartlett, P. L., Pereira, F. C. N., and Weinberger, K. Q. (eds.), NIPS, pp. 1143–1151,
2011.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,
L., et al. Pytorch: An imperative style, high-performance
deep learning library. NeurIPS, 32:8026–8037, 2019.
* Patrick, M., Huang, P.-Y., Asano, Y., Metze, F., Hauptmann,
A. G., Henriques, J. F., and Vedaldi, A. Support-set
bottlenecks for video-text representation learning. In
ICLR, 2021.
* Plummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,
Hockenmaier, J., and Lazebnik, S. Flickr30k entities:
Collecting region-to-phrase correspondences for richer
image-to-sentence models. In ICCV, pp. 2641–2649,
2015.
* Puri, R., Spring, R., Shoeybi, M., Patwary, M., and Catan￾zaro, B. Training question answering models from syn￾thetic data. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), EMNLP, pp. 5811–5826, 2020.
* Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,
Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,
et al. Learning transferable visual models from natural
language supervision. arXiv preprint arXiv:2103.00020,
2021.
* Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk,
R., Mullis, C., Katta, A., Coombes, T., Jitsev, J., and
Komatsuzaki, A. Laion-400m: Open dataset of clip-
filtered 400 million image-text pairs. arXiv preprint
arXiv:2111.02114, 2021.
* Sharma, P., Ding, N., Goodman, S., and Soricut, R. Con￾ceptual captions: A cleaned, hypernymed, image alt-text
dataset for automatic image captioning. In Gurevych, I. and Miyao, Y. (eds.), ACL, pp. 2556–2565, 2018.
* Shorten, C. and Khoshgoftaar, T. M. A survey on image
data augmentation for deep learning. J. Big Data, 6:60,
2019.
* Suhr, A., Zhou, S., Zhang, A., Zhang, I., Bai, H., and
Artzi, Y. A corpus for reasoning about natural language
grounded in photographs. In Korhonen, A., Traum, D. R.,
and M`arquez, L. (eds.), ACL, pp. 6418–6428, 2019.
* Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles,
A., and J´egou, H. Training data-efficient image trans￾formers & distillation through attention. arXiv preprint
arXiv:2012.12877, 2020.
* Wang, Y., Joty, S. R., Lyu, M. R., King, I., Xiong, C., and
Hoi, S. C. H. VD-BERT: A unified vision and dialog
transformer with BERT. In Webber, B., Cohn, T., He, Y.,
and Liu, Y. (eds.), EMNLP, pp. 3325–3338, 2020.
* Wang, Z., Yu, J., Yu, A. W., Dai, Z., Tsvetkov, Y., and Cao,
Y. Simvlm: Simple visual language model pretraining
with weak supervision. arXiv preprint arXiv:2108.10904,
2021.
* Xie, Q., Luong, M., Hovy, E. H., and Le, Q. V. Self-training
with noisy student improves imagenet classification. In
CVPR, pp. 10684–10695, 2020.
* Xu, H., Ghosh, G., Huang, P.-Y., Okhonko, D., Aghajanyan,
A., Metze, F., Zettlemoyer, L., and Feichtenhofer, C.
* Videoclip: Contrastive pre-training for zero-shot video￾text understanding. In EMNLP, pp. 6787–6800, 2021.
* Yang, A., Miech, A., Sivic, J., Laptev, I., and Schmid, C. Just ask: Learning to answer questions from millions of
narrated videos. In ICCV, pp. 1686–1697, 2021.
* Yang, Y., Malaviya, C., Fernandez, J., Swayamdipta, S.,
Bras, R. L., Wang, J., Bhagavatula, C., Choi, Y., and
Downey, D. G-daug: Generative data augmentation for
commonsense reasoning. In Cohn, T., He, Y., and Liu, Y. (eds.), EMNLP Findings, pp. 1008–1025, 2020.
* Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L.,
Choi, Y., and Gao, J. Vinvl: Making visual representa￾tions matter in vision-language models. arXiv preprint
arXiv:2101.00529, 2021.
* Zhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J. J., and
Gao, J. Unified vision-language pre-training for image
captioning and VQA. In AAAI, pp. 13041–13049, 2020.
* Zhu, L. and Yang, Y. Actbert: Learning global-local video￾text representations. In CVPR, pp. 8746–8755, 2020. 
