
* Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic Dimensionality Explains theEffectiveness of Language Model Fine-Tuning. arXiv:2012.13255 [cs], December 2020. URL http://arxiv.org/abs/2012.13255.
* Zeyuan Allen-Zhu and Yuanzhi Li. What Can ResNet Learn Efficiently, Going Beyond Kernels? InNeurIPS, 2019. Full version available at http://arxiv.org/abs/1905.10337.
* Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deeplearning. arXiv preprint arXiv:2001.04413, 2020a.
* Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robustdeep learning. arXiv preprint arXiv:2005.10190, 2020b.
* Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over￾parameterization. In ICML, 2019. Full version available at http://arxiv.org/abs/1811.03962.
* Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
* Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari￾wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.
* Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. arXiv:2005.14165[cs], July 2020. URL http://arxiv.org/abs/2005.14165.
* Jian-Feng Cai, Emmanuel J Cand`es, and Zuowei Shen. A singular value thresholding algorithm formatrix completion. SIAM Journal on optimization, 20(4):1956–1982, 2010.
* Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings ofthe 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017. doi: 10.18653/v1/s17-2001. URL http://dx.doi.org/10.18653/v1/S17-2001.
* Ronan Collobert and Jason Weston. A unified architecture for natural language processing: deepneural networks with multitask learning. In Proceedings of the 25th international conferenceon Machine learning, ICML ’08, pp. 160–167, New York, NY, USA, July 2008. Associationfor Computing Machinery. ISBN 978-1-60558-205-4. doi: 10.1145/1390156.1390177. URLhttps://doi.org/10.1145/1390156.1390177.
* Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando de Freitas. Predictingparameters in deep learning, 2014.
* Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deepbidirectional transformers for language understanding, 2019a.
* Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of DeepBidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May 2019b. URL http://arxiv.org/abs/1810.04805. arXiv: 1810.04805.
* William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URLhttps://aclanthology.org/I05-5002.
* Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The webnlgchallenge: Generating text from rdf data. In Proceedings of the 10th International Conference onNatural Language Generation, pp. 124–133, 2017.Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neuralnetworks outperform kernel methods? arXiv preprint arXiv:2006.13409, 2020.
* Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human￾annotated dialogue dataset for abstractive summarization. CoRR, abs/1911.12237, 2019. URLhttp://arxiv.org/abs/1911.12237.
* Lars Grasedyck, Daniel Kressner, and Christine Tobler. A literature survey of low-rank tensorapproximation techniques. GAMM-Mitteilungen, 36(1):53–78, 2013.
* Jihun Ham and Daniel D. Lee. Grassmann discriminant analysis: a unifying view on subspace-basedlearning. In ICML, pp. 376–383, 2008. URL https://doi.org/10.1145/1390156.1390204.
* Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level AdversarialReProgramming. arXiv:2101.00121 [cs], December 2020. URL http://arxiv.org/abs/2101.00121. arXiv: 2101.00121.
* Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bertwith disentangled attention, 2021.
* Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe,Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learningfor NLP. arXiv:1902.00751 [cs, stat], June 2019. URL http://arxiv.org/abs/1902.00751.
* Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networkswith low rank expansions. arXiv preprint arXiv:1405.3866, 2014.
* Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicol`o Fusi. Initialization and regularizationof factorized neural layers, 2021.
* Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
* Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditionalcomputation and automatic sharding, 2020.
* Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient PromptTuning. arXiv:2104.08691 [cs], April 2021. URL http://arxiv.org/abs/2104.08691. arXiv: 2104.08691.
* Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the Intrinsic Di￾mension of Objective Landscapes. arXiv:1804.08838 [cs, stat], April 2018a. URL http://arxiv.org/abs/1804.08838. arXiv: 1804.08838.
* Xiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190 [cs], January 2021. URL http://arxiv.org/abs/2101.00190.
* Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradientdescent on structured data. In Advances in Neural Information Processing Systems, 2018.
* Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank ap￾proximation via alternating minimization. In International Conference on Machine Learning, pp.2358–2367. PMLR, 2016.
* Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterizedmatrix sensing and neural networks with quadratic activations. In Conference On Learning The￾ory, pp. 2–47. PMLR, 2018b.
* Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language modelvia parameter-efficient transfer learning. In Findings of the Association for Computational Lin￾guistics: EMNLP 2020, pp. 441–459, Online, November 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.findings-emnlp.41. URL https://aclanthology.org/2020.findings-emnlp.41.
* Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPTUnderstands, Too. arXiv:2103.10385 [cs], March 2021. URL http://arxiv.org/abs/2103.10385. arXiv: 2103.10385.
* Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, MikeLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretrainingapproach, 2019.
* Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprintarXiv:1711.05101, 2017.
* Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
* Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rankhypercomplex adapter layers, 2021.
* Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structureddata record to text generation. arXiv preprint arXiv:2007.02871, 2020.
* Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end￾to-end generation. arXiv preprint arXiv:1706.09254, 2017.
* Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran￾tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprintarXiv:1906.05392, 2019.
* Jonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter￾fusion: Non-destructive task composition for transfer learning, 2021.
* Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and San￾jeev Khudanpur. Semi-orthogonal low-rank matrix factorization for deep neural networks. InInterspeech, pp. 3743–3747, 2018.
* Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Under￾standing by Generative Pre-Training. pp. 12, a.
* Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. LanguageModels are Unsupervised Multitask Learners. pp. 24, b.
* Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questionsfor squad. CoRR, abs/1806.03822, 2018. URL http://arxiv.org/abs/1806.03822.
* Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains withresidual adapters. arXiv:1705.08045 [cs, stat], November 2017. URL http://arxiv.org/abs/1705.08045. arXiv: 1705.08045.
* Andreas R¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, andIryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers, 2020.
* Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhadran. Low￾rank matrix factorization for deep neural network training with high-dimensional output targets.
* In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 6655–6659. IEEE, 2013.
* Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and BryanCatanzaro. Megatron-lm: Training multi-billion parameter language models using model par￾allelism, 2020.
* Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng,and Christopher Potts. Recursive deep models for semantic compositionality over a sentimenttreebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing, pp. 1631–1642, Seattle, Washington, USA, October 2013. Association for Computa￾tional Linguistics. URL https://aclanthology.org/D13-1170.
* Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pp. 6000–6010, 2017.
* Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
* Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019.
* Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, OmerLevy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose languageunderstanding systems, 2020.
* Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471, 2018.
* Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 1112–1122, New Orleans, Louisiana, June 2018. Associationfor Computational Linguistics. doi: 10.18653/v1/N18-1101. URL https://www.aclweb.org/anthology/N18-1101.
* Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrickvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-artnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing: System Demonstrations, pp. 38–45, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6.
* Greg Yang and Edward J. Hu. Feature Learning in Infinite-Width Neural Networks. arXiv:2011.14522 [cond-mat], May 2021. URL http://arxiv.org/abs/2011.14522. arXiv: 2011.14522.
* Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuningfor transformer-based masked language-models, 2021.
* Yu Zhang, Ekapol Chuangsuwanich, and James Glass. Extracting deep neural network bottleneckfeatures using low-rank matrix factorization. In 2014 IEEE international conference on acoustics,speech and signal processing (ICASSP), pp. 185–189. IEEE, 2014.
* Yong Zhao, Jinyu Li, and Yifan Gong. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),pp. 5005–5009. IEEE, 2016.
* Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries fromnatural language using reinforcement learning. CoRR, abs/1709.00103, 2017. URL http://arxiv.org/abs/1709.00103.