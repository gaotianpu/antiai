# Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes 
一步一步蒸馏！使用较少的训练数据和较小的模型大小，性能优于较大的语言模型 2023.5.3  https://arxiv.org/abs/2305.02301

## Abstract
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLMgenerated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller model sizes. Third, we reduce both the model size and the amount of data required to outperform LLMs; our 770M T5 model outperforms the 540B PaLM model using only 80% of available data on a benchmark task.

部署大语言模型(LLM)具有挑战性，因为它们对于实际应用程序来说内存不足且计算密集。作为应对，研究人员通过使用人类标签进行微调或使用LLM生成的标签进行蒸馏来训练较小的任务特定模型。然而，微调和蒸馏需要大量的训练数据才能实现与LLM相当的性能。我们引入了逐步蒸馏，这是一种新的机制，它(a)训练的较小模型优于LLM，以及(b)通过利用微调或蒸馏所需的较少训练数据来实现这一点。我们的方法蒸馏LLM的推理，作为在多任务训练框架内对小型模型的额外监督。我们在4个NLP基准上提出了三个发现：
1. 与微调和蒸馏相比，我们的机制通过更少的标注/未标注训练样本实现了更好的性能。
2. 与LLM相比，我们使用更小的模型尺寸实现了更好的性能。
3. 我们减少了模型大小和超越LLM所需的数据量; 我们的770M T5模型在基准任务中仅使用80%的可用数据，其性能优于540B PaLM模型。

## 1 Introduction
Despite the impressive few-shot ability offered by large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Thoppilan et al., 2022; Hoffmann et al., 2022; Smith et al., 2022b; Zhang et al., 2022), these models are challenging to deploy in real world applications due to their sheer size. Serving a single 175 billion LLM requires at least 350GB GPU memory using specialized infrastructure (Zheng et al., 2022). To make matters worse, today’s state-of-the-art LLMs are composed of over 500B parameters (Chowdhery et al., 2022), requiring significantly more memory and compute. Such computational requirements are far beyond affordable for most product teams, especially for applications that require low latency performance.

尽管大语言模型(LLM)提供了令人印象深刻的少样本能力(Brow et al., 2020; howdher et al., 2022; hoppila et al., 2022; offman et al., 2022，Smit et al., 2022b; han et al., 2022)，但由于其庞大的规模，这些模型在现实世界应用中的部署具有挑战性。为单个1750亿LLM提供服务需要使用专用基础设施的至少350GB GPU内存(Zheng et al.，2022)。更糟糕的是，今天最先进的LLM由超过500B的参数组成(Chowdhery et al.，2022)，需要更多的内存和计算。对于大多数产品团队来说，这种计算需求远远超出了他们的承受能力，尤其是对于需要低延迟性能的应用程序来说。

Figure 1: While large language models (LLMs) offer strong zero/few-shot performance, they are challenging to serve in practice. Traditional ways of training small task-specific models, on the other hand, requires large amount of training data. We propose Distilling step-by-step, a new paradigm that extracts rationales from LLMs as informative task knowledge into training small models, which reduces both the deployed model size as well as the data required for training.
图1：虽然大语言模型(LLM)提供了强大的零/少样本性能，但它们在实践中很有挑战性。另一方面，训练小的任务特定模型的传统方法需要大量的训练数据。我们提出了逐步蒸馏，这是一种新的范式，从LLM中蒸馏推理作为信息性任务知识，用于训练小模型，这既减少了部署的模型大小，也减少了训练所需的数据。

To circumvent these deployment challenges of large models, practitioners often choose to deploy smaller specialized models instead. These smaller models are trained using one of two common paradigms: finetuning or distillation. Finetuning updates a pretrained smaller model (e.g. BERT (Devlin et al., 2018) or T5 (Raffel et al., 2020)) using downstream human annotated data (Howard and Ruder, 2018). Distillation trains the same smaller models with labels generated by a larger LLM (Tang et al., 2019; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022). Unfortunately, these paradigms reduce model size at a cost: to achieve comparable performance to LLMs, finetuning requires expensive human labels, and distillation requires large amounts of unlabeled data which can be hard to obtain (Tang et al., 2019; Liang et al., 2020).

为了规避大模型的这些部署挑战，从业者通常会选择部署较小的专用模型。这些较小的模型是使用两种常见范式之一进行训练的：微调或蒸馏。微调使用下游人类标注数据更新预训练的较小模型(例如BERT(Devlin et al.，2018)或T5(Raffel et al.，2020))(Howard和Ruder，2018)。蒸馏用较大LLM生成的标签训练相同的较小模型(Tan et al., 2019; an et al., 2021; mit et al., 2022a; ror et al., 2022)。不幸的是，这些范式以一定的代价降低了模型的大小：为了实现与LLM相当的性能，微调需要昂贵的人工标签，而蒸馏需要大量难以获得的未标注数据(Tang et al.，2019; iang et al.，2020)。

In this work, we introduce Distilling step-by-step, a new simple mechanism for training smaller models with less training data. Our mechanism reduces the amount of training data required for both finetuning and distillation of LLMs into smaller model sizes. Core to our mechanism is changing our perspective from viewing LLMs as more than a source of noisy labels to viewing them as agents that can reason: LLMs can produce natural language rationales justifying their predicted labels (Wei et al., 2022; Kojima et al., 2022). For example, when asked “A gentleman is carrying equipment for golf, what does he likely have? (a) club, (b) assembly hall, (c) meditation center, (d) meeting, (e) church”, an LLM can be prompted with chain-of-thought (CoT) reasoning (Wei et al., 2022) to answer “(a) club” and rationalize the label by stating, “The answer must be something that is used for golf. Of the above choices, only clubs are used for golf.”. We train smaller models using these extracted rationales as additional, richer information within a multi-task training setup with both label prediction and rationale prediction (Raffel et al., 2020; Narang et al., 2020).

在这项工作中，我们介绍了逐步蒸馏，这是一种新的简单机制，用于使用较少的数据来训练较小的模型。我们的机制减少了LLM微调和蒸馏到较小模型大小所需的训练数据量。我们机制的核心是改变我们的观点，从将LLM视为噪声标签的来源，转变为将其视为可以推理的智能体：LLM可以产生自然语言推理来证明其预测的标签(We et al., 2022; ojim et al., 2022)。例如，当被问及“一位绅士正在携带高尔夫设备，他可能有什么？(A)球杆、(b)礼堂、(c)冥想中心、(d)会议、(e)教堂”时，LLM可以通过思维链(CoT)推理(Wei et al.，2022)来回答“(A)球杆”，并通过陈述来合理化标签，“答案必须是用于高尔夫的东西。在上述选择中，只有球杆用于高尔夫”。我们在多任务训练设置中使用这些蒸馏的基本原理作为额外的、更丰富的信息来训练较小的模型，同时进行标签预测和基本原理预测(Raffe et al., 2020; aran et al., 2020)。

Distilling step-by-step allows us to learn taskspecific smaller models that outperform LLMs using over 500× less model parameters, and it does so with far fewer training examples compared to traditional finetuning or distillation (Figure 1). Our results show three promising empirical conclusions across 4 NLP benchmarks. First, compared to both finetuning and distillation, our resulting models achieve better performance with over 50% less training examples on average across datasets (and up to over 85% reduction). Second, our models outperform LLMs with much smaller model sizes (up to 2000× smaller), drastically reducing the computation cost required for model deployment. Third, we simultaneously reduce the model size as well as the amount of data required to outperform LLMs. We surpass the performance of 540B parameter LLMs using a 770M T5 model; this smaller model only uses 80% of a labeled dataset that would otherwise be required if using an existing finetuning method. When only unlabeled data is present, our small models still perform on par or better than LLMs. We outperform 540B PaLM’s performance with only a 11B T5 model. We further show that when a smaller model performs worse than an LLM, Distilling step-by-step can more efficiently leverage additional unlabeled data to match the LLM performance compared to the standard distillation approach.

逐步蒸馏使我们能够使用500倍以下的模型参数来学习特定任务的较小模型，这些模型的性能优于LLM，而且与传统的微调或蒸馏相比，它的训练样本要少得多(图1)。我们的研究结果显示，在4个NLP基准上得出了三个有希望的实证结论。
1. 与微调和蒸馏相比，我们得到的模型实现了更好的性能，数据集的训练样本平均减少了50%以上(减少了85%以上)。
2. 我们的模型优于LLM，模型尺寸小得多(小到2000×)，大大降低了模型部署所需的计算成本。
3. 我们同时减少了模型大小以及优于LLM所需的数据量。我们使用770M T5模型超越了540B参数LLM的性能; 这个较小的模型只使用标注数据集的80%，如果使用现有的微调方法，则需要标注数据集。当只存在未标注的数据时，我们的小型模型仍然可以与LLM不相上下或更好地执行。仅使用11B T5模型，我们的性能就超过了540B PaLM的性能。
我们进一步表明，与标准蒸馏方法相比，当较小的模型比LLM表现更差时，逐步蒸馏可以更有效地利用额外的未标注数据来匹配LLM性能。

## 2 Related Work
Our work distills the capabilities of LLMs into smaller models by leveraging the emergent reasoning capabilities of today’s LLMs. We draw on recent knowledge distillation research and other methods that learn from both human-generated rationales and LLM-generated rationales.

我们的工作通过利用当今LLM的紧急推理能力，将LLM的能力蒸馏成更小的模型。我们借鉴了最近的知识蒸馏研究和其他方法，从人类产生的基本原理和LLM产生的基本原理中学习。

### Knowledge distillation from large models.  从大模型中蒸馏知识
Knowledge distillation has been successfully used to transfer knowledge from larger, more competent teacher models into smaller student models affordable for practical applications (Buciluˇa et al., 2006; Hinton et al., 2015; Beyer et al., 2022). It supports learning from limited labeled data, since the larger teacher model is often used to generate a training dataset with noisy pseudo labels (Chen et al., 2020; Iliopoulos et al., 2022; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022; Agrawal et al., 2022). The one limitation that knowledge distillation often faces is its reliance on large amounts of unlabelled data required to create a useful noisy training dataset. Although prior work has explored using data augmentation techniques to reduce this hunger for data (Tang et al., 2019; Liang et al., 2020; Srinivas and Fleuret, 2018; Milli et al., 2019), we propose an alternative approach: we reduce the need for large unlabeled data by distilling not just labels but also the teacher’s rationales.

知识蒸馏已被成功地用于将知识从更大、更有能力的教师模型迁移到可用于实际应用的更小的学生模型中(Buciluˇa et al., 2006; Hinton et al., 2015; Beyer et al., 2022)。它支持从有限的标注数据中学习，因为更大的教师模型通常用于生成具有噪声伪标签的训练数据集(Chen et al., 2020; Iliopoulos et al., 2022; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022; Agrawal et al., 2022)。知识蒸馏经常面临的一个限制是，它依赖于创建有用的噪声训练数据集所需的大量未标注数据。尽管之前的工作已经探索了使用数据增广技术来减少对数据的渴望(Tang et al.，2019; iang et al.，2020; rinivas和Fleuret，2018; illi et al.，2017)，但我们提出了一种替代方法：我们不仅蒸馏标签，还蒸馏教师的基本原理，从而减少对大量未标注数据的需求。
<!-- 不仅蒸馏标签，还蒸馏教师的推理 -->

### Learning with human rationales. 用人类的基本原理学习
While utilizing LLM-generated rationales is a new exciting area of investigation, using human-generated rationales has a rich history (Hase and Bansal, 2021). For instance, human rationales can be used to regularize model behavior (Ross et al., 2017); it can be used as additional inputs to guide a model’s predictions (Rajani et al., 2019); it can be used to improve overall model performance (Zaidan et al., 2007; Zhang et al., 2016; Camburu et al., 2018; Pruthi et al., 2022); and human rationales can be used as gold standard labels to make models more interpretable by generating similar rationales (Wiegreffe et al., 2021; Narang et al., 2020; Eisenstein et al., 2022). Unfortunately, human rationales are expensive.

虽然利用LLM生成的基本原理是一个新的令人兴奋的研究领域，但使用人类生成的基本原理有着丰富的历史(Hase和Bansal，2021)。例如，人类的基本原理可以用来规范模型行为(Ross et al.，2017); 可以用作指导模型预测的额外输入(Rajani et al.，2019); 可用于提高整体模型性能(Zaidan et al., 2007; Zhang et al., 2016; Camburu et al., 2018; Pruthi et al., 2022); 类基本原理可以被用作黄金标准标签，通过生成类似的基本原理使模型更具可解释性(Wiegreffe et al., 2021; Narang et al., 2020; Eisenstein et al., 2022)。不幸的是，人类的基本原理是昂贵的。

### Learning with LLM generated rationales. 用LLM产生的基本原理学习
Today’s LLMs are capable of explaining their predictions by generating high-quality reasoning steps (Wei et al., 2022; Kojima et al., 2022). These reasoning steps have been used to augment input prompts to LLMs, improving their few-shot or zeroshot performance (Wei et al., 2022; Kojima et al.,2022; Wang et al., 2022b); reasoning steps have also been used as additional finetuning data “selfimprove” LLMs (Zelikman et al., 2022; Huang et al., 2022). Unfortunately, regardless of how LLMs are improved, their large size limits their utility in most test-time applications.

今天的LLM能够通过生成高质量的推理步骤来解释它们的预测(Wei et al., 2022; Kojima et al., 2022)。这些推理步骤已被用于增广LLM的输入提示，提高其少样本或零样本性能(Wei et al., 2022; Kojima et al.,2022; Wang et al., 2022b); 推理步骤也被用作额外的微调数据“自我改进”LLM(Zelikman et al., 2022; Huang et al., 2022)。不幸的是，无论LLM是如何改进的，它们的大尺寸都限制了它们在大多数测试时应用程序中的实用性。

Figure 2: Overview on Distilling step-by-step. We first utilize CoT prompting to extract rationales from an LLM (Section 3.1). We then use the generated rationales to train small task-specific models within a multi-task learning framework where we prepend task prefixes to the input examples and train the model to output differently based on the given task prefix (Section 3.2).
图2：逐步蒸馏概述。我们首先利用CoT提示从LLM中蒸馏推理(第3.1节)。然后，我们使用生成的推理在多任务学习框架内训练特定于小任务的模型，在该框架中，我们为输入样本预先设置任务前缀，并根据给定的任务前缀训练模型以不同的方式输出(第3.2节)。

By contrast, we leverage generated rationales as informative supervision to train smaller taskspecific models, i.e. models that can be deployed without incurring large computation or memory costs. In the past few months, three concurrent works have also proposed a similar idea to ours – that of using extracted rationales as supervision (Wang et al., 2022a; Ho et al., 2022; Magister et al., 2022). Amongst them, PINTO (Wang et al., 2022a) relies on an LLM to generate rationales at test-time, and thus does not fully solve deployment challenges. Compared with Ho et al. (2022) and Magister et al. (2022), we go beyond their experiments to provide a granular study by varying training dataset size, exploring downstream model sizes, and demonstrating the effectiveness of our method on fully unlabeled datasets.

相比之下，我们利用生成的推理作为信息监督来训练较小的特定任务模型，即可以在不产生大量计算或内存成本的情况下部署的模型。在过去的几个月里，三项同时进行的工作也提出了与我们类似的想法——使用蒸馏的推理作为监督(Wan et al., 2022a;  et al., 2022; agiste et al., 2022)。其中，PINTO(Wang et al.，2022a)依赖LLM在测试时生成推理，因此不能完全解决部署挑战。与H et al., 相比。(2022)和Magiste et al., (2022)，我们超越了他们的实验，通过改变训练数据集的大小，探索下游模型的大小，并证明我们的方法在完全未标注的数据集上的有效性，提供了一个细粒度的研究。

## 3 Distilling step-by-step 逐步蒸馏
We propose a new paradigm, Distilling step-bystep, that leverages the ability of LLMs to reason about their predictions to train smaller models in a data-efficient way. Our overall framework is illustrated in Figure 2. Our paradigm has two simple steps: First, given an LLM and an unlabeled dataset, we prompt the LLM to generate output labels along with rationales to justify the labels. Rationales are natural language explanations that provide support for the model’s predicted label (see Figure 2). Rationales are an emergent behavioral property of today’s self-supervised LLMs. Second, we leverage these rationales in addition to the task labels to train smaller downstream models. Intuitively, rationales provide richer, more detailed information about why an input is mapped to a specific output label.

我们提出了一种新的范式，逐步蒸馏，利用LLM对其预测进行推理的能力，以数据高效的方式训练较小的模型。我们的总体框架如图2所示。我们的范式有两个简单的步骤：
1. 给定一个LLM和一个未标注的数据集，我们提示LLM生成输出标签以及证明标签符合基本原理的推理。基本原理是为模型的预测标签提供支持的自然语言解释(见图2)。基本原理是当今自我监督LLM的一种新兴行为特性。
2. 除了任务标签之外，我们还利用这些原理来训练较小的下游模型。直观地说，基本原理提供了关于为什么将输入映射到特定输出标签的更丰富、更详细的信息。

### 3.1 Extracting rationales from LLMs 从LLM中提取基本原理
Recent studies observe one intriguing emerging property of LLMs: their ability to generate rationales that support their predictions (Wei et al., 2022; Kojima et al., 2022). While the studies have largely focused on how to elicit such reasoning capability from LLMs (Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022), we use them in training smaller downstream models.

最近的研究观察到LLM的一个有趣的新兴特性：它们能够产生支持其预测的基本原理(We et al., 2022; Kojim et al., 2022)。虽然这些研究主要集中在如何从LLM中获得这种推理能力(Nye et al., 2021; Wei et al., 2022; Kojima et al., 2022)，但我们将其用于训练较小的下游模型。

Specifically, we utilize Chain-of-Thought (CoT) prompting (Wei et al., 2022) to elicit and extract rationales from LLMs. As illustrated in Figure 3, given an unlabeled dataset $x_i$ ∈ D, we first curate a prompt template p that articulates how the task should be solved. Each prompt is a triplet ($x^p , r^p , y^p$ ), where $x^p$ is an example input, $y^p$ is its corresponding label and $r^p$ is a user-provided rationale that explains why $x^p$ can be categorized as $y^p$ . We append each input $x_i$ to p and use it as an input to prompt the LLM to generate rationales and labels for each $x_i$ ∈ D. With the demonstrations seen in p, the LLM is able to mimic the triplet demonstration to generate the rationale $\hat{r}_i$ and output $\hat{y}_i$ for $x_i$ .

具体而言，我们利用思维链(CoT)提示(Wei et al.，2022)从LLM中引出和提取基本原理。如图3所示，给定一个未标注的数据集$x_i$∈D，我们首先创建一个提示模板p，该模板阐明了应该如何解决任务。每个提示都是一个三元组($x^p , r^p , y^p$)，其中$x^p$是一个样本输入，$y^p$是其对应的标签，$r^p$是用户提供的基本原理，解释了为什么$x^p$可以被归类为$y^p$。我们将每个输入$x_i$附加到p，并将其用作输入，以提示LLM为每个$x_i$∈D生成基本原理和标签。通过p中的演示，LLM能够模拟三元组演示，以生成基本原理$\hat{r}_i$和$x_i$的输出y \710; 。

Figure 3: We use few-shot CoT prompting that contains both an example rationale (highlighted in green) and a label (highlighted in blue) to elicit rationales from an LLM on new input examples.
图3：我们使用包含样本基本原理(以绿色突出显示)和标签(以蓝色突出显示)的少样本CoT提示，从新输入样本的LLM中引出基本原理。

### 3.2 Training smaller models with rationales 利用基本原理训练小模型
We first describe the current framework for learning task-specific models. With this framework in place, we extend it to incorporate rationales into the training process. Formally, we denote a dataset as $D = \{(x_i , y_i)\}^N_{i=1}$ where each $x_i$ represents an input and $y_i$ is the corresponding desired output label. While our framework supports inputs and outputs of any modality, our experiments limits x and y to be natural language. This text-to-text framework (Raffel et al., 2020) encompasses a variety of natural language processing tasks: classification, natural language inference, question answering and more.

我们首先描述当前用于学习特定任务模型的框架。有了这个框架，我们将其扩展到将基本原理纳入训练过程中。形式上，我们将数据集表示为$D = \{(x_i , y_i)\}^N_{i=1}$，其中每个$x_i$表示一个输入，$y_i$是相应的期望输出标签。虽然我们的框架支持任何模态的输入和输出，但我们的实验将x和y限制为自然语言。该文本到文本框架(Raffel et al.，2020)包括各种自然语言处理任务：分类、自然语言推理、问答等。

Standard finetuning and task distillation. The most common practice to train a task-specific model is to finetune a pretrained model with supervised data (Howard and Ruder, 2018). In the absence of human-annotated labels, task-specific distillation (Hinton et al., 2015; Tang et al., 2019) uses LLM teachers to generates pseudo noisy training labels, $\hat{y}_i$ in place of $y_i$ (Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022).

标准微调和任务蒸馏。训练特定任务模型的最常见做法是使用监督数据对预训练的模型进行微调(Howard和Ruder，2018)。在没有人工标注标签的情况下，任务特异性蒸馏(Hinto et al., 2015; an et al., 2019)使用LLM教师生成伪噪声训练标签，$\hat{y}_i$代替$y_i$(Wan et al., 2021; mit et al., 2022a; ror et al., 2022)。

For both scenarios, the smaller model f is trained to minimize the following label prediction loss:

对于这两种情况，较小的模型f被训练以最小化以下标签预测损失：

$L_{label} = \frac{1}{N} \sum^N_{i=1} l (f(x_i), \hat{y}_i)$, (1)

where l is the cross-entropy loss between the predicted and target tokens. Note that for ease of exposition, we overload $\hat{y}_i$ in Eq. 1 to be either human-annotated labels $y_i$ for the standard finetuning case, or LLM-predicted labels $\hat{y}_i$ for the model distillation case.

其中l是预测的和目标令牌之间的交叉熵损失。请注意，为了便于说明，我们将等式1中的$\hat{y}_i$重载为标准微调情况下的人工标注标签$y_i$，或模型蒸馏情况下的LLM预测标签$\hat{y}_i$。

Multi-task learning with rationales. To create a more explicit connection between $x_i$’s to $\hat{y}_i$’s, we use extracted rationales $\hat{r}_i$ as additional supervision. There are several ways to incorporate rationales into the downstream model’s training process.One straightforward approach is feed $\hat{r}_i$ as an additional input—as proposed by other concurrent research (Rajani et al., 2019; Wang et al., 2022a). In other words, the f($x_i$ , $\hat{r}_i$) → $\hat{y}_i$ is trained with both text and rationale [x, r] as inputs:

基于基本原理的多任务学习。为了在$x_i$和$\hat{y}_i$之间建立更明确的联系，我们使用蒸馏的$\hat{r}_i$作为额外的监督。有几种方法可以将基本原理纳入下游模型的训练过程。一种简单的方法是将$\hat{r}_i$作为其他并行研究提出的额外输入(Rajani et al.，2019; ang et al.，2022a)。 换句话说，f($x_i$ , $\hat{r}_i$) → $\hat{y}_i$ 用文本和基本原理[x，r]作为输入来训练yõi：

$L = \frac{1}{N} \sum^N_{i=1} l (f(x_i , \hat{r}_i), \hat{y}_i)$. (2)

Unfortunately, this design requires an LLM to first generate a rationale before the f can make a prediction. The LLM is still necessary during deployment, limited its deployability.

不幸的是，这种设计需要LLM首先生成一个基本原理，然后f才能进行预测。LLM在部署过程中仍然是必要的，限制了其可部署性。

In this work, instead of using rationales as additional model inputs, we frame learning with rationales as a multi-task problem. Specifically, we train the model f($x_i$) → (ˆ$y_i$ , $\hat{r}_i$) to not only predict the task labels but also generate the corresponding rationales given the text inputs:

在这项工作中，我们没有使用基本原理作为额外的模型输入，而是将基本原理学习作为一个多任务问题。具体来说，我们训练模型 f($x_i$) → (ˆ$y_i$ , $\hat{r}_i$) 在给定文本输入的情况下，不仅预测任务标签，而且生成相应的推理：

$L = L_{label} + λL_{rationale}$, (3)

where $L_{label}$ is the label prediction loss in Eq. 1 and $L_{rationale}$ is the rationale generation loss:

其中$L_{label}$是等式1中的标签预测损失，Lrationality是rationalgeneration损失：

$L_{rationale} = \frac{1}{N} \sum^N_{i=1} l(f(x_i), \hat{r}_i)$. (4)

The rationale generation loss enables the model to learn to generate the intermediate reasoning steps for the prediction, and could therefore guide the model in better predicting the resultant label. This is our proposed Distilling step-by-step. Compared with Eq. 2, the rationale $\hat{r}_i$ is not required in the test time, which removes the need for an LLM at test-time.

基本原理生成损失使模型能够学习生成用于预测的中间推理步骤，因此可以指导模型更好地预测结果标签。这是我们建议的逐步蒸馏。与等式2相比，在测试时间中不需要基本原理$\hat{r}_i$，这消除了在测试时间对LLM的需要。

We prepend “task prefixes” ([label], [rationale]) to the input examples and train the smaller model to output $\hat{y}_i$ when [label] is provided and to produce $\hat{r}_i$ with [rationale] (Raffel et al., 2020).

我们将“任务前缀”([标签]，[基本原理])预先添加到输入样本中，并训练较小的模型在提供[标签]时输出y Plot i，并使用[基本原理】生成$\hat{r}_i$(Raffe et al., 2020)。

## 4 Experiments 
We empirically validate the effectiveness of Distilling step-by-step. First, we show that when compared to standard finetuning and task distillation approaches, Distilling step-by-step achieves better performance with much fewer number of training examples, substantially improving the data efficiency to learn small task-specific models (Sec. 4.1). Second, we show that Distilling step-by-step surpasses the performance of LLMs with much smaller model size, drastically lowering the deployment cost compared to LLMs (Sec. 4.2).

我们通过实验验证了逐步蒸馏的有效性。首先，我们表明，与标准的微调和任务蒸馏方法相比，逐步蒸馏通过更少的训练样本实现了更好的性能，大大提高了学习小任务特定模型的数据效率(第4.1节)。其次，我们表明逐步蒸馏以小得多的模型大小超过了LLM的性能，与LLM相比，大大降低了部署成本(第4.2节)。

Figure 4: We compare Distilling step-by-step and Standard finetuning using 220M T5 models on varying sizes of human-labeled datasets. On all datasets, Distilling step-by-step is able to outperform Standard finetuning, trained on the full dataset, by using much less training examples (e.g., 12.5% of the full e-SNLI dataset).
图4：我们在不同大小的人类标注数据集上使用220M T5模型比较了逐步蒸馏和标准微调。在所有数据集上，通过使用更少的训练样本(例如，完整e-SNLI数据集的12.5%)，逐步蒸馏能够优于在完整数据集上训练的标准微调。

Figure 5: Similar to the plots above, we compare Distilling step-by-step and Standard task distillation using 220M T5 models on varying sizes of unlabeled datasets. Distilling step-by-step is able to outperform Standard task distillation by using only a small subset of the full unlabeled dataset (e.g., 12.5% on ANLI dataset).
图5：与上面的图类似，我们在不同大小的未标注数据集上使用220M T5模型对逐步蒸馏和标准任务蒸馏进行了比较。通过仅使用完整未标注数据集的一小部分(例如，ANLI数据集上的12.5%)，逐步蒸馏能够优于标准任务蒸馏。

Finally, we investigate the minimum resources required, w.r.t. both number of training examples and model size, for Distilling step-by-step to outperform LLMs. We show that Distilling step-by-step outperforms LLMs by using less data and smaller model, simultaneously improving both dataand deployability-efficiency (Sec. 4.3).

最后，我们研究了逐步蒸馏所需的最小资源，包括训练样本的数量和模型大小，以优于LLM。我们表明，逐步蒸馏通过使用更少的数据和更小的模型而优于LLM，同时提高了数据和可部署性的效率(第4.3节)。

Setup. In the experiments, we consider the 540B PaLM model (Chowdhery et al., 2022) as the LLM. For task-specific downstream models, we use T5 models (Raffel et al., 2020) where we initialize the models with pretrained weights obtained from publicly available sources1 . For CoT prompting, we follow Wei et al. (2022) when available, and curate our own examples for new datasets. We include more implementation details in Appendix A.1.

设置。在实验中，我们将540B PaLM模型(Chowdhery et al.，2022)视为LLM。对于特定任务的下游模型，我们使用T5模型(Raffe et al., 2020)，其中我们使用从公开来源获得的预训练权重初始化模型1。对于CoT提示，我们遵循We et al., (2022)(如果可用)，并为新的数据集策划我们自己的样本。我们在附录A.1中包含了更多实施细节。

1 https://huggingface.co/

Datasets. We conduct the experiments on 4 popular benchmark datasets across 3 different NLP tasks: e-SNLI (Camburu et al., 2018) and ANLI (Nie et al., 2020) for natural language inference; CQA (Talmor et al., 2019; Rajani et al., 2019) for commonsense question answering; SVAMP (Patel et al., 2021) for arithmetic math word problems. We include more dataset details in Appendix A.2.

数据集。我们在3个不同的NLP任务的4个流行的基准数据集上进行了实验：用于自然语言推理的e-SNLI(Camburu et al.，2018)和ANLI(Nie et al.，2020); 识性问答的CQA(Talmo et al., 2019; ajan et al., 2019); 于算术数学单词问题的SVAMP(Pate et al., 2021)。我们在附录A.2中包含了更多数据集详细信息。

### 4.1 Reducing training data 减少训练数据
We compare Distilling step-by-step to two most common methods in learning task-specific models: (1) STANDARD FINETUNING when human-labeled examples are available, and (2) STANDARD TASK DISTILLATION when only unlabeled examples are available. Specifically, standard finetuning refers to the prevailing pretrain-then-finetune paradigm that finetunes a model with ground-truth labels via standard label supervision (Howard and Ruder, 2018). On the other hand, when only unlabeled examples are available, standard task distillation learns the task-specific model by treating a teacher LLM’s predicted labels as ground-truths (Hinton et al., 2015; Chen et al., 2020; Wang et al., 2021; Smith et al., 2022a; Arora et al., 2022).

我们将逐步蒸馏与学习任务特定模型中最常见的两种方法进行了比较：(1)当有人令牌的样本可用时，进行标准微调; 2)当只有未标注的样本可供使用时进行标准任务蒸馏。具体而言，标准微调是指通过标准标签监督用基本事实标签微调模型的普遍的预训练-然后微调范式(Howard和Ruder，2018)。另一方面，当只有未标注的样本可用时，标准任务蒸馏通过将教师LLM的预测标签视为基本事实来学习特定任务模型(Hinton et al.，2015; hen et al.，2020; ang et al.，2021; mith et al.，2022a; rora et al.，2022)。

In the following set of experiments, we fix the task-specific models to be 220M T5-Base models, and compare the task performances achieved by different methods under varying number of available training examples.

在下面的一组实验中，我们将特定于任务的模型固定为220M T5 Base模型，并比较不同方法在不同数量的可用训练样本下实现的任务性能。

Distilling step-by-step outperforms standard finetuning with much less labeled examples. When finetuned with human-labeled examples, Figure 4 shows that Distilling step-by-step consistently achieves better performance than standard finetuning across varying numbers of labeled examples used. Furthermore, we see that Distilling step-bystep can achieve the same performance as standard finetuning with much less labeled examples. In particular, by using only 12.5% of the full eSNLI dataset, Distilling step-by-step can outperform standard finetuning trained with 100% of the full dataset. Similarly, we achieve 75%, 25%, and 20% reduction in training examples required to outperform standard finetuning on ANLI, CQA, and SVAMP respectively.

逐步蒸馏优于令牌较少的样本的标准微调。当使用人工令牌的样本进行微调时，图4显示，在使用的不同数量的令牌样本中，逐步蒸馏始终比标准微调获得更好的性能。此外，我们可以看到，逐步蒸馏可以实现与标准微调相同的性能，使用更少令牌的样本。特别是，通过仅使用完整eSNLI数据集的12.5%，逐步蒸馏可以优于使用100%完整数据集训练的标准微调。同样，我们在ANLI、CQA和SVAMP上分别实现了75%、25%和20%的训练样本减少，以优于标准微调。

Distilling step-by-step outperforms standard distillation with much less unlabeled examples. When only unlabeled data is available, we compare Distilling step-by-step to standard task distillation. In Figure 5, we observe an overall similar trend to the finetuning setup. Specifically, we see that Distilling step-by-step outperforms standard task distillation on all 4 datasets under different numbers of unlabeled data used. We as well see that Distilling step-by-step requires much less unlabeled data to outperform standard task distillation. For instance, we need only 12.5% of the full unlabeled dataset to outperform the performance achieved by standard task distillation using 100% of the training examples on e-SNLI dataset.

分步蒸馏优于标准蒸馏，未标注的样本要少得多。当只有未标注的数据可用时，我们将逐步蒸馏与标准任务蒸馏进行比较。在图5中，我们观察到与微调设置总体相似的趋势。具体来说，我们看到，在使用不同数量的未标注数据的情况下，逐步蒸馏在所有4个数据集上都优于标准任务蒸馏。我们还可以看到，逐步蒸馏所需的未标注数据要少得多，才能优于标准任务蒸馏。例如，我们只需要12.5%的完整未标注数据集，就可以优于使用e-SNLI数据集上100%的训练样本进行标准任务蒸馏所获得的性能。

### 4.2 Reducing model size 缩小模型尺寸
In the following set of experiments, we hold the training set size fixed (using 100% of the datasets), and compare varying sizes of small T5 models trained with Distilling step-by-step and standard approaches to LLMs. Specifically, we consider 3 different sizes of T5 models, i.e., 220M T5-Base, 770M T5-Large, and 11B T5-XXL. For LLMs, we include two baseline methods: (1) FEW-SHOT COT (Wei et al., 2022), and (2) PINTO TUNING (Wang et al., 2022a). Few-shot CoT directly utilizes CoT demonstrations to prompt the 540B PaLM to generate intermediate steps before predicting the final labels without any further finetuning of the LLM. PINTO tuning refers to our extension of Wang et al. (2022a) to handle tasks beyond question-answering, which are not studied by Wang et al. (2022a). Here, we finetune a 220M T5-Base model on top of the outputs generated from the PaLM model, which can be viewed as a finetuning method for LLMs with additional parameters (Zhang et al., 2020; Lester et al., 2021).

在下面的一组实验中，我们将训练集大小固定(使用100%的数据集)，并比较使用逐步蒸馏和LLM标准方法训练的不同大小的小型T5模型。具体而言，我们考虑了3种不同尺寸的T5模型，即220M T5 Base、770M T5 Large和11B T5-XXL。对于LLM，我们包括两种基线方法：(1)FEW-SHOT COT(Wei et al.，2022)和(2)PINTO TUNING(Wang et al.，2022a)。少样本CoT直接利用CoT演示来提示540B PaLM在预测最终标签之前生成中间步骤，而无需对LLM进行任何进一步的微调。PINTO调优指的是我们对Wan et al., (2022a)的扩展，以处理问答之外的任务，Wan et al., 没有对此进行研究。在这里，我们在PaLM模型生成的输出的基础上微调220M T5 Base模型，这可以被视为具有附加参数的LLM的微调方法(Zhang et al.，2020; ester et al.，2021)。

We present the experimental results under the two broad scenarios of having access to labeled datasets or unlabeled datasets in Figure 6 and Figure 7, respectively. We plot each method by their deployed model sizes for prediction (x-axis), and their corresponding task performances (y-axis).

我们分别在图6和图7中演示了在访问标注数据集或未标注数据集这两种广泛场景下的实验结果。我们根据每种方法的预测部署模型大小(x轴)和相应的任务性能(y轴)绘制了每种方法。

Distilling step-by-step improves over standard baselines across varying model sizes used. In Figure 6 and Figure 7 respectively, we see that Distilling step-by-step consistently improves over standard finetuning and standard distillation across all sizes of T5 models. The improvements are most pronounced on ANLI, where Distilling step-bystep outperforms standard finetuning and distillation by an average of 8% and 13% on task accuracy respectively.

逐步蒸馏在所使用的不同模型尺寸上优于标准基线。在图6和图7中，我们分别看到，在所有尺寸的T5模型中，逐步蒸馏比标准微调和标准蒸馏持续改进。改进在ANLI上最为明显，其中逐步蒸馏在任务精度上分别优于标准微调和蒸馏平均8%和13%。

Distilling step-by-step outperforms LLMs by using much smaller task-specific models. In Figure 6 when human-labeled datasets are available, Distilling step-by-step can always outperform Few-shot CoT and PINTO tuning on all 4 datasets considered, by using much smaller T5 models. For instance, we can achieve better performances than 540B PaLM model’s Few-shot CoT with 220M (over 2000× smaller) T5 model on eSNLI, 770M (over 700× smaller) T5 models on ANLI and SVAMP, and 11B (over 45× smaller) T5 model on CQA. These results hold true even by further finetuning the 540B PaLM model on available labeled data with PINTO tuning2 .

逐步蒸馏通过使用更小的特定于任务的模型而优于LLM。在图6中，当人类令牌的数据集可用时，通过使用小得多的T5模型，逐步蒸馏在所考虑的所有4个数据集上总是优于少样本CoT和PINTO调优。例如，我们可以在eSNLI上使用220M(超过2000×更小)T5模型，在ANLI和SVAMP上使用770M(超过700×更小)T5模型，在CQA上使用11B(超过45×更小)的T5模型，实现比540B PaLM模型的少样本CoT更好的性能。即使通过PINTO微调2在可用标注数据上进一步微调540B PaLM模型，这些结果仍然成立。

In Figure 7, by only utilizing unlabeled examples, Distilling step-by-step also outperforms the teacher LLM on 3 out of 4 datasets. Specifically, Distilling step-by-step surpasses the 540B PaLM model’s Few-shot CoT performance by using 11B T5 with less than 3% of PaLM’s size. On SVAMP where the distilled model underperforms, we hypothesize that the performance gap is due to the relatively small number of data points in the dataset (i.e., 800). In reaction, we propose to augment the dataset with additional unlabeled examples to close the performance gap as shown in next.

在图7中，通过仅使用未标注的样本，逐步蒸馏在四分之三的数据集上也优于教师LLM。具体而言，Distilleing逐步超越了540B PaLM模型的少样本CoT性能，使用的11B T5尺寸不到PaLM尺寸的3%。在蒸馏模型表现不佳的SVAMP上，我们假设性能差距是由于数据集中的数据点数量相对较少(即800个)。作为回应，我们建议用额外的未标注样本来增广数据集，以缩小性能差距，如下所示。

2We note that PETuning methods may outperform PINTO tuning. However, they require massive resource in both training and deployment, which is not the focus of this work.

2我们注意到，PET微调方法可能优于PINTO微调。然而，它们在训练和部署方面都需要大量资源，而这并不是这项工作的重点。

Figure 6: We perform Distilling step-by-step and Standard finetuning, using the full human-labeled datasets, on varying sizes of T5 models and compare their performance to LLM baselines, i.e., Few-shot CoT and PINTO Tuning. Distilling step-by-step is able to outperform LLM baselines by using much smaller models, e.g., over 700× smaller model on ANLI. Standard finetuning fails to match LLM’s performance using the
图6：我们使用完整的人类标注数据集，对不同大小的T5模型进行逐步蒸馏和标准微调，并将其性能与LLM基线进行比较，即少样本CoT和PINTO微调。逐步蒸馏能够通过使用更小的模型(例如，ANLI上超过700×更小的模型)来优于LLM基线。使用的标准微调无法匹配LLM的性能

Figure 7: Using unlabeled datasets, we perform Distilling step-by-step and Standard task distillation on varying sizes of T5 models and compare them to Few-shot CoT. Distilling step-by-step outperforms Few-shot CoT by using 2000× smaller models on e-SNLI and 45× smaller models on ANLI and CQA. On SVAMP, by adding unlabeled examples from ASDiv, we close the gap to Few-shot CoT whereas Standard distillation still struggles to catch up.
图7：使用未标注的数据集，我们对不同大小的T5模型执行逐步蒸馏和标准任务蒸馏，并将其与少量CoT进行比较。逐步蒸馏通过在e-SNLI上使用2000×更小的模型以及在ANLI和CQA上使用45×更小的模型而优于少量CoT。在SVAMP上，通过添加来自ASDiv的未标注样本，我们缩小了与少量注射CoT的差距，而标准蒸馏仍在努力追赶。

Unlabeled data augmentation further improves Distilling step-by-step. We augment the SVAMP training set with unlabeled examples from the ASDiv dataset (Miao et al., 2020). ASDiv dataset contains a total of 2, 305 examples, where each example is a math word problem similar to the ones in SVAMP. In Figure 7 on SVAMP, we show the performances of Distilling step-by-step and standard task distillation using 11B T5 model after augmenting the training set with ASDiv. We see the data augmentation much improves the performance for both Distilling step-by-step and standard task distillation. However, even with the added unlabeled examples, standard task distillation still underperforms Few-shot CoT. On the other hand, Distilling step-by-step is able to much more efficiently exploit the value of the added examples to achieve the same performance level of Few-shot CoT, again, using a T5 model of size less than 3% of the 540B PaLM.

无标签数据增广进一步改进了逐步蒸馏。我们使用来自ASDiv数据集的未标注样本来增广SVAMP训练集(Miao et al.，2020)。ASDiv数据集总共包含2305个样本，其中每个样本都是一个类似于SVAMP中的数学单词问题。在SVAMP的图7中，我们演示了在使用ASDiv增广训练集后，使用11B T5模型逐步蒸馏和标准任务蒸馏的性能。我们看到数据的增加大大提高了分步蒸馏和标准任务蒸馏的性能。然而，即使添加了未标注的样本，标准任务蒸馏仍然表现不佳。另一方面，逐步蒸馏能够更有效地利用添加的样本的价值，再次使用尺寸小于540B PaLM 3%的T5模型，实现与少量CoT相同的性能水平。

### 4.3 Outperforming LLMs using minimum model size and least training data 使用最小模型大小和最少训练数据来超越LLM
Here, using the LLM’s performance as an anchor point, we explore the most efficient resource requirements in terms of both number of training examples and deployed model size, that Distilling step-by-step and standard finetuning/distillation need to outperform the LLM. We present the results, again under human-labeled setting and unlabeled setting, in Figure 8 and Figure 9 respectively. We visualize the results by plotting different resultant models by (1) the number of training examples used (x-axis), (2) the final task performance achieved (y-axis), and (3) the size of the model (visualized by the size of the shaded area).

在这里，使用LLM的性能作为锚点，我们从训练样本的数量和部署的模型大小两个方面探讨了最有效的资源需求，即逐步蒸馏和标准微调/蒸馏需要优于LLM。我们分别在图8和图9中演示了在人类令牌设置和未标注设置下的结果。我们通过以下方式绘制不同的结果模型来可视化结果：(1)使用的训练样本的数量(x轴)，(2)实现的最终任务性能(y轴)，以及(3)模型的大小(通过阴影区域的大小可视化)。

Distilling step-by-step outperforms LLMs with much smaller models by using less data. On all datasets in Figure 8, we see that Distilling stepby-step outperforms PaLM’s Few-shot CoT with much smaller T5 models using only a subset of the available training examples. Specifically, on e-SNLI, Distilling step-by-step can achieve better performance than Few-shot CoT with a model over 2000× smaller (220M T5) and only 0.1% of the full dataset. In Figure 9 where only unlabeled datasets are available, we observe the same trend that Distilling step-by-step can, at most time, outperform Few-shot CoT with smaller model as well as less data. For instance, on ANLI, Distilling stepby-step outperforms the LLM with a 45× smaller model and 50% of the full unlabeled set.

通过使用更少的数据，逐步蒸馏优于具有更小模型的LLM。在图8中的所有数据集上，我们看到Distilleing逐步优于PaLM的少样本CoT，其T5模型要小得多，只使用了可用训练样本的一个子集。具体而言，在e-SNLI上，与模型小于2000倍(220M T5)且仅占整个数据集的0.1%的少样本CoT相比，逐步蒸馏可以获得更好的性能。在图9中，只有未标注的数据集可用，我们观察到了同样的趋势，即逐步蒸馏在大多数情况下都可以用更小的模型和更少的数据来优于少样本CoT。例如，在ANLI上，逐步蒸馏的性能优于LLM，其模型小45倍，完整未标注集占50%。

Figure 8: We show the minimum size of T5 models and the least amount of human-labeled examples required for Distilling step-by-step to outperform LLM’s Few-shot CoT by a coarse-grained search. Distilling step-by-step is able to outperform Few-shot CoT using not only much smaller models, but it also achieves so with much less training examples compared to Standard finetuning. On ANLI, we outperform the LLM CoT with a 770M model using only 80% of the dataset, whereas Standard finetuning struggles to match even using 100% of the dataset.
图8：我们显示了逐步蒸馏所需的T5模型的最小尺寸和最少的人类令牌样本，以通过粗粒度搜索优于LLM的少样本CoT。逐步蒸馏不仅可以使用更小的模型，而且与标准微调相比，它还可以通过更少的训练样本来实现这一点。在ANLI上，我们的770M模型仅使用80%的数据集，优于LLM-CoT，而标准微调即使使用100%的数据集也很难匹配。

Figure 9: Similar to Figure 8 but using only unlabeled examples, Distilling step-by-step is able to outperform Few-shot CoT using much smaller models and with much less examples compared to Standard task distillation. On SVAMP, the x-axis corresponds to the size of ASDiv dataset used for augmenting the original SVAMP dataset, i.e., x = 0 is without augmentation and x = 100 corresponds to adding the full ASDiv dataset.
图9：与图8类似，但仅使用未标注的样本，与标准任务蒸馏相比，使用更小的模型和更少的样本，逐步蒸馏能够优于少量CoT。在SVAMP上，x轴对应于用于增广原始SVAMP数据集的ASDiv数据集的大小，即，x=0不进行增广，x=100对应于添加完整的ASDiv数据集。

Standard finetuning and distillation require more data and larger model. Finally, in Figure 8 and Figure 9, we see that standard finetuning and distillation often need either more data or larger models to match LLM’s performance. For instance, on e-SNLI in Figure 8, we observe that Distilling step-by-step outperform the LLM using only 0.1% of the dataset while standard finetuning requires more data to match the performance. Furthermore, on ANLI in Figure 8, we observe that Distilling step-by-step can outperform PaLM using 770M model with only 80% of the training set while standard finetuning struggles to match the LLM even using the full dataset and thus requires larger model to close the performance gap.

标准的微调和蒸馏需要更多的数据和更大的模型。最后，在图8和图9中，我们看到标准的微调和蒸馏通常需要更多的数据或更大的模型来匹配LLM的性能。例如，在图8中的e-SNLI上，我们观察到逐步蒸馏仅使用0.1%的数据集就优于LLM，而标准微调需要更多的数据来匹配性能。此外，在图8中的ANLI上，我们观察到，使用770M模型，逐步蒸馏仅使用80%的训练集就可以优于PaLM，而标准微调即使使用完整的数据集也难以匹配LLM，因此需要更大的模型来缩小性能差距。

## 5 Discussion 讨论
We propose Distilling step-by-step to extract rationales from LLMs as informative supervision in training small task-specific models. We show that Distilling step-by-step reduces the training dataset required to curate task-specific smaller models; it also reduces the model size required to achieve, and even surpass, the original LLM’s performance. Distilling step-by-step proposes a resource-efficient training-to-deployment paradigm compared to existing methods.

我们建议逐步蒸馏LLM的基本原理，作为训练小型任务特定模型的信息监督。我们表明，逐步蒸馏减少了策划特定任务的较小模型所需的训练数据集; 还减少了实现甚至超过原始LLM性能所需的模型大小。与现有方法相比，逐步蒸馏提出了一种资源高效的训练到部署模型。

## Limitations 限制
There are a number of limitations with our approach. First, we require users to produce a few example demonstrations (∼ 10-shot for all tasks) in order to use the few-shot CoT (Wei et al., 2022) prompting mechanism. This limitation can be improved by using recent advances that suggest that rationales can be elicited without any userannotated demonstrations (Kojima et al., 2022). Second, while we observe success using LLM rationales, there is evidence that LLMs exhibit limited reasoning capability on more complex reasoning and planning tasks (Valmeekam et al., 2022). Future work should characterize how rationale quality affects Distilling step-by-step.

我们的方法有很多局限性。首先，我们要求用户制作一些样本演示(所有任务为~10次)，以便使用少次CoT(Wei et al.，2022)提示机制。这一限制可以通过使用最近的进展来改善，这些进展表明，在没有任何用户标注的演示的情况下，可以得出合理的推理(Kojim et al., 2022)。其次，虽然我们观察到LLM原理的成功，但有证据表明，LLM在更复杂的推理和规划任务中表现出有限的推理能力(Valmickam et al.，2022)。未来的工作应该描述基本原理质量如何影响逐步蒸馏。

## Ethics statement 道德声明
It is worth noting that the behavior of the our downstream smaller models is subject to biases inherited from the larger teacher LLM. We envision that the same research progress in reducing anti-social behaviors in LLMs can also be applied to improve smaller languag

值得注意的是，我们下游较小模型的行为受到从较大教师LLM继承来的偏见的影响。我们设想，在减少LLM中反社会行为方面的同样研究进展也可以应用于改善较小的语言

## References
