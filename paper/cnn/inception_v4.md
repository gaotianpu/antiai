# Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning
Inception-v4, Inception-ResNetå’Œæ®‹å·®è¿æ¥å¯¹å­¦ä¹ çš„å½±å“ 2016.02 https://arxiv.org/abs/1602.07261

## é˜…è¯»ç¬”è®°
* ç½‘ç»œè¶³å¤Ÿæ·±ï¼Œæ•ˆæœæ— å·®å¼‚ï¼Œä½†æ®‹å·®è¿æ¥èƒ½åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹?
* æ®‹å·®å’Œinception, æœ¬è´¨éƒ½æ˜¯å¢åŠ äº†åŸºæ•°(cardinality)

## Abstract
Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.

ææ·±å·ç§¯ç½‘ç»œæ˜¯è¿‘å¹´æ¥å›¾åƒè¯†åˆ«æ€§èƒ½è¿›æ­¥çš„æœ€å¤§æ ¸å¿ƒã€‚ä¸€ä¸ªä¾‹å­æ˜¯Inceptionæ¶æ„ï¼Œå®ƒå·²è¢«è¯æ˜èƒ½å¤Ÿä»¥ç›¸å¯¹è¾ƒä½çš„è®¡ç®—æˆæœ¬å®ç°éå¸¸å¥½çš„æ€§èƒ½ã€‚æœ€è¿‘ï¼Œåœ¨2015å¹´ILSVRCæŒ‘æˆ˜ä¸­ï¼Œç»“åˆæ›´ä¼ ç»Ÿçš„æ¶æ„å¼•å…¥æ®‹å·®è¿æ¥ï¼Œäº§ç”Ÿäº†æœ€å…ˆè¿›çš„æ€§èƒ½; å…¶æ€§èƒ½ä¸æœ€æ–°ä¸€ä»£Inception-v3ç½‘ç»œç›¸ä¼¼ã€‚è¿™å°±æå‡ºäº†è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼šå°†Inceptionæ¶æ„ä¸æ®‹å·®è¿æ¥ç›¸ç»“åˆæ˜¯å¦æœ‰ä»»ä½•å¥½å¤„ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç»™å‡ºäº†æ˜ç¡®çš„ç»éªŒè¯æ®ï¼Œå³ä½¿ç”¨æ®‹å·®è¿æ¥è¿›è¡Œè®­ç»ƒå¯ä»¥æ˜¾è‘—åŠ å¿«Inceptionç½‘ç»œçš„è®­ç»ƒã€‚è¿˜æœ‰ä¸€äº›è¯æ®è¡¨æ˜ï¼Œæ®‹å·®Inceptionç½‘ç»œæ¯”åŒæ ·æ˜‚è´µçš„æ²¡æœ‰æ®‹å·®è¿æ¥çš„Inceptç½‘ç»œè¡¨ç°æ›´å¥½ã€‚æˆ‘ä»¬è¿˜ä¸ºæ®‹å·®å’Œéæ®‹å·®Inceptionç½‘ç»œæä¾›äº†å‡ ç§æ–°çš„ç®€åŒ–æ¶æ„ã€‚è¿™äº›å˜åŒ–æ˜¾è‘—æé«˜äº†ILSVRC 2012åˆ†ç±»ä»»åŠ¡çš„å•å¸§è¯†åˆ«æ€§èƒ½ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æ¼”ç¤ºäº†é€‚å½“çš„æ¿€æ´»ç¼©æ”¾å¦‚ä½•ç¨³å®šéå¸¸å®½çš„æ®‹å·®Inceptionç½‘ç»œçš„è®­ç»ƒã€‚é€šè¿‡ä¸‰ä¸ªæ®‹å·®å’Œä¸€ä¸ªInception-v4çš„ç»„åˆï¼Œæˆ‘ä»¬åœ¨ImageNetåˆ†ç±»(CLS)æŒ‘æˆ˜çš„æµ‹è¯•é›†top5è·å¾—äº†3.08%é”™è¯¯ç‡ã€‚

## 1. Introduction
Since the 2012 ImageNet competition [11] winning entry by Krizhevsky et al [8], their network â€œAlexNetâ€ has been successfully applied to a larger variety of computer vision tasks, for example to object-detection [4], segmentation [10], human pose estimation [17], video classification [7], object tracking [18], and superresolution [3]. These examples are but a few of all the applications to which deep convolutional networks have been very successfully applied ever since.

è‡ª2012å¹´Krizhevskyet al [8]èµ¢å¾—ImageNetç«èµ›[11]æ¯”èµ›ä»¥æ¥ï¼Œä»–ä»¬çš„ç½‘ç»œâ€œAlexNetâ€å·²æˆåŠŸåº”ç”¨äºå¤šç§è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚ç›®æ ‡æ£€æµ‹[4]ã€åˆ†å‰²[10]ã€äººä½“å§¿åŠ¿ä¼°è®¡[17]ã€è§†é¢‘åˆ†ç±»[7]ã€ç›®æ ‡è·Ÿè¸ª[18]å’Œè¶…åˆ†è¾¨ç‡[3]ã€‚è¿™äº›ä¾‹å­åªæ˜¯æ·±å·ç§¯ç½‘ç»œè‡ªé‚£æ—¶ä»¥æ¥æˆåŠŸåº”ç”¨çš„æ‰€æœ‰åº”ç”¨ä¸­çš„ä¸€å°éƒ¨åˆ†ã€‚

In this work we study the combination of the two most recent ideas: Residual connections introduced by He et al. in [5] and the latest revised version of the Inception architecture [15]. In [5], it is argued that residual connections are of inherent importance for training very deep architectures. Since Inception networks tend to be very deep, it is natural to replace the filter concatenation stage of the Inception architecture with residual connections. This would allow Inception to reap all the benefits of the residual approach while retaining its computational efficiency.

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸¤ç§æœ€æ–°æ€æƒ³çš„ç»“åˆï¼šHeet al åœ¨[5]ä¸­å¼•å…¥çš„æ®‹å·®è¿æ¥å’Œæœ€æ–°ä¿®è®¢ç‰ˆçš„Inceptionæ¶æ„[15]ã€‚åœ¨[5]ä¸­ï¼Œæœ‰äººè®¤ä¸ºæ®‹å·®è¿æ¥å¯¹äºè®­ç»ƒéå¸¸æ·±å…¥çš„æ¶æ„å…·æœ‰å›ºæœ‰çš„é‡è¦æ€§ã€‚å› ä¸ºInceptionç½‘ç»œå¾€å¾€å¾ˆæ·±ï¼Œæ‰€ä»¥ç”¨æ®‹å·®è¿æ¥æ›¿æ¢Inceptionæ¶æ„çš„çš„å·ç§¯æ ¸ä¸²è”(concatenation)é˜¶æ®µæ˜¯å¾ˆè‡ªç„¶çš„ã€‚è¿™å°†ä½¿Inceptionåœ¨ä¿æŒå…¶è®¡ç®—æ•ˆç‡çš„åŒæ—¶ï¼Œè·å¾—æ®‹å·®æ–¹æ³•çš„æ‰€æœ‰å¥½å¤„ã€‚

Besides a straightforward integration, we have also studied whether Inception itself can be made more efficient by making it deeper and wider. For that purpose, we designed a new version named Inception-v4 which has a more uniform simplified architecture and more inception modules than Inception-v3. Historically, Inception-v3 had inherited a lot of the baggage of the earlier incarnations. The technical constraints chiefly came from the need for partitioning the model for distributed training using DistBelief [2]. Now, after migrating our training setup to TensorFlow [1] these constraints have been lifted, which allowed us to simplify the architecture significantly. The details of that simplified architecture are described in Section 3.

é™¤äº†ç®€å•çš„é›†æˆä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ç ”ç©¶äº†Inceptionæœ¬èº«æ˜¯å¦å¯ä»¥é€šè¿‡ä½¿å…¶æ›´æ·±å’Œæ›´å®½æ¥æé«˜æ•ˆç‡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªåä¸ºInception-v4çš„æ–°ç‰ˆæœ¬ï¼Œå®ƒå…·æœ‰æ¯”Incepton-v3æ›´ç»Ÿä¸€çš„ç®€åŒ–æ¶æ„å’Œæ›´å¤šçš„Inceptionæ¨¡å—ã€‚å†å²ä¸Šï¼ŒInception v3ç»§æ‰¿äº†æ—©æœŸç‰ˆæœ¬çš„è®¸å¤šåŒ…è¢±ã€‚æŠ€æœ¯é™åˆ¶ä¸»è¦æ¥è‡ªä½¿ç”¨ DistBelief åˆ’åˆ†åˆ†å¸ƒå¼è®­ç»ƒæ¨¡å‹çš„éœ€è¦[2]ã€‚ç°åœ¨ï¼Œåœ¨å°†æˆ‘ä»¬çš„è®­ç»ƒè®¾ç½®è¿ç§»åˆ°TensorFlow[1]ä¹‹åï¼Œè¿™äº›çº¦æŸå·²ç»è§£é™¤ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ˜¾è‘—ç®€åŒ–æ¶æ„ã€‚ç¬¬3èŠ‚æè¿°äº†ç®€åŒ–æ¶æ„çš„ç»†èŠ‚ã€‚

In this report, we will compare the two pure Inception variants, Inception-v3 and v4, with similarly expensive hybrid Inception-ResNet versions. Admittedly, those models were picked in a somewhat ad hoc manner with the main constraint being that the parameters and computational complexity of the models should be somewhat similar to the cost of the non-residual models. In fact we have tested bigger and wider Inception-ResNet variants and they performed very similarly on the ImageNet classification challenge [11] dataset.

åœ¨æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒä¸¤ä¸ªçº¯Inceptionå˜ä½“ï¼ŒInception-v3å’Œv4ï¼Œä»¥åŠåŒæ ·æ˜‚è´µçš„æ··åˆIncepton-ResNetç‰ˆæœ¬ã€‚è¯šç„¶ï¼Œè¿™äº›æ¨¡å‹æ˜¯ä»¥æŸç§ç‰¹å®šçš„æ–¹å¼æŒ‘é€‰çš„ï¼Œä¸»è¦é™åˆ¶æ˜¯æ¨¡å‹çš„å‚æ•°å’Œè®¡ç®—å¤æ‚åº¦åº”è¯¥ä¸éæ®‹å·®æ¨¡å‹çš„æˆæœ¬æœ‰ç‚¹ç±»ä¼¼ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å·²ç»æµ‹è¯•äº†æ›´å¤§æ›´å¹¿çš„Inception-ResNetå˜ä½“ï¼Œå®ƒä»¬åœ¨ImageNetåˆ†ç±»æŒ‘æˆ˜[11]æ•°æ®é›†ä¸Šçš„è¡¨ç°éå¸¸ç›¸ä¼¼ã€‚

The last experiment reported here is an evaluation of an ensemble of all the best performing models presented here. As it was apparent that both Inception-v4 and InceptionResNet-v2 performed similarly well, exceeding state-ofthe art single frame performance on the ImageNet validation dataset, we wanted to see how a combination of those pushes the state of the art on this well studied dataset. Surprisingly, we found that gains on the single-frame performance do not translate into similarly large gains on ensembled performance. Nonetheless, it still allows us to report 3.1% top-5 error on the validation set with four models ensembled setting a new state of the art, to our best knowledge.

è¿™é‡ŒæŠ¥å‘Šçš„æœ€åä¸€ä¸ªå®éªŒæ˜¯å¯¹è¿™é‡Œæ‰€æœ‰è¡¨ç°æœ€å¥½çš„æ¨¡å‹çš„ç»¼åˆè¯„ä¼°ã€‚æ˜¾ç„¶ï¼ŒInception-v4å’ŒInceptonResNet-v2éƒ½è¡¨ç°å¾—å¾ˆå¥½ï¼Œè¶…è¿‡äº†ImageNetéªŒè¯æ•°æ®é›†çš„æœ€å…ˆè¿›å•å¸§æ€§èƒ½ï¼Œå› æ­¤æˆ‘ä»¬æƒ³çœ‹çœ‹è¿™äº›ç»„åˆå¦‚ä½•æ¨åŠ¨è¿™ä¸€ç»è¿‡å……åˆ†ç ”ç©¶çš„æ•°æ®é›†çš„å…ˆè¿›æ°´å¹³ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç°å•å¸§æ€§èƒ½çš„æé«˜å¹¶æ²¡æœ‰è½¬åŒ–ä¸ºä¿¡å·ç¾¤æ€§èƒ½çš„åŒæ ·å¤§çš„æé«˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä»ç„¶å…è®¸æˆ‘ä»¬æŠ¥å‘ŠéªŒè¯é›†ä¸Š3.1%çš„top-5ä¸ªé”™è¯¯ï¼Œå››ä¸ªæ¨¡å‹é›†åˆè®¾ç½®äº†ä¸€ä¸ªæ–°çš„æŠ€æœ¯æ°´å¹³ã€‚

In the last section, we study some of the classification failures and conclude that the ensemble still has not reached the label noise of the annotations on this dataset and there is still room for improvement for the predictions.

åœ¨æœ€åä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€äº›åˆ†ç±»å¤±è´¥ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼Œé›†æˆä»ç„¶æ²¡æœ‰è¾¾åˆ°è¯¥æ•°æ®é›†ä¸Šæ³¨é‡Šçš„æ ‡ç­¾å™ªå£°ï¼Œé¢„æµ‹ä»æœ‰æ”¹è¿›çš„ä½™åœ°ã€‚

## 2. Related Work
Convolutional networks have become popular in large scale image recognition tasks after Krizhevsky et al. [8]. Some of the next important milestones were Network-innetwork [9] by Lin et al., VGGNet [12] by Simonyan et al. and GoogLeNet (Inception-v1) [14] by Szegedy et al.

ç»§Krizhevskyet al [8]ä¹‹åï¼Œå·ç§¯ç½‘ç»œåœ¨å¤§è§„æ¨¡å›¾åƒè¯†åˆ«ä»»åŠ¡ä¸­å˜å¾—è¶Šæ¥è¶Šæµè¡Œã€‚æ¥ä¸‹æ¥çš„ä¸€äº›é‡è¦é‡Œç¨‹ç¢‘æ˜¯Linet al çš„Network-innetwork[9]ã€Simonyanet al çš„VGGNet[12]å’ŒSzegedyet al çš„GoogLeNet(Inception-v1)[14]ã€‚

Residual connection were introduced by He et al. in [5] in which they give convincing theoretical and practical evidence for the advantages of utilizing additive merging of signals both for image recognition, and especially for object detection. The authors argue that residual connections are inherently necessary for training very deep convolutional models. Our findings do not seem to support this view, at least for image recognition. However it might require more measurement points with deeper architectures to understand the true extent of beneficial aspects offered by residual connections. In the experimental section we demonstrate that it is not very difficult to train competitive very deep networks without utilizing residual connections. However the use of residual connections seems to improve the training speed greatly, which is alone a great argument for their use.

Heet al åœ¨[5]ä¸­å¼•å…¥äº†æ®‹å·®è¿æ¥ï¼Œä»–ä»¬ä¸ºåˆ©ç”¨ä¿¡å·çš„åŠ æ€§åˆå¹¶(additive merging)è¿›è¡Œå›¾åƒè¯†åˆ«ï¼Œç‰¹åˆ«æ˜¯åœ¨ç›®æ ‡æ£€æµ‹çš„ä¼˜åŠ¿æä¾›äº†ä»¤äººä¿¡æœçš„ç†è®ºå’Œå®è·µè¯æ®ã€‚ä½œè€…è®¤ä¸ºï¼Œæ®‹å·®è¿æ¥å¯¹äºè®­ç»ƒéå¸¸æ·±çš„å·ç§¯æ¨¡å‹æ˜¯å†…åœ¨å¿…è¦çš„ã€‚æˆ‘ä»¬çš„å‘ç°ä¼¼ä¹ä¸æ”¯æŒè¿™ç§è§‚ç‚¹ï¼Œè‡³å°‘åœ¨å›¾åƒè¯†åˆ«æ–¹é¢æ˜¯è¿™æ ·ã€‚ç„¶è€Œï¼Œå®ƒå¯èƒ½éœ€è¦æ›´å¤šå…·æœ‰æ›´æ·±å±‚æ¶æ„çš„æµ‹é‡ç‚¹ï¼Œä»¥äº†è§£æ®‹å·®è¿æ¥æ‰€æä¾›çš„æœ‰ç›Šæ–¹é¢çš„çœŸæ­£ç¨‹åº¦ã€‚åœ¨å®éªŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œåœ¨ä¸åˆ©ç”¨æ®‹å·®è¿æ¥çš„æƒ…å†µä¸‹ï¼Œè®­ç»ƒå…·æœ‰ç«äº‰åŠ›çš„æ·±åº¦ç½‘ç»œå¹¶ä¸å›°éš¾ã€‚ç„¶è€Œï¼Œä½¿ç”¨æ®‹å·®è¿æ¥ä¼¼ä¹å¯ä»¥å¤§å¤§æé«˜è®­ç»ƒé€Ÿåº¦ï¼Œè¿™æ˜¯ä½¿ç”¨æ®‹å·®è¿æ¥çš„ä¸€ä¸ªå¾ˆå¥½çš„ç†ç”±ã€‚

The Inception deep convolutional architecture was introduced in [14] and was called GoogLeNet or Inception-v1 in our exposition. Later the Inception architecture was refined in various ways, first by the introduction of batch normalization [6] (Inception-v2) by Ioffe et al. Later the architecture was improved by additional factorization ideas in the third iteration [15] which will be referred to as Inception-v3 in this report.

Inceptionæ·±åº¦å·ç§¯æ¶æ„äº[14]ä¸­å¼•å…¥ï¼Œåœ¨æˆ‘ä»¬çš„è®ºè¿°ä¸­è¢«ç§°ä¸ºGoogLeNetæˆ–Inception-v1ã€‚åæ¥ï¼Œå…ˆæ˜¯Ioffeet al å¼•å…¥äº†æ‰¹å½’ä¸€åŒ–(BN)[6](Inception-v2)ï¼Œä»¥å„ç§æ–¹å¼å¯¹Inceptionæ¶æ„è¿›è¡Œäº†æ”¹è¿›ã€‚éšåï¼Œåœ¨ç¬¬ä¸‰æ¬¡è¿­ä»£[15]ä¸­ï¼Œé€šè¿‡é¢å¤–çš„å› å­åˆ†è§£æ€æƒ³å¯¹æ¶æ„è¿›è¡Œäº†æ”¹å–„ï¼Œåœ¨æœ¬æŠ¥å‘Šä¸­ç§°ä¹‹ä¸ºIncepton-v3ã€‚

![Figure 1_2](../images/inception_v4/fig_1_2.png)<br/>
Figure 1. Residual connections as introduced in He et al. [5].
å›¾1.Heet al [5]ä¸­ä»‹ç»çš„æ®‹å·®è¿æ¥ã€‚

Figure 2. Optimized version of ResNet connections by [5] to shield computation.
å›¾2.é€šè¿‡[5]ä¼˜åŒ–çš„ResNetè¿æ¥ç‰ˆæœ¬ä»¥æ©ç ?è®¡ç®—ã€‚

## 3. Architectural Choices
### 3.1. Pure Inception blocks
Our older Inception models used to be trained in a partitioned manner, where each replica was partitioned into a multiple sub-networks in order to be able to fit the whole model in memory. However, the Inception architecture is highly tunable, meaning that there are a lot of possible changes to the number of filters in the various layers that do not affect the quality of the fully trained network. In order to optimize the training speed, we used to tune the layer sizes carefully in order to balance the computation between the various model sub-networks. In contrast, with the introduction of TensorFlow our most recent models can be trained without partitioning the replicas. This is enabled in part by recent optimizations of memory used by backpropagation, achieved by carefully considering what tensors are needed for gradient computation and structuring the compu- tation to reduce the number of such tensors. Historically, we have been relatively conservative about changing the architectural choices and restricted our experiments to varying isolated network components while keeping the rest of the network stable. Not simplifying earlier choices resulted in networks that looked more complicated that they needed to be. In our newer experiments, for Inception-v4 we decided to shed this unnecessary baggage and made uniform choices for the Inception blocks for each grid size. Plase refer to Figure 9 for the large scale structure of the Inception-v4 network and Figures 3, 4, 5, 6, 7 and 8 for the detailed structure of its components. All the convolutions not marked with â€œVâ€ in the figures are same-padded meaning that their output grid matches the size of their input. Convolutions marked with â€œVâ€ are valid padded, meaning that input patch of each unit is fully contained in the previous layer and the grid size of the output activation map is reduced accordingly.

æˆ‘ä»¬æ—©æœŸçš„Inceptionæ¨¡å‹è¿‡å»æ˜¯ä»¥åˆ†åŒºçš„æ–¹å¼è¿›è¡Œè®­ç»ƒçš„ï¼Œå…¶ä¸­æ¯ä¸ªå‰¯æœ¬è¢«åˆ’åˆ†ä¸ºå¤šä¸ªå­ç½‘ç»œï¼Œä»¥ä¾¿èƒ½å¤Ÿåœ¨å†…å­˜ä¸­å®¹çº³æ•´ä¸ªæ¨¡å‹ã€‚ç„¶è€Œï¼ŒInceptionæ¶æ„æ˜¯é«˜åº¦å¯è°ƒçš„ï¼Œè¿™æ„å‘³ç€åœ¨å„ä¸ªå±‚ä¸­å·ç§¯æ ¸çš„æ•°é‡æœ‰å¾ˆå¤šå¯èƒ½çš„å˜åŒ–ï¼Œè¿™äº›å˜åŒ–ä¸ä¼šå½±å“ç»è¿‡å……åˆ†è®­ç»ƒçš„ç½‘ç»œçš„è´¨é‡ã€‚ä¸ºäº†ä¼˜åŒ–è®­ç»ƒé€Ÿåº¦ï¼Œæˆ‘ä»¬ç»å¸¸ä»”ç»†è°ƒæ•´å±‚å¤§å°ï¼Œä»¥å¹³è¡¡ä¸åŒæ¨¡å‹å­ç½‘ç»œä¹‹é—´çš„è®¡ç®—ã€‚ç›¸åï¼Œéšç€TensorFlowçš„å¼•å…¥ï¼Œæˆ‘ä»¬æœ€æ–°çš„æ¨¡å‹å¯ä»¥åœ¨ä¸åˆ’åˆ†å‰¯æœ¬çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚è¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ˜¯ç”±äºæœ€è¿‘å¯¹åå‘ä¼ æ’­ä½¿ç”¨çš„å†…å­˜è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé€šè¿‡ä»”ç»†è€ƒè™‘æ¢¯åº¦è®¡ç®—æ‰€éœ€çš„å¼ é‡ä»¥åŠæ„é€ è®¡ç®—ä»¥å‡å°‘æ­¤ç±»å¼ é‡çš„æ•°é‡æ¥å®ç°çš„ã€‚ä»å†å²ä¸Šçœ‹ï¼Œæˆ‘ä»¬åœ¨æ”¹å˜æ¶æ„é€‰æ‹©æ–¹é¢ç›¸å¯¹ä¿å®ˆï¼Œå¹¶å°†æˆ‘ä»¬çš„å®éªŒé™åˆ¶åœ¨æ”¹å˜å­¤ç«‹çš„ç½‘ç»œç»„ä»¶ï¼ŒåŒæ—¶ä¿æŒç½‘ç»œçš„å…¶ä½™éƒ¨åˆ†ç¨³å®šã€‚ä¸ç®€åŒ–æ—©æœŸçš„é€‰æ‹©ä¼šå¯¼è‡´ç½‘ç»œçœ‹èµ·æ¥æ›´åŠ å¤æ‚ã€‚åœ¨æˆ‘ä»¬æœ€æ–°çš„å®éªŒä¸­ï¼Œå¯¹äºInception-v4ï¼Œæˆ‘ä»¬å†³å®šæ‘†è„±è¿™ä¸ªä¸å¿…è¦çš„åŒ…è¢±ï¼Œä¸ºæ¯ä¸ªç½‘æ ¼å¤§å°çš„Inceptionå—åšå‡ºç»Ÿä¸€çš„é€‰æ‹©ã€‚Inception-v4ç½‘ç»œçš„å¤§è§„æ¨¡ç»“æ„è§å›¾9ï¼Œå…¶ç»„ä»¶çš„è¯¦ç»†ç»“æ„è§å›¾3ã€4ã€5ã€6ã€7å’Œ8ã€‚å›¾ä¸­æ‰€æœ‰æœªæ ‡è®°â€œVâ€çš„å·ç§¯éƒ½æ˜¯ç›¸åŒçš„å¡«å……ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„è¾“å‡ºç½‘æ ¼ä¸å…¶è¾“å…¥çš„å¤§å°ç›¸åŒ¹é…ã€‚æ ‡æœ‰â€œVâ€çš„å·ç§¯æ˜¯æœ‰æ•ˆçš„å¡«å……ï¼Œè¿™æ„å‘³ç€æ¯ä¸ªå•å…ƒçš„è¾“å…¥åˆ†å—å®Œå…¨åŒ…å«åœ¨å‰ä¸€å±‚ä¸­ï¼Œè¾“å‡ºæ¿€æ´»å›¾çš„ç½‘æ ¼å¤§å°ä¹Ÿç›¸åº”å‡å°ã€‚

### 3.2. Residual Inception Blocks
For the residual versions of the Inception networks, we use cheaper Inception blocks than the original Inception. Each Inception block is followed by filter-expansion layer (1 Ã— 1 convolution without activation) which is used for scaling up the dimensionality of the filter bank before the addition to match the depth of the input. This is needed to compensate for the dimensionality reduction induced by the Inception block.

å¯¹äºInceptionç½‘ç»œçš„æ®‹å·®ç‰ˆæœ¬ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„Inceptionå—æ¯”åŸå§‹çš„Inceptæ›´ä¾¿å®œã€‚æ¯ä¸ªInceptionå—ä¹‹åæ˜¯å·ç§¯æ ¸æ‰©å±•å±‚(1Ã—1å·ç§¯ï¼Œæ— æ¿€æ´»)ï¼Œç”¨äºåœ¨æ·»åŠ ä¹‹å‰æ”¾å¤§å·ç§¯æ ¸ç»„çš„ç»´æ•°ï¼Œä»¥åŒ¹é…è¾“å…¥æ·±åº¦ã€‚è¿™æ˜¯ä¸ºäº†è¡¥å¿ç”±Inceptionå—å¼•èµ·çš„ç»´åº¦å‡å°‘ã€‚

We tried several versions of the residual version of Inception. Only two of them are detailed here. The first one â€œInception-ResNet-v1â€ roughly the computational cost of Inception-v3, while â€œInception-ResNet-v2â€ matches the raw cost of the newly introduced Inception-v4 network. See Figure 15 for the large scale structure of both varianets. (However, the step time of Inception-v4 proved to be significantly slower in practice, probably due to the larger number of layers.)

æˆ‘ä»¬å°è¯•äº†Inceptionæ®‹å·®ç‰ˆæœ¬çš„å‡ ä¸ªç‰ˆæœ¬ã€‚è¿™é‡Œåªè¯¦ç»†ä»‹ç»äº†å…¶ä¸­çš„ä¸¤ä¸ªã€‚ç¬¬ä¸€ä¸ªâ€œInception-ResNet-v1â€å¤§è‡´ç›¸å½“äºIncepton-v3çš„è®¡ç®—æˆæœ¬ï¼Œè€Œâ€œIncept-ResNet-v2â€åˆ™ç›¸å½“äºæ–°å¼•å…¥çš„Inception-v4ç½‘ç»œçš„åŸå§‹æˆæœ¬ã€‚ä¸¤ä¸ªæ–¹å·®çš„å¤§è§„æ¨¡ç»“æ„è§å›¾15ã€‚(ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼ŒInception-v4çš„æ­¥é•¿åœ¨å®è·µä¸­æ˜æ˜¾è¾ƒæ…¢ï¼Œå¯èƒ½æ˜¯å› ä¸ºå±‚æ•°è¾ƒå¤šã€‚)

Another small technical difference between our residual and non-residual Inception variants is that in the case of Inception-ResNet, we used batch-normalization only on top of the traditional layers, but not on top of the summations. It is reasonable to expect that a thorough use of batchnormalization should be advantageous, but we wanted to keep each model replica trainable on a single GPU. It turned out that the memory footprint of layers with large activation size was consuming disproportionate amount of GPUmemory. By omitting the batch-normalization on top of those layers, we were able to increase the overall number of Inception blocks substantially. We hope that with better utilization of computing resources, making this trade-off will become unecessary. 

æˆ‘ä»¬çš„æ®‹å·®å’Œéæ®‹å·®Inceptionå˜ä½“ä¹‹é—´çš„å¦ä¸€ä¸ªå°æŠ€æœ¯å·®å¼‚æ˜¯ï¼Œåœ¨Inception-ResNetçš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªåœ¨ä¼ ç»Ÿå±‚çš„é¡¶éƒ¨ä½¿ç”¨äº†æ‰¹å½’ä¸€åŒ–(BN)ï¼Œè€Œä¸æ˜¯åœ¨æ€»å’Œçš„é¡¶éƒ¨ã€‚å®Œå…¨ä½¿ç”¨æ‰¹å½’ä¸€åŒ–(BN)åº”è¯¥æ˜¯æœ‰åˆ©çš„ï¼Œè¿™æ˜¯åˆç†çš„ï¼Œä½†æˆ‘ä»¬å¸Œæœ›åœ¨å•ä¸ªGPUä¸Šä¿æŒæ¯ä¸ªæ¨¡å‹å‰¯æœ¬çš„å¯è®­ç»ƒæ€§ã€‚äº‹å®è¯æ˜ï¼Œå…·æœ‰è¾ƒå¤§æ¿€æ´»å¤§å°çš„å±‚çš„å†…å­˜å ç”¨å ç”¨äº†ä¸æˆæ¯”ä¾‹çš„GPUå†…å­˜ã€‚é€šè¿‡åœ¨è¿™äº›å±‚ä¹‹ä¸Šçœç•¥æ‰¹å½’ä¸€åŒ–(BN)ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå¤§å¹…å¢åŠ Inceptionå—çš„æ€»æ•°ã€‚æˆ‘ä»¬å¸Œæœ›ï¼Œéšç€è®¡ç®—èµ„æºçš„æ›´å¥½åˆ©ç”¨ï¼Œè¿™ç§æƒè¡¡å°†å˜å¾—ä¸å¿…è¦ã€‚

Figure 3. The schema for stem of the pure Inception-v4 and Inception-ResNet-v2 networks. This is the input part of those networks. Cf. Figures 9 and 15 
å›¾3.çº¯Inception-v4å’ŒIncepton-ResNet-v2ç½‘ç»œçš„ä¸»å¹²æ¨¡å¼ã€‚è¿™æ˜¯è¿™äº›ç½‘ç»œçš„è¾“å…¥éƒ¨åˆ†ã€‚å‚è§å›¾9å’Œå›¾15

Figure 4. The schema for 35 Ã— 35 grid modules of the pure Inception-v4 network. This is the Inception-A block of Figure 9. 
å›¾4.çº¯Inception-v4ç½‘ç»œçš„35Ã—35ç½‘æ ¼æ¨¡å—çš„æ¨¡å¼ã€‚è¿™æ˜¯å›¾9ä¸­çš„Inception-AåŒºå—ã€‚

Figure 5. The schema for 17 Ã— 17 grid modules of the pure Inception-v4 network. This is the Inception-B block of Figure 9. 
å›¾5.çº¯Inception-v4ç½‘ç»œ17Ã—17ç½‘æ ¼æ¨¡å—çš„æ¨¡å¼ã€‚è¿™æ˜¯å›¾9ä¸­çš„Inception-BåŒºå—ã€‚

Figure 6. The schema for 8Ã—8 grid modules of the pure Inceptionv4 network. This is the Inception-C block of Figure 9
å›¾6.çº¯Inceptionv4ç½‘ç»œçš„8Ã—8ç½‘æ ¼æ¨¡å—çš„æ¨¡å¼ã€‚è¿™æ˜¯å›¾9çš„Inception-Cå—

Figure 7. The schema for 35 Ã— 35 to 17 Ã— 17 reduction module. Different variants of this blocks (with various number of filters) are used in Figure 9, and 15 in each of the new Inception(-v4, -ResNet-v1, -ResNet-v2) variants presented in this paper. The k, l, m, n numbers represent filter bank sizes which can be looked up in Table 1. 
å›¾7.35Ã—35è‡³17Ã—17ç®€åŒ–æ¨¡å—çš„ç¤ºæ„å›¾ã€‚å›¾9ä¸­ä½¿ç”¨äº†æ­¤å—çš„ä¸åŒå˜ä½“(å…·æœ‰ä¸åŒæ•°é‡çš„å·ç§¯æ ¸)ï¼Œæœ¬æ–‡ä¸­æå‡ºçš„æ¯ä¸ªæ–°Inceptionå˜ä½“(-v4ã€-ResNet-v1ã€-ResNet-v2)ä¸­ä½¿ç”¨äº†15ä¸ªå˜ä½“ã€‚kã€lã€mã€næ•°å­—è¡¨ç¤ºå·ç§¯æ ¸ç»„å¤§å°ï¼Œå¯åœ¨è¡¨1ä¸­æŸ¥æ‰¾ã€‚

Figure 8. The schema for 17 Ã— 17 to 8 Ã— 8 grid-reduction module. This is the reduction module used by the pure Inception-v4 network in Figure 9.
å›¾8.17Ã—17è‡³8Ã—8ç½‘æ ¼ç¼©å‡æ¨¡å—çš„ç¤ºæ„å›¾ã€‚è¿™æ˜¯å›¾9ä¸­çº¯Inception-v4ç½‘ç»œä½¿ç”¨çš„ç®€åŒ–æ¨¡å—ã€‚

Figure 9. The overall schema of the Inception-v4 network. For the detailed modules, please refer to Figures 3, 4, 5, 6, 7 and 8 for the detailed structure of the various components. 
å›¾9.Inception-v4ç½‘ç»œçš„æ€»ä½“æ¶æ„ã€‚å¯¹äºè¯¦ç»†çš„æ¨¡å—ï¼Œè¯·å‚è€ƒå›¾3ã€4ã€5ã€6ã€7å’Œ8äº†è§£å„ä¸ªç»„ä»¶çš„è¯¦ç»†ç»“æ„ã€‚

Figure 10. The schema for 35 Ã— 35 grid (Inception-ResNet-A) module of Inception-ResNet-v1 network.
å›¾10.Inception-ResNet-v1ç½‘ç»œçš„35Ã—35ç½‘æ ¼(Incepton-ResNet-A)æ¨¡å—çš„æ¨¡å¼ã€‚

Figure 11. The schema for 17 Ã— 17 grid (Inception-ResNet-B) module of Inception-ResNet-v1 network
å›¾11.Inception-ResNet-v1ç½‘ç»œ17Ã—17ç½‘æ ¼(Incepton-ResNet-B)æ¨¡å—çš„æ¨¡å¼

Figure 12. â€œReduction-Bâ€ 17Ã—17 to 8Ã—8 grid-reduction module. This module used by the smaller Inception-ResNet-v1 network in Figure 15. 
å›¾12â€œReduction Bâ€17Ã—17è‡³8Ã—8ç½‘æ ¼ç¼©å‡æ¨¡å—ã€‚å›¾15ä¸­è¾ƒå°çš„Inception-ResNet-v1ç½‘ç»œä½¿ç”¨è¯¥æ¨¡å—ã€‚

Figure 13. The schema for 8Ã—8 grid (Inception-ResNet-C) module of Inception-ResNet-v1 network. 
å›¾13.Inception-ResNet-v1ç½‘ç»œçš„8Ã—8ç½‘æ ¼(Incepton-ResNet-C)æ¨¡å—æ¨¡å¼ã€‚

Figure 14. The stem of the Inception-ResNet-v1 network.
å›¾14.Inception-ResNet-v1ç½‘ç»œçš„ä¸»å¹²ã€‚

Figure 15. Schema for Inception-ResNet-v1 and InceptionResNet-v2 networks. This schema applies to both networks but the underlying components differ. Inception-ResNet-v1 uses the blocks as described in Figures 14, 10, 7, 11, 12 and 13. InceptionResNet-v2 uses the blocks as described in Figures 3, 16, 7,17, 18 and 19. The output sizes in the diagram refer to the activation vector tensor shapes of Inception-ResNet-v1. 
å›¾15. Inception-ResNet-v1å’ŒInceptonResNet-v2ç½‘ç»œçš„æ¨¡å¼ã€‚æ­¤æ¨¡å¼é€‚ç”¨äºä¸¤ä¸ªç½‘ç»œï¼Œä½†åº•å±‚ç»„ä»¶ä¸åŒã€‚Inception-ResNet-v1ä½¿ç”¨å›¾14ã€10ã€7ã€11ã€12å’Œ13ä¸­æ‰€ç¤ºçš„å—ã€‚InceptonResNet-v2ä½¿ç”¨å›¾3ã€16ã€7ã€17ã€18å’Œ19ä¸­æ‰€è¿°çš„å—ã€‚å›¾ä¸­çš„è¾“å‡ºå¤§å°å‚è€ƒInception-ResNet-v1çš„æ¿€æ´»å‘é‡å¼ é‡å½¢çŠ¶ã€‚

Figure 16. The schema for 35 Ã— 35 grid (Inception-ResNet-A) module of the Inception-ResNet-v2 network. 
å›¾16.Inception-ResNet-v2ç½‘ç»œçš„35Ã—35ç½‘æ ¼(Incepton-ResNet-A)æ¨¡å—æ¨¡å¼ã€‚

Figure 17. The schema for 17 Ã— 17 grid (Inception-ResNet-B) module of the Inception-ResNet-v2 network. 
å›¾17.Inception-ResNet-v2ç½‘ç»œ17Ã—17ç½‘æ ¼(Incepton-ResNet-B)æ¨¡å—çš„æ¨¡å¼ã€‚

Figure 18. The schema for 17 Ã— 17 to 8 Ã— 8 grid-reduction module. Reduction-B module used by the wider Inception-ResNet-v1 network in Figure 15. 
å›¾18. 17Ã—17è‡³8Ã—8ç½‘æ ¼ç¼©å‡æ¨¡å—çš„ç¤ºæ„å›¾ã€‚å›¾15ä¸­æ›´å®½çš„Inception-ResNet-v1ç½‘ç»œä½¿ç”¨çš„Reduction-Bæ¨¡å—ã€‚

Figure 19. The schema for 8Ã—8 grid (Inception-ResNet-C) module of the Inception-ResNet-v2 network.
å›¾19.Inception-ResNet-v2ç½‘ç»œçš„8Ã—8ç½‘æ ¼(Incepton-ResNet-C)æ¨¡å—æ¨¡å¼ã€‚

Table 1. The number of filters of the Reduction-A module for the three Inception variants presented in this paper. The four numbers in the colums of the paper parametrize the four convolutions of Figure 7
è¡¨1.æœ¬æ–‡ä»‹ç»çš„ä¸‰ç§Inceptionå˜ä½“çš„Reduction-Aæ¨¡å—çš„å·ç§¯æ ¸æ•°é‡ã€‚è®ºæ–‡åˆ—ä¸­çš„å››ä¸ªæ•°å­—å°†å›¾7çš„å››ä¸ªå·ç§¯å‚æ•°åŒ–

Figure 20. The general schema for scaling combined Inceptionresnet moduels. We expect that the same idea is useful in the general resnet case, where instead of the Inception block an arbitrary subnetwork is used. The scaling block just scales the last linear activations by a suitable constant, typically around 0.1.
å›¾20.æ‰©å±•ç»„åˆInceptionresnetæ¨¡å‹çš„ä¸€èˆ¬æ¨¡å¼ã€‚æˆ‘ä»¬å¸Œæœ›åœ¨ä¸€èˆ¬çš„resnetæƒ…å†µä¸‹ï¼Œä½¿ç”¨ä»»æ„å­ç½‘ä»£æ›¿Inceptionå—ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ç›¸åŒçš„æƒ³æ³•ã€‚ç¼©æ”¾å—åªæŒ‰é€‚å½“çš„å¸¸æ•°ç¼©æ”¾æœ€åçš„çº¿æ€§æ¿€æ´»ï¼Œé€šå¸¸çº¦ä¸º0.1ã€‚

### 3.3. Scaling of the Residuals
Also we found that if the number of filters exceeded 1000, the residual variants started to exhibit instabilities and the network has just â€œdiedâ€ early in the training, meaning that the last layer before the average pooling started to produce only zeros after a few tens of thousands of iterations. This could not be prevented, neither by lowering the learning rate, nor by adding an extra batch-normalization to this layer.

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‘ç°ï¼Œå¦‚æœå·ç§¯æ ¸çš„æ•°é‡è¶…è¿‡1000ï¼Œåˆ™æ®‹å·®å˜é‡å¼€å§‹è¡¨ç°å‡ºä¸ç¨³å®šæ€§ï¼Œç½‘ç»œåœ¨è®­ç»ƒçš„æ—©æœŸå°±å·²ç»â€œæ­»äº¡â€ï¼Œè¿™æ„å‘³ç€å¹³å‡æ± åŒ–ä¹‹å‰çš„æœ€åä¸€å±‚åœ¨å‡ ä¸‡æ¬¡è¿­ä»£åå¼€å§‹åªäº§ç”Ÿé›¶ã€‚è¿™æ˜¯æ— æ³•é¿å…çš„ï¼Œæ— è®ºæ˜¯é€šè¿‡é™ä½å­¦ä¹ ç‡ï¼Œè¿˜æ˜¯é€šè¿‡å‘è¯¥å±‚æ·»åŠ é¢å¤–çš„æ‰¹å½’ä¸€åŒ–(BN)ã€‚

We found that scaling down the residuals before adding them to the previous layer activation seemed to stabilize the training. In general we picked some scaling factors between 0.1 and 0.3 to scale the residuals before their being added to the accumulated layer activations (cf. Figure 20).

æˆ‘ä»¬å‘ç°ï¼Œåœ¨å°†æ®‹å·®æ·»åŠ åˆ°å‰ä¸€å±‚æ¿€æ´»ä¹‹å‰ï¼Œç¼©å°æ®‹å·®çš„æ¯”ä¾‹ä¼¼ä¹å¯ä»¥ç¨³å®šè®­ç»ƒã€‚é€šå¸¸ï¼Œæˆ‘ä»¬é€‰æ‹©0.1åˆ°0.3ä¹‹é—´çš„ä¸€äº›æ¯”ä¾‹å› å­ï¼Œä»¥åœ¨å°†æ®‹å·®æ·»åŠ åˆ°ç´¯ç§¯çš„å±‚æ¿€æ´»ä¸­ä¹‹å‰å¯¹å…¶è¿›è¡Œç¼©æ”¾(å‚è§å›¾20)ã€‚

A similar instability was observed by He et al. in [5] in the case of very deep residual networks and they suggested a two-phase training where the first â€œwarm-upâ€ phase is done with very low learning rate, followed by a second phase with high learning rata. We found that if the number of filters is very high, then even a very low (0.00001) learning rate is not sufficient to cope with the instabilities and the training with high learning rate had a chance to destroy its effects. We found it much more reliable to just scale the residuals.

Heet al åœ¨[5]ä¸­å‘ç°ï¼Œå¯¹äºéå¸¸æ·±çš„æ®‹å·®ç½‘ç»œï¼Œä¹Ÿå­˜åœ¨ç±»ä¼¼çš„ä¸ç¨³å®šæ€§ï¼Œä»–ä»¬å»ºè®®è¿›è¡Œä¸¤é˜¶æ®µè®­ç»ƒï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªâ€œçƒ­èº«â€é˜¶æ®µçš„å­¦ä¹ ç‡éå¸¸ä½ï¼Œç„¶åæ˜¯ç¬¬äºŒä¸ªå­¦ä¹ ç‡è¾ƒé«˜çš„é˜¶æ®µã€‚æˆ‘ä»¬å‘ç°ï¼Œå¦‚æœå·ç§¯æ ¸çš„æ•°é‡éå¸¸é«˜ï¼Œé‚£ä¹ˆå³ä½¿æ˜¯éå¸¸ä½çš„(0.00001)å­¦ä¹ ç‡ä¹Ÿä¸è¶³ä»¥åº”å¯¹ä¸ç¨³å®šæ€§ï¼Œé«˜å­¦ä¹ ç‡çš„è®­ç»ƒä¹Ÿæœ‰å¯èƒ½ç ´åå…¶æ•ˆæœã€‚æˆ‘ä»¬å‘ç°ï¼Œåªè®¡ç®—æ®‹å·®æ›´å¯é ã€‚

Even where the scaling was not strictly necessary, it never seemed to harm the final accuracy, but it helped to stabilize the training.

å³ä½¿åœ¨ä¸¥æ ¼æ„ä¹‰ä¸Šä¸éœ€è¦ç¼©æ”¾çš„æƒ…å†µä¸‹ï¼Œå®ƒä¼¼ä¹ä¹Ÿä¸ä¼šå½±å“æœ€ç»ˆç²¾åº¦ï¼Œä½†å®ƒæœ‰åŠ©äºç¨³å®šè®­ç»ƒã€‚

## 4. Training Methodology
We have trained our networks with stochastic gradient utilizing the TensorFlow [1] distributed machine learning system using 20 replicas running each on a NVidia Kepler GPU. Our earlier experiments used momentum [13] with a decay of 0.9, while our best models were achieved using RMSProp [16] with decay of 0.9 and  = 1.0. We used a learning rate of 0.045, decayed every two epochs using an exponential rate of 0.94. Model evaluations are performed using a running average of the parameters computed over time.

æˆ‘ä»¬åˆ©ç”¨TensorFlow[1]åˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œåœ¨NVidia Kepler GPUä¸Šè¿è¡Œ20ä¸ªå‰¯æœ¬ï¼Œåˆ©ç”¨éšæœºæ¢¯åº¦è®­ç»ƒç½‘ç»œã€‚æˆ‘ä»¬æ—©æœŸçš„å®éªŒä½¿ç”¨åŠ¨é‡[13]ï¼Œè¡°å‡ä¸º0.9ï¼Œè€Œæˆ‘ä»¬çš„æœ€ä½³æ¨¡å‹æ˜¯ä½¿ç”¨RMSProp[16]ï¼Œè¡°å‡0.9ä¸”=1.0ã€‚æˆ‘ä»¬ä½¿ç”¨çš„å­¦ä¹ ç‡ä¸º0.045ï¼Œæ¯ä¸¤ä¸ªå‘¨æœŸè¡°å‡ä¸€æ¬¡ï¼ŒæŒ‡æ•°ç‡ä¸º0.94ã€‚æ¨¡å‹è¯„ä¼°æ˜¯ä½¿ç”¨éšæ—¶é—´è®¡ç®—çš„å‚æ•°çš„è¿è¡Œå¹³å‡å€¼è¿›è¡Œçš„ã€‚

Figure 21. Top-1 error evolution during training of pure Inceptionv3 vs a residual network of similar computational cost. The evaluation is measured on a single crop on the non-blacklist images of the ILSVRC-2012 validation set. The residual model was training much faster, but reached slightly worse final accuracy than the traditional Inception-v3.

å›¾21.çº¯Inceptionv3è®­ç»ƒæœŸé—´Top-1é”™è¯¯æ¼”åŒ–ä¸è®¡ç®—æˆæœ¬ç›¸ä¼¼çš„æ®‹å·®ç½‘ç»œã€‚è¯„ä¼°æ˜¯åœ¨ILSVRC-2012éªŒè¯é›†çš„éé»‘åå•å›¾åƒä¸Šçš„å•å‰ªè£ä¸Šæµ‹é‡çš„ã€‚æ®‹å·®æ¨¡å‹çš„è®­ç»ƒé€Ÿåº¦å¿«å¾—å¤šï¼Œä½†æœ€ç»ˆç²¾åº¦æ¯”ä¼ ç»ŸInception-v3ç¨å·®ã€‚

## 5. Experimental Results
First we observe the top-1 and top-5 validation-error evolution of the four variants during training. After the experiment was conducted, we have found that our continuous evaluation was conducted on a subset of the validation set which omitted about 1700 blacklisted entities due to poor bounding boxes. It turned out that the omission should have been only performed for the CLSLOC benchmark, but yields somewhat incomparable (more optimistic) numbers when compared to other reports including some earlier reports by our team. The difference is about 0.3% for top-1 error and about 0.15% for the top-5 error. However, since the differences are consistent, we think the comparison between the curves is a fair one.

é¦–å…ˆï¼Œæˆ‘ä»¬è§‚å¯Ÿäº†è®­ç»ƒæœŸé—´å››ç§å˜ä½“çš„top-1å’Œtop-5éªŒè¯é”™è¯¯æ¼”å˜ã€‚åœ¨è¿›è¡Œå®éªŒä¹‹åï¼Œæˆ‘ä»¬å‘ç°ï¼Œæˆ‘ä»¬å¯¹éªŒè¯é›†çš„ä¸€ä¸ªå­é›†è¿›è¡Œäº†æŒç»­è¯„ä¼°ï¼Œç”±äºè¾¹ç•Œæ¡†ä¸å¥½ï¼Œè¯¥éªŒè¯é›†é—æ¼äº†çº¦1700ä¸ªè¢«åˆ—å…¥é»‘åå•çš„å®ä½“ã€‚äº‹å®è¯æ˜ï¼Œè¿™ä¸€é—æ¼æœ¬åº”ä»…é’ˆå¯¹CLSLOCåŸºå‡†è¿›è¡Œï¼Œä½†ä¸å…¶ä»–æŠ¥å‘Š(åŒ…æ‹¬æˆ‘ä»¬å›¢é˜Ÿçš„ä¸€äº›æ—©æœŸæŠ¥å‘Š)ç›¸æ¯”ï¼Œå¾—å‡ºäº†ä¸€äº›æ— æ³•æ¯”æ‹Ÿçš„(æ›´ä¹è§‚çš„)æ•°å­—ã€‚top-1ä¸ªé”™è¯¯çš„å·®å€¼çº¦ä¸º0.3%ï¼Œtop-5ä¸ªé”™è¯¯çš„ç›¸å·®çº¦ä¸º0.15%ã€‚ç„¶è€Œï¼Œç”±äºå·®å¼‚æ˜¯ä¸€è‡´çš„ï¼Œæˆ‘ä»¬è®¤ä¸ºæ›²çº¿ä¹‹é—´çš„æ¯”è¾ƒæ˜¯å…¬å¹³çš„ã€‚

On the other hand, we have rerun our multi-crop and ensemble results on the complete validation set consisting of 50000 images. Also the final ensemble result was also performed on the test set and sent to the ILSVRC test server for validation to verify that our tuning did not result in an over-fitting. We would like to stress that this final validation was done only once and we have submitted our results only twice in the last year: once for the BN-Inception paper and later during the ILSVR-2015 CLSLOC competition, so we believe that the test set numbers constitute a true estimate of the generalization capabilities of our model.

å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬å¯¹åŒ…å«50000å¼ å›¾åƒçš„å®Œæ•´éªŒè¯é›†é‡æ–°è¿è¡Œäº†å¤šè£å‰ªå’Œé›†æˆç»“æœã€‚æ­¤å¤–ï¼Œè¿˜å¯¹æµ‹è¯•é›†æ‰§è¡Œäº†æœ€ç»ˆçš„é›†æˆç»“æœï¼Œå¹¶å°†å…¶å‘é€åˆ°ILSVRCæµ‹è¯•æœåŠ¡å™¨è¿›è¡ŒéªŒè¯ï¼Œä»¥éªŒè¯æˆ‘ä»¬çš„è°ƒæ•´æ²¡æœ‰å¯¼è‡´è¿‡åº¦æ‹Ÿåˆã€‚æˆ‘ä»¬æƒ³å¼ºè°ƒçš„æ˜¯ï¼Œè¿™ä¸€æœ€ç»ˆéªŒè¯åªè¿›è¡Œäº†ä¸€æ¬¡ï¼Œå»å¹´æˆ‘ä»¬åªæäº¤äº†ä¸¤æ¬¡ç»“æœï¼šä¸€æ¬¡ç”¨äºBN-Inceptionæ–‡ä»¶ï¼Œéšååœ¨ILSVR-2015 CLSLOCç«èµ›ä¸­æäº¤ï¼Œå› æ­¤æˆ‘ä»¬è®¤ä¸ºæµ‹è¯•é›†æ•°é‡æ˜¯å¯¹æˆ‘ä»¬æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„çœŸå®ä¼°è®¡ã€‚

Finally, we present some comparisons, between various versions of Inception and Inception-ResNet. The models Inception-v3 and Inception-v4 are deep convolutional net works not utilizing residual connections while InceptionResNet-v1 and Inception-ResNet-v2 are Inception style networks that utilize residual connections instead of filter concatenation.

æœ€åï¼Œæˆ‘ä»¬ç»™å‡ºäº†ä¸åŒç‰ˆæœ¬çš„Inceptionå’ŒInception-ResNetä¹‹é—´çš„ä¸€äº›æ¯”è¾ƒã€‚æ¨¡å‹Inception-v3å’ŒIncepton-v4æ˜¯æ·±åº¦å·ç§¯ç½‘ç»œï¼Œä¸ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œè€ŒInception ResNet-v1å’ŒIncept ResNet-v2æ˜¯Inceptionç±»å‹çš„ç½‘ç»œï¼Œä½¿ç”¨æ®‹å·®è¿æ¥è€Œä¸æ˜¯å·ç§¯æ ¸ä¸²è”ã€‚

Figure 22. Top-5 error evolution during training of pure Inceptionv3 vs a residual Inception of similar computational cost. The evaluation is measured on a single crop on the non-blacklist images of the ILSVRC-2012 validation set. The residual version has trained much faster and reached slightly better final recall on the validation set. 
å›¾22.çº¯Inceptionv3è®­ç»ƒæœŸé—´Top-5é”™è¯¯æ¼”å˜ä¸ç±»ä¼¼è®¡ç®—æˆæœ¬çš„æ®‹å·®Inceptionã€‚è¯„ä¼°æ˜¯åœ¨ILSVRC-2012éªŒè¯é›†çš„éé»‘åå•å›¾åƒä¸Šçš„å•å‰ªè£ä¸Šæµ‹é‡çš„ã€‚æ®‹å·®ç‰ˆæœ¬çš„è®­ç»ƒé€Ÿåº¦å¿«å¾—å¤šï¼Œåœ¨éªŒè¯é›†çš„æœ€ç»ˆå¬å›ç‡ç•¥é«˜ã€‚

Figure 23. Top-1 error evolution during training of pure Inceptionv3 vs a residual Inception of similar computational cost. The evaluation is measured on a single crop on the non-blacklist images of the ILSVRC-2012 validation set. The residual version was training much faster and reached slightly better final accuracy than the traditional Inception-v4.
å›¾23.çº¯Inceptionv3è®­ç»ƒæœŸé—´çš„Top-1é”™è¯¯æ¼”å˜ä¸ç±»ä¼¼è®¡ç®—æˆæœ¬çš„æ®‹å·®Inceptionã€‚è¯„ä¼°æ˜¯åœ¨ILSVRC-2012éªŒè¯é›†çš„éé»‘åå•å›¾åƒä¸Šçš„å•å‰ªè£ä¸Šæµ‹é‡çš„ã€‚æ®‹å·®ç‰ˆæœ¬çš„è®­ç»ƒé€Ÿåº¦è¦å¿«å¾—å¤šï¼Œæœ€ç»ˆç²¾åº¦æ¯”ä¼ ç»Ÿçš„Inception-v4ç¨å¥½ã€‚

Table 2. Single crop - single model experimental results. Reported on the non-blacklisted subset of the validation set of ILSVRC 2012. 
è¡¨2.å•å‰ªè£-å•æ¨¡å‹è¯•éªŒç»“æœã€‚æŠ¥å‘Šäº†ILSVRC 2012éªŒè¯é›†çš„éé»‘åå•å­é›†ã€‚

Table 2 shows the single-model, single crop top-1 and top-5 error of the various architectures on the validation set.

è¡¨2æ˜¾ç¤ºäº†éªŒè¯é›†ä¸­å„ç§æ¶æ„çš„å•æ¨¡å‹ã€å•è£å‰ªtop-1å’Œtop-5é”™è¯¯ã€‚

Table 3. 10/12 crops evaluations - single model experimental results. Reported on the all 50000 images of the validation set of ILSVRC 2012.
è¡¨3. 10/12å‰ªè£è¯„ä¼°-å•æ¨¡å¼è¯•éªŒç»“æœã€‚æŠ¥å‘Šäº†ILSVRC 2012éªŒè¯é›†çš„æ‰€æœ‰50000å¼ å›¾åƒã€‚

Table 3 shows the performance of the various models with a small number of crops: 10 crops for ResNet as was reported in [5]), for the Inception variants, we have used the 12 crops evaluation as as described in [14].

è¡¨3æ˜¾ç¤ºäº†ä½¿ç”¨å°‘é‡å‰ªè£çš„å„ç§æ¨¡å‹çš„æ€§èƒ½ï¼š[5]ä¸­æŠ¥å‘Šäº†ResNetçš„10ç§å‰ªè£ï¼Œå¯¹äºInceptionå˜ä½“ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†[14]ä¸­æè¿°çš„12ç§å‰ªè£è¯„ä¼°ã€‚

Figure 24. Top-5 error evolution during training of pure Inceptionv4 vs a residual Inception of similar computational cost. The evaluation is measured on a single crop on the non-blacklist images of the ILSVRC-2012 validation set. The residual version trained faster and reached slightly better final recall on the validation set. 

å›¾24.çº¯Inceptionv4è®­ç»ƒæœŸé—´Top-5é”™è¯¯æ¼”å˜ä¸ç±»ä¼¼è®¡ç®—æˆæœ¬çš„æ®‹å·®Inceptionã€‚è¯„ä¼°æ˜¯åœ¨ILSVRC-2012éªŒè¯é›†çš„éé»‘åå•å›¾åƒä¸Šçš„å•å‰ªè£ä¸Šæµ‹é‡çš„ã€‚æ®‹å·®ç‰ˆæœ¬çš„è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œåœ¨éªŒè¯é›†çš„æœ€ç»ˆå¬å›ç‡ç•¥å¥½ã€‚

Figure 25. Top-5 error evolution of all four models (single model, single crop). Showing the improvement due to larger model size.
Although the residual version converges faster, the final accuracy seems to mainly depend on the model size. 
å›¾25.æ‰€æœ‰å››ä¸ªæ¨¡å‹(å•æ¨¡å‹ã€å•å‰ªè£)çš„Top-5è¯¯å·®æ¼”å˜ã€‚æ˜¾ç¤ºç”±äºè¾ƒå¤§çš„æ¨¡å‹å°ºå¯¸è€Œå¸¦æ¥çš„æ”¹è¿›ã€‚å°½ç®¡æ®‹å·®ç‰ˆæœ¬æ”¶æ•›æ›´å¿«ï¼Œä½†æœ€ç»ˆç²¾åº¦ä¼¼ä¹ä¸»è¦å–å†³äºæ¨¡å‹å¤§å°ã€‚

Figure 26. Top-1 error evolution of all four models (single model, single crop). This paints a similar picture as the top-5 evaluation.
å›¾26.æ‰€æœ‰å››ä¸ªæ¨¡å‹(å•æ¨¡å‹ã€å•å‰ªè£)çš„Top-1è¯¯å·®æ¼”å˜ã€‚è¿™ä¸æ’åtop-5çš„è¯„ä¼°ç»“æœç±»ä¼¼ã€‚

Table 4. 144 crops evaluations - single model experimental results.
è¡¨4.144ç§å‰ªè£è¯„ä¼°-å•æ¨¡å¼è¯•éªŒç»“æœã€‚

Table 4 shows the single model performance of the various models using. For residual network the dense evaluation result is reported from [5]. For the inception networks, the 144 crops strategy was used as described in [14].

è¡¨4æ˜¾ç¤ºäº†ä½¿ç”¨çš„å„ç§æ¨¡å‹çš„å•æ¨¡å‹æ€§èƒ½ã€‚å¯¹äºæ®‹å·®ç½‘ç»œï¼Œå¯†é›†è¯„ä¼°ç»“æœè§[5]ã€‚å¯¹äºInceptionç½‘ç»œï¼Œå¦‚[14]æ‰€è¿°ï¼Œä½¿ç”¨äº†144ç§å‰ªè£ç­–ç•¥ã€‚

Table 5. Ensemble results with 144 crops/dense evaluation. Reported on the all 50000 images of the validation set of ILSVRC 2012. For Inception-v4(+Residual), the ensemble consists of one pure Inception-v4 and three Inception-ResNet-v2 models and were evaluated both on the validation and on the test-set. The test-set performance was 3.08% top-5 error verifying that we donâ€™t over- fit on the validation set.
è¡¨5. 144ç§å‰ªè£/å¯†åº¦è¯„ä¼°çš„ç»¼åˆç»“æœã€‚åœ¨ILSVRC 2012éªŒè¯é›†çš„æ‰€æœ‰50000å¼ å›¾åƒä¸Šè¿›è¡Œäº†æŠ¥å‘Šã€‚å¯¹äºInception-v4(+æ®‹å·®)ï¼Œé›†åˆç”±ä¸€ä¸ªçº¯Inception v4å’Œä¸‰ä¸ªIncepton-ResNet-v2æ¨¡å‹ç»„æˆï¼Œå¹¶åœ¨éªŒè¯å’Œæµ‹è¯•é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚æµ‹è¯•é›†æ€§èƒ½ä¸º3.08%çš„top-5ä¸ªé”™è¯¯ï¼ŒéªŒè¯äº†æˆ‘ä»¬æ²¡æœ‰è¿‡åº¦é€‚åˆéªŒè¯é›†ã€‚

Table 5 compares ensemble results. For the pure residual network the 6 models dense evaluation result is reported from [5]. For the inception networks 4 models were ensembled using the 144 crops strategy as described in [14].

è¡¨5æ¯”è¾ƒäº†æ€»ä½“ç»“æœã€‚å¯¹äºçº¯æ®‹å·®ç½‘ç»œï¼Œ[5]æŠ¥å‘Šäº†6ä¸ªæ¨¡å‹çš„å¯†é›†è¯„ä¼°ç»“æœã€‚å¯¹äºInceptionç½‘ç»œï¼Œä½¿ç”¨[14]ä¸­æè¿°çš„144ç§å‰ªè£ç­–ç•¥å°†4ä¸ªæ¨¡å‹æ•´åˆèµ·æ¥ã€‚

## 6. Conclusions
We have presented three new network architectures in detail:
* Inception-ResNet-v1: a hybrid Inception version that has a similar computational cost to Inception-v3 from [15].
* Inception-ResNet-v2: a costlier hybrid Inception version with significantly improved recognition performance.
* Inception-v4: a pure Inception variant without residual connections with roughly the same recognition performance as Inception-ResNet-v2.

æˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä¸‰ç§æ–°çš„ç½‘ç»œæ¶æ„ï¼š
* Inception-ResNet-v1ï¼šä¸€ä¸ªæ··åˆçš„Inceptionç‰ˆæœ¬ï¼Œå…¶è®¡ç®—æˆæœ¬ä¸[15]ä¸­çš„Incept v3ç±»ä¼¼ã€‚
* Inception-ResNet-v2ï¼šæˆæœ¬æ›´é«˜çš„Inceptionæ··åˆç‰ˆï¼Œè¯†åˆ«æ€§èƒ½æ˜¾è‘—æé«˜ã€‚
* Inception-v4ï¼šçº¯Inceptionå˜ä½“ï¼Œæ— æ®‹å·®è¿æ¥ï¼Œè¯†åˆ«æ€§èƒ½ä¸Incepton-ResNet-v2å¤§è‡´ç›¸åŒã€‚

We studied how the introduction of residual connections leads to dramatically improved training speed for the Inception architecture. Also our latest models (with and without residual connections) outperform all our previous networks, just by virtue of the increased model size.

æˆ‘ä»¬ç ”ç©¶äº†æ®‹å·®è¿æ¥çš„å¼•å…¥å¦‚ä½•æ˜¾è‘—æé«˜Inceptionæ¶æ„çš„è®­ç»ƒé€Ÿåº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æœ€æ–°æ¨¡å‹(æœ‰æˆ–æ— æ®‹å·®è¿æ¥)çš„æ€§èƒ½ä¼˜äºæ‰€æœ‰ä»¥å‰çš„ç½‘ç»œï¼Œä»…ä»…æ˜¯å› ä¸ºæ¨¡å‹å°ºå¯¸å¢åŠ äº†ã€‚

## References
1. M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. ManÂ´e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. ViÂ´egas, O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org.
2. J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, A. Senior, P. Tucker, K. Yang, Q. V. Le, et al. Large scale distributed deep networks. In Advances in Neural Information Processing Systems, pages 1223â€“1231, 2012.
3. C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image super-resolution. In Computer Visionâ€“ECCV 2014, pages 184â€“199. Springer, 2014.
4. R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2014.
5. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015.
6. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of The 32nd International Conference on Machine Learning, pages 448â€“456, 2015.
7. A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and L. Fei-Fei. Large-scale video classification with convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1725â€“1732. IEEE, 2014.
8. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097â€“1105, 2012.
9. M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013.
10. J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3431â€“3440, 2015.
11. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. 2014.
12. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
13. I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 1139â€“1147. JMLR Workshop and Conference Proceedings, May 2013.
14. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1â€“9, 2015.
15. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. arXiv preprint arXiv:1512.00567, 2015.
16. T. Tieleman and G. Hinton. Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012. Accessed: 2015- 11-05.
17. A. Toshev and C. Szegedy. Deeppose: Human pose estimation via deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on, pages 1653â€“1660. IEEE, 2014.
18. N. Wang and D.-Y. Yeung. Learning a deep compact image representation for visual tracking. In Advances in Neural Information Processing Systems, pages 809â€“817, 2013.
