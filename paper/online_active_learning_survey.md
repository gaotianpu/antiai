# A survey on online active learning
在线主动学习调查 2023.2.17 https://arxiv.org/abs/2302.08893 


## Abstract
Online active learning is a paradigm in machine learning that aims to select the most informative data points to label from a data stream. The problem of minimizing the cost associated with collecting labeled observations has gained a lot of attention in recent years, particularly in real-world applications where data is only available in an unlabeled form. Annotating each observation can be time-consuming and costly, making it difficult to obtain large amounts of labeled data. To overcome this issue, many active learning strategies have been proposed in the last decades, aiming to select the most informative observations for labeling in order to improve the performance of machine learning models. These approaches can be broadly divided into two categories: static pool-based and stream-based active learning. Pool-based active learning involves selecting a subset of observations from a closed pool of unlabeled data, and it has been the focus of many surveys and literature reviews. However, the growing availability of data streams has led to an increase in the number of approaches that focus on online active learning, which involves continuously selecting and labeling observations as they arrive in a stream. This work aims to provide an overview of the most recently proposed approaches for selecting the most informative observations from data streams in the context of online active learning. We review the various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this area of research. Our review aims to provide a comprehensive and up-to-date overview of the field and to highlight directions for future work.

在线主动学习是机器学习中的一种范式，旨在从数据流中选择信息量最大的数据点进行标注。 最小化与收集标注观察相关的成本的问题近年来引起了很多关注，特别是在数据仅以未标注形式提供的实际应用中。 对每个观察结果进行标注可能既费时又费钱，因此很难获得大量标注数据。 为了克服这个问题，在过去几十年中提出了许多主动学习策略，旨在选择最具信息量的观察结果进行标注，以提高机器学习模型的性能。 这些方法可以大致分为两类：基于静态池和基于流的主动学习。 基于池的主动学习涉及从封闭的未标注数据池中选择观察子集，它一直是许多调查和文献综述的焦点。 然而，数据流的可用性不断提高导致专注于在线主动学习的方法数量增加，这涉及在观察数据到达数据流时不断选择和标注观察数据。 这项工作旨在概述最近提出的方法，用于在在线主动学习的背景下从数据流中选择最具信息量的观察结果。 我们回顾了已提出的各种技术，并讨论了它们的优势和局限性，以及该研究领域存在的挑战和机遇。 我们的评论旨在提供对该领域的全面和最新的概述，并突出未来工作的方向。

Keywords: active learning, data streams, online learning, unlabeled data, query strategy, literature review. 

## 1 Introduction
The deployment of machine learning models in real-world applications is often reliant on the availability of significant amounts of annotated data. While recent advancements in sensor technology have facilitated the collection of larger amounts of data, this data is not always labeled and ready for use in training models. Indeed, the process of obtaining labeled observations for supervised learning models can be cost-prohibitive and timeconsuming, as it often requires quality inspections or manual annotation. In such cases, active learning proves to be a valuable strategy to identify the most informative data points for use in training, thereby reducing the overall cost of labeling and improving the performance of the model.

Over the years, a plethora of active learning approaches have been proposed in the literature, each offering its own benefits and limitations. These approaches seek to strike a balance between the cost of labeling and the quality of the model by selectively choosing the most informative observations for querying. By carefully selecting the most informative observations, active learning helps to minimize the amount of labeled data required and streamlines the learning process, contributing to its overall efficiency. While several surveys have been published on pool-based active learning [1–5], which involves selecting a fixed set of observations from a pool of unlabeled data, the dynamic and sequential nature of many real-world problems often renders these approaches impractical. This has led to growing interest in the online variant of active learning, which involves continuously selecting and labeling observations as they arrive in a stream, allowing for real-time adaptation to changing data distributions. In 2017,

Lughofer [6] provided a comprehensive review of online active learning approaches with a focus on fuzzy models.

However, since then, numerous other online active learning approaches have been proposed, and to the best of our knowledge, no other surveys have been published to synthesize these developments.

The aim of this review is to fill this gap by providing an overview of the most recently developed query strategies for online active learning. We will review various techniques that have been proposed and discuss their strengths and limitations, as well as the challenges and opportunities that exist in this field. In addition, we will provide an 2 overview of evaluation strategies for online active learning algorithms and highlight some real-world applications of online active learning. Finally, we will identify potential future research directions in this area.

The structure of this paper is as follows: in section 2, we provide an overview of active learning, including the main instance selection criteria, an overview of the main active learning scenarios, and the connection between active learning and semi-supervised learning. Section 3 represents the core of the review, with a brief overview of how online active learning approaches have been classified, followed by a detailed description of the state-of-the-art approaches. In section 4, we examine evaluation strategies for online active learning algorithms. Section 5 provides a summary of the most common online active learning methods and highlights potential directions for future research. Finally, section 6 provides conclusions and summarizes the key contributions of the review. 

## 2 Preliminaries on active learning

In supervised learning, we seek to learn a function that can predict the output variable, also known as response, given a set of input variables, also known as covariates. This function is often learned by training a model on a labeled dataset that consists of a large number of input-output pairs. However, obtaining labeled examples is not always straightforward, and it may not be possible or practical to label all the available data. In these cases, active learning can be used to select a subset of the data for labeling in order to improve the performance of the model, when there is a budget constraint on the number of unlabeled observations that can be queried. Indeed, there are many examples of how a classification or regression model can achieve a performance that is similar to what can be achieved when all the labels are available, using only a small fraction of the available observations.

### 2.1 Instance selection criteria
The main challenge in active learning is deciding which data points to label. There are many strategies for selecting data points in active learning, and most of them can be associated with one of these groups:
* Uncertainty-based query strategies: these approaches focus on selecting data points that the model is least confident about, in order to reduce its uncertainty [7,8]. When using classification models, the most widely used query strategy is the margin-based one, where data points close to the decision boundary are selected [9,10].
* Expected error or variance minimization: these strategies estimate the future error or variance, when a newly labeled example is made available, and directly try to minimize it [11,12].
* Expected model change maximization: this strategy involves selecting data points that would have the greatest impact on the estimate of the current model parameters if they were labeled and added to the training set [13].
* Disagreement-based query strategies: these approaches focus on selecting data points where there is disagreement among multiple models or experts [14–17]. One of the most common approaches that use an ensemble of models is query by committee [18–20], which uses an ensemble of models to identify instances where the models have conflicting predictions.
* Diversity- and density-based approaches: these methods exploit the structural information of the instances and try to select data points that are diverse and representative of the overall distribution of the data. One example of this approach is the use of Mahalanobis distance to seek observations that are far from the currently labeled data points [21,22]. Clustering may be applied to label representative data points [23–25], and graphbased methods can be employed to explore the structure information of labeled and unlabeled data points [26] or to build upon the semi-supervised label propagation strategy [27].
* Hybrid strategies: these are active learning algorithms that combine multiple instance selection criteria [28,29].

For example, by combining margin-based sampling with clustering the learner can select the most uncertain observations within different areas of the input space.

By considering these different strategies, one can select the most appropriate approach for a given problem based on the characteristics of the data and the specific requirements of the application.

### 2.2 Active learning scenarios
Active learning can be broadly categorized into three macro scenarios, based on how the unlabeled instances are supplied to the learner and then selected to be labeled by an oracle. Regardless of the particular query strategy being employed, these macro scenarios provide a framework for understanding the flow of information and the decision-making steps involved in active learning. These scenarios serve as a high-level categorization of different 3 methods for approaching the active learning problem, each with its own set of advantages and disadvantages depending on the specific use case. Understanding these macro scenarios is crucial for selecting the appropriate active learning technique for a particular problem and for comparing different active learning algorithms. In the next subsections, each of the three macro scenarios will be discussed.

#### 2.2.1 Membership query synthesis active learning
This scenario represents the case when the learner is given complete freedom to ask for the label of any data point belonging to the input space or for a synthetically generated one. Some examples of membership query synthesis active learning include image classification, where the learner can generate modified versions of existing images to be labeled, or object detection, where the learner can generate new instances by combining and transforming existing instances. In natural language processing (NLP) tasks such as text classification or sentiment analysis, the learner might generate synthetic examples in the form of sentences or paragraphs that cover a wider range of variations in the language. Also, in speech recognition, the learner might generate synthetic speech samples in different accents, pronunciations, or speaking styles in order to improve the recognition accuracy. However, as highlighted by Baum et al. [30] and Settles [2], the main drawback of this strategy is that it could generate unlabeled examples for which no labels can be associated by a human annotator (e.g., a mixture between a number and a letter). A general flowchart for this scenario is reported in Figure 1, where the scheme is repeated until a budget constraint on the requested labels is met, or a stopping criterion on the achieved performance is satisfied.

Figure 1. Membership query synthesis active learning.

In the context of deep active learning [31], the membership query synthesis scenario can be addressed by using generative models. For instance, generative adversarial networks (GANs) have been used to generate additional instances from the input space that may provide more informative labels for the learner [32]. This can be done by using GANs for data augmentation, as GANs are capable of generating diverse and high-quality instances [33].

Another approach is to combine the use of variational autoencoders (VAEs) [34] and Bayesian data augmentation, as demonstrated by Tran et al. [35,36]. The authors used VAEs to generate instances from the disagreement regions between multiple models, and Bayesian data augmentation to incorporate the uncertainty of the generated instances in the learning process.

#### 2.2.2 Pool-based active learning
Pool-based active learning is one of the most widely studied scenarios in the machine learning literature, and its goal is to select the most informative subset of observations from a closed, static set of unlabeled data points. The majority of the proposed pool-based active learning approaches have been developed for classification tasks [13], with image classification being a common application in computer vision [37], as manually labeling large image datasets can be a challenging task.

Figure 2. Pool-based active learning.

Labeled data

Train/update model

Synthetically generate instance(s)

Ask for the label(s)

Model

Train/update model

Rank observations

Ask for the label(s)

Labeled data

Unlabeled data

Select top ! instance(s)

Model 4

The flowchart in Figure 2 provides an overview of pool-based active learning sampling schemes, where � represents the number of unlabeled instances whose label is queried at each round. Traditional machine learning models that do not require substantial computational resources to train are typically associated with a choice of � equal to one [38]. This allows a timely update of the instance selection criteria, avoiding the redundant labeling of similar data points. However, larger values of � have also been used in practice, such as the analysis performed by Ge [21] for values ranging from 5 to 30 or the approach used by Cai et al. [13] to add 3% of the total number of observations to the training set each time. Using a higher � value may be more practical when working with large models, as repeated training can be computationally expensive and challenging. To this extent, batch mode active learning is generally considered to be a more efficient and effective option for image classification or detection tasks compared to the one-by-one query strategy, as the latter can be resource-intensive and time-consuming when working with large neural networks [31]. This is because re-training the model with just one new data point with high input dimensionality may not result in significant improvement [31]. In general, the choice of � may be problem- or model-specific, as it represents a trade-off between computational efficiency and the risk of querying redundant labels.

To enhance pool-based active learning, many approaches combine uncertainty-based instance selection criteria with acquisition functions such as entropy [39,40], mutual information [41], or variation ratio [42]. Entropy is commonly used as an acquisition function in active learning because it provides a way to measure the uncertainty of the model predictions for a given data point. The entropy of a probability distribution is a measure of the amount of disorder or randomness in the distribution. In the context of active learning, the entropy of a model's predicted class probabilities for a data point can be used as a measure of the model's uncertainty about the correct class label for that data point. Acquiring examples with the highest uncertainty is one way to select data points for annotation, but it is not the only way. Using an acquisition function like entropy allows to balance exploration and exploitation, where the model can select instances with high uncertainty but also instances where the model is confident but uncertain among similar classes. Additionally, using entropy as an acquisition function allows you to trade off the exploration of different regions of the feature space by exploiting the information you already have. For example, you may want to focus more on instances where the model is uncertain among similar classes rather than instances where the model is uncertain among very dissimilar classes. Mutual information and variation ratio can also be used on the predictions obtained with the current model, in order to seek a diverse set of data points for which the predictions are the most uncertain. For a more comprehensive discussion on pool-based active learning, readers are referred to the surveys [1–5].

#### 2.2.3. Online active learning
This scenario is also known as stream-based active learning or sequential selective sampling. In this type of active learning, we cannot greedily select the most informative observations from a static pool, as the instances are generated in a continuous stream and cannot be stored in their entirety before a decision is made. This is similar to the famous statistical puzzle known as the secretary problem [43], where a hiring manager must make a hiring decision for each applicant as they are interviewed, without the benefit of seeing all applicants first. In general, online active learning is a crucial scenario for various real-world applications where the ability to make a sampling decision in real-time is of utmost importance. A few examples are:
* Chemical or manufacturing processes: in these applications, a learner is tasked with predicting the quality of small parts or components but may only have a short timeframe to make the sampling decision, as the parts might undergo changes that make their labels no longer traceable. Also, tasks like predictive maintenance and visual inspection might benefit from a real-time selection of new examples to be labeled and included in the training set.
* Video streaming and clinical trials: in these cases, a decision must be made on the fly, as users arrive or volunteers appear sequentially, and there may not be enough time to accumulate a pool of potential users or patients [44].
* Text classification: in NLP, online active learning can be used for tasks such as sentiment analysis and spam detection, where the learner continuously learns from new incoming data points which need to be labeled to update the model in real-time and improve accuracy.
* Fraud detection: to effectively detect fraudulent activities, the learner must continuously select new examples to label so that it can continuously update its decision-making process.
* Online customer service: online customer service agents can use online active learning to improve their performance by continuously learning from customer interactions. To do this, the learner must continuously select new examples to label, so that it can predict the best response based on past interactions and improve its accuracy over time. 5
* Marketing: online active learning can also be applied in the field of marketing to select informative examples in real-time and continuously optimize customer targeting and personalization.

Figure 3. Single-pass online active learning.

One of the defining features of online active learning strategies is their data processing capabilities. Figure 3 and

Figure 4 provide a visual representation of the two main approaches, single-pass and window-based. Single-pass algorithms observe and evaluate each incoming data point on the fly, whereas window-based algorithms, also referred to as batch-based methods, observe a fixed-size chunk of data at a time. In this approach, the learner evaluates the entire batch of data and selects the top � observations as the most informative ones to be labeled.

This approach is referred to as best-out-of-window sampling. The specific value of � and the dimensionality of the buffer can vary based on the storage capabilities of the system and the computational time required to update the model. Window-based methods are useful in situations where data is generated in large quantities and the algorithm does not have a tight constraint on the time available for decision-making. In contrast, single-pass methods are necessary when the algorithm needs to make a decision immediately after observing a specific data point.

Figure 4. Window- or batch-based online active learning.

Another critical property in the design of an effective online active learning strategy is the assumption made about the data stream distribution. One important difference to consider is whether the data stream is stationary or drifting. A stationary data stream is characterized by a stable data generating process where the statistical properties of the data distribution that remain constant over time. Conversely, a drifting data stream is marked by changing statistical properties of the data distribution over time, potentially due to alterations in the underlying data generating process. The distinction between stationary and drifting data streams is significant because it affects the performance of the active learning strategies. Online active learning strategies that have been developed for stationary data streams may lead to suboptimal performance when applied to drifting data streams. This is because concept drift can alter the scale of the informativeness measure of unlabeled data points or even urge a complete change of the model, with the acquisition of more observations to accommodate the new concept.

Therefore, it is important to accurately assess the nature of the data stream distribution in the design of an active learning strategy. A failure to do so can result in a suboptimal performance and a reduced ability to effectively leverage the strengths of active learning. Another important property to consider when designing an active learning strategy is the label delay or verification latency. This refers to the time needed by the oracle to provide the label when it is requested by the learner. In some cases, there may be a delay � in the oracle providing the label after it has been requested. This property must be taken into account when designing a sampling strategy as there may be redundant label requests for similar instances if this issue is not properly addressed. Label delay can be classified into null latency, intermediate latency, or extreme latency [45]. The case with null latency, or immediate availability of the label upon request, is commonly used in the stream mining community, but may not be realistic for many practical applications. Extreme latency, where labels are never made available to the learner, is closer to an

Train/update model

Observe an unlabeled data point

Ask for the label Labeled data

Data stream

Is it useful?

Discard the observation

Yes

No

Model

Train/update model

Observe an unlabeled data point

Ask for the

Labeled data label(s)

Data stream

Is it full?

Yes

No

Model

Buffer

Select top ! instance(s) 6 unsupervised learning task. Intermediate latency assumes a delay 0 < � < ∞ in the availability of the labels from the oracle.

Finally, the training efficiency of the online active learning algorithms should also be taken into consideration.

There are two main training approaches in active learning, incremental training and complete re-training.

Incremental training involves updating model parameters with a small batch of new data, without starting the training process from scratch [46–49]. This approach allows the model to learn from new data while preserving its existing knowledge. This can be achieved through fine-tuning the model parameters with the new data, or by using techniques such as elastic weight consolidation, which prevent previous knowledge from being erased. Complete retraining, on the other hand, involves training a new model from scratch using the entire labeled data collected so far. This approach discards the previous knowledge of the model and starts anew, which may result in the loss of knowledge learned from previous data. Complete re-training is typically used when the amount of new data is substantial, the previous model is no longer relevant, or when the model architecture needs to be altered. It is important to note that the choice of training approach in online active learning algorithms can have a significant impact on the overall performance and effectiveness of the model.

### 2.3 Connection between active learning and semi-supervised learning
Semi-supervised learning is a field of research that is closely related to active learning, as both methods are developed to deal with limited labeled data. While active learning aims to minimize the amount of labeled data required to train a model, semi-supervised learning is a technique that trains a model using a combination of labeled and unlabeled data. Active learning can be considered a special case of semi-supervised learning, as it allows the model to actively select which data points it wants to be labeled, rather than relying on a fixed set of labeled data.

In the context of online learning, Kulkarni et al. [50] conducted a study that provided an overview of semi-supervised learning techniques for classifying data streams. These techniques do not address the primary question of active learning, which is “when to query”, but they are useful in exploiting the information contained in the unlabeled data points and in addressing issues related to model update and retraining in limited labeled data environments.

It is also worth noting that semi-supervised learning can be used in combination with active learning to improve the data selection strategy. By leveraging the strengths of both methods, it is possible to achieve better performance and more efficient learning compared to using either method alone.

Semi-supervised learning approaches can be distinguished into three categories, unsupervised preprocessing, wrapper methods, and graph-based methods. Unsupervised preprocessing refers to the use of unsupervised learning techniques, such as dimensionality reduction, clustering, or feature extraction, to preprocess the entire dataset, labeled and unlabeled, before it is fed to the supervised model. The goal is to transform the data into a more useful representation that can be learned more easily by a supervised model and can support the sampling of more informative data points. This strategy can also help reduce the dimensionality of the learning problem, thus improving the model parameter estimation when only a few queries can be made. Related to the online active learning problem, Rožanec et al. [51] used a pre-trained network to extract salient features from unlabeled images before starting the sampling routine. Similarly, Cacciarelli et al. [22] used an autoencoder trained on all the available unlabeled data points to improve the performance of online active learning for linear regression models. Wrapper methods, on the other hand, use one or more supervised base learners that are trained on labeled data and pseudolabeled unlabeled data. There are two main variants of wrapper methods, self-training and co-training. Self-training uses a single supervised model that is trained on labeled data, and pseudo-labels are used for the data points with confident predictions. Co-training, on the other hand, extends self-training to multiple supervised models, where two or more models exchange the most confident predictions to obtain pseudo-labels. Pseudo-labels can be very beneficial in label-scarce environments, but one must be mindful of the confirmatory bias issue, where the model might rely on incorrect self-created labels. This problem has been extensively analyzed by Baykal et al. [52] in the active distillation scenario, which is a strategy where a smaller model, known as the student model, is trained to mimic the behavior of a larger pre-trained model, known as the teacher model [53,54]. In this context, confirmatory bias refers to the student model tendency to reproduce the predictions of the teacher model, even when the teacher predictions are incorrect. This can happen when the student model is trained to mimic the teacher model output too closely, without considering the underlying errors. To mitigate this, active distillation techniques use sample selection methods that encourage the student model to learn from data points where the teacher model makes errors, rather than just reproducing the teacher model predictions. In the more general active learning framework, confirmation bias might also refer to the tendency of an active learning algorithm to select examples that confirm its current hypothesis, rather than selecting examples that would challenge or improve it. Finally, graph-based methods construct a graph on all available data and fit a supervised model, where the loss comprises a supervised loss and a regularization term that penalizes the difference between the labels predicted for connected data points. 7

In the online active learning scenario, the graph structure can be used to model the similarity between data points, and the active learning algorithm can select the examples to label based on their position on the graph, such as selecting examples that are in low-density regions or are distant from other labeled examples. 

## 3 Online active learning approaches

In this review, we present a classification of online active learning strategies into four categories:

1. Stationary data stream classification approaches: these methods are designed to tackle online classification tasks, where the model is updated on the fly using newly labeled examples selected from a stream of data that does not change significantly over time. These methods are particularly useful in scenarios where the data distribution is relatively stable, such as monitoring systems, activity recognition, and recommender systems.

In monitoring systems for patient vital signs, data from patients is collected over time to monitor their vital signs and identify any potential health problems. This scenario would fall into this category because the data stream is relatively stable, and the model can be updated in real-time as new labeled examples become available. In activity recognition using wearable devices, data with a relatively stable distribution is collected over time from wearable devices such as fitness trackers to identify patterns of activity like walking, running, or sleeping.

2. Drifting data stream classification approaches: these online active learning strategies are specifically designed to handle classification tasks in dynamic environments where the data distribution constantly changes. These approaches are designed to adapt to changes in the data distribution in order to maintain high classification accuracy. Some real-world applications might be fraud detection or intrusion detection. In financial fraud detection, fraudsters often change their methods to evade detection, so a classification model used for fraud detection must be able to adapt to new patterns of fraud as they emerge. In real-time intrusion detection, computer networks detection systems must be able to detect new forms of cyberattacks as they appear, so the classification models used must be able to adapt to changes in the data distribution over time. This scenario would fall into this category because the data stream is constantly changing, and the model must be able to adapt to changes in the data distribution over time to maintain high accuracy.

3. Evolving fuzzy system approaches: these approaches are based on a type of fuzzy system that can adapt and change over time, in response to new data or changes in the environment. In traditional fuzzy systems, the rules and membership functions that define the system are fixed and do not change over time. Evolving fuzzy systems, on the other hand, are able to adapt their rules and membership functions based on new data or changes in the environment. This is particularly useful in applications where the data or the environment is non-stationary and evolves over time, such as in control systems for autonomous vehicles, where we must be able to adapt to changes in the environment, such as traffic patterns, road conditions, and weather.

4. Experimental design and bandit approaches: these methods, mostly related to regression models, actively select the most informative data points to improve model predictions. This category includes online active linear regression and sequential decision-making strategies like bandit algorithms or reinforcement learning. These methods adaptively select the most promising options in a given situation. An example is given by online advertising, where a model is used to select the most promising advertisements to display to users based on their browsing history and other factors. This scenario would fall into this category because the model must adaptively select the most promising options in real-time based on the information available at that time.

Also, in clinical trials, a model is used to select the most promising patients to enroll in a clinical trial based on their medical history and other personal information. Finally, in drug development studies, online active learning can be used to select the most promising compounds for further testing and development, based on their potential efficacy and safety.

This categorization provides a comprehensive overview of the different types of online active learning strategies and how they can be applied in various scenarios.

### 3.1 Stationary data stream classification approaches
In online active learning, a commonly employed strategy is to request labels for data points that are considered to be informative enough based on a pre-determined threshold. This threshold can be established through a variety of techniques, depending on the instance selection criterion used to evaluate the informativeness of the unlabeled observations. Another method, sometimes referred to as �-sampling, is to calculate the probability that a data point will be queried by adjusting the parameter of a Bernoulli random variable, as proposed by Cesa-Bianchi et al. in one of the pioneering studies on online active learning [55,56]. They used a linear predictor characterized by the 8 weight vector � ∈ ℝ! and, at each time step �, after observing the current data point �", the binary output � ∈ {−1, +1} is predicted using �3" = SGN8�"#$ % �"9 (1) where �"#$ is the weight vector estimated with the previously seen labeled examples (�$, �$), … , (�"#$, �"#$). The value �"#$ % �" is the margin, �̂ ", of �"#$ on the instance �". If the learner queries the label �", a new weight vector is estimated using the newly added labeled example (�", �") with the regular perceptron update rule [57] as in �" = �"#$ + �"�"�" (2) where �" represents the indicator function of the event �3" ≠ �". If the label is not requested, the model remains unchanged, and we have that �" = �"#$. At each time step �, the learner decides whether to query the label of a data point �" by drawing a Bernoulli random variable �" ∈ {0, 1}, whose parameter is given by �" = � � + |�̂ "| (3) where � > 0 is a positive smoothing constant that can be tuned to adjust the labeling rate. In general, as �̂ " approaches 0, the sampling probability �" converges to 1, suggesting that the labels are requested for highly uncertain observations. The sampling scheme introduced by Cesa-Bianchi et al. [55] is referred to as selective sampling perceptron, and it is reported in Algorithm 1.

Algorithm 1. Selective sampling perceptron

Require: an initial model �& = (0, … , 0)', a time horizon �, a sampling budget �, a parameter �. 1: Set � ← 1, � ← 0 2: while � ≤ � and � ≤ � do 3: Observe incoming data point �" ∈ � and set �̂ " = �"#$ % �" 4: Predict the label �3" = SGN(�̂ ") 5: Draw a Bernoulli random variable �" of parameter �" = � (� + |�̂ " ⁄ |) 6: if �" = 1 then // sampling decision 7: Ask for the true label �" and update the model 8: � ← � + 1 9: else 10: Discard �" 11: � ← � + 1 12: end if 13: end while

A similar approach to the one proposed by Cesa-Bianchi et al. [55] was investigated by Dasgupta et al. [58], who presented one of the first thresholding techniques for online active learning. They suggested setting a threshold on the margin, with the idea of sampling data points �" with a value of |�̂ "| lower than a given threshold �. The threshold is initially set at a high value and iteratively divided by two until enough misclassifications occur among the queried points. The linear classifier is updated using the reflection concept [59] to give more focus to recent data points. Sculley [60] built on the works of Cesa-Bianchi and Dasgupta to analyze the online active learning scenario for real-time spam filtering. The author compares two models, a perceptron and a support vector machine (SVM), and tries three different instance selection criteria, the fixed thresholding approach by Dasgupta et al. [58], the Bernoulli-based approach by Cesa-Bianchi et al. [55], and a newly developed logistic margin sampling. The perceptron is updated as per Dasgupta et al. [58], while the SVM is retrained on all available labeled observations each time a new data point is added. According to the logistic margin sampling strategy, the confidence �" that the predicted label �3" is correct is modeled using a logistic function �" = 1 1 + �#(|*+!| (4)

Then, the sampling decision is taken by drawing a Bernoulli random variable �" ∈ {0, 1} of parameter �" = 1 − �", which is equivalent to �" = �#(|*+!| (5)

As in the traditional �-sampling approach introduced by Cesa-Bianchi et al. [55], this sampling strategy depends on the uncertainty, meant as the distance from the prediction hyperplane. The main difference between the two strategies is the shape of the resulting sampling distribution, which can be observed in Figure 5. 9

Figure 5. Shape of the sampling distributions for �-sampling (a) and logistic sampling (b), for different values of � and �.

The selective sampling perceptron approach has also been investigated by Lu et al. [7], who proposed an online passive-aggressive active learning variant of the algorithm. Similarly to the �-sampling approach, at each time step �, a Bernoulli random variable �" ∈ {0, 1} is drawn to decide whether to query the label of the current data point �" or not. In this case, the parameter of �" is given by �" = � � + |�̂ "| (6) where � ≥ 1 is a smoothing parameter. Besides not allowing the smoothing parameter to assume value lower than 1, the sampling distribution is the same as the one governed by the parameter in Equation 3. The main difference lies in the passive-aggressive approach used for updating the weight vector. Indeed, while the traditional perceptron update, shown in Equation 2, only uses misclassified examples to update the model, the passive-aggressive approach updates the weight vector � ∈ ℝ! whenever the current loss ℓ"8�"#$; (�", �")9 is nonzero [61]. The new parameter �" is found using �" = �"#$ + �"�"�" (7) where �" represents the step size, and can be computed according to three different policies �" = [ ℓ"8�"#$; (�", �")9 ‖�"‖, ⁄ (�) min8�, ℓ"8�"#$; (�", �")9 ‖�"‖, ⁄ 9 (��) ℓ"8�"#$; (�", �")9 (‖�"‖, ⁄ + 1/2�) (���) (8) where � > 0 is a penalty cost parameter. Passive-aggressive algorithms are known for their aggressive approach in updating the model, which is motivated by the fact that traditional perceptron updates might waste data points that have been correctly classified but with low prediction confidence. A related issue to the update of the weight vector �" was emphasized by Bordes et al. [62], who noted that always picking the most misclassified example is a reasonable sampling strategy only when the training examples are highly confident. When dealing with noisy labels, this strategy could lead to the selection of misclassified examples or examples lying on the wrong side of the optimal decision boundary. To address this, Bordes et al. suggested a more conservative approach that selects examples for updating �" based on a minimax gradient strategy. In addition to confidence in the labels of the training examples, confidence in the model itself must be considered when the sampling strategy is based solely on model predictions.

Hao et al. [63] pointed out that a margin-based sampling strategy may be suboptimal when the classifier is not precise, especially in the early rounds of active learning when the model performance may be poor due to limited training feedback, leading to misleading sampling decisions. This issue is also referred to as cold-start active learning [64–66]. To address this, Hao et al. [63] propose considering second-order information in addition to margin value when deciding whether or not to query the label of a data point �". In general, first-order online active learning strategies only consider the margin value, while second-order methods also take into account the confidence associated with it. To do this, they assume that the weight vector of the classifier � ∈ ℝ! is distributed as � ∼ �(�, �) (9) where the values �- and Σ-,- encode the model knowledge and confidence in the weight vector for the �th feature �-.

The interactions between the �th and �th features is captured by the term Σ-,/. The smaller the variance associated with the coefficient �-, the more confident the learner is about its mean value �-. The objective of the proposed method is to take into account the confidence of the model when updating the model and making the sampling decision. With regards to the model update, when the true label �" of �" is queried, the Gaussian distribution in (a) (b) 10

Equation 9 is updated by minimizing an objective function based on the Kullback-Leibler (KL) divergence given by ℓ"(�, �) = D01(�(�, �)p�(�", �")9 + ��" '� + 1 2� �" '��" (10) where �, � > 0 are two regularization parameters, and �" = �ℓ"(�") = −�"�" represents the gradient of the hinge loss function defined as ℓ"(�") = max(0, 1 − �"�" '�"). The main goal of this objective function is to ensure a small loss on the current instance �, and a high confidence associated to its prediction. In general, the KL divergence is an information-based measure of disparity among probability distributions [67], and in this case it is used to ensure that the updated model is not too different from the previous one. The sampling decision uses an additional parameter to the margin �̂ ", which is defined as �" = 1 2 −� 1 �" + 1 � (11) where �, � > 0 are two fixed hyper-parameters and �" models the variance of the margin related to the data point �", as in �" = ���[�" '�"] = �" '�"�" (12)

The intuition is that, when the variance �" is high, the model has not been sufficiently trained on instances similar to �", and querying its label would lead to a model improvement. Then, a soft margin-based approach is employed by computing �" = |�̂ "| + �" (13)

If �" ≤ 0, the label is always queried as the model is extremely uncertain about the margin. Instead, when �" > 0, the model is more confident, and the labeling decision is taken by drawing a Bernoulli random variable of parameter �" = � � + �" (14) where � > 0 is a smoothing parameter. Finally, Hao et al. [63] also introduced a cost-sensitive variant of the loss function, for dealing with class-imbalanced applications.

The cold-start issue related to the application of active learning to imbalanced distributions has also been highlighted by Qin et al. [68], who used extreme learning machines (ELM) [69] and extended to the multiclass classification case the active learning ELM framework introduced by Yu et al. [70]. They highlighted the challenge of the lack of instances for certain classes in imbalanced datasets, which can seriously impact the predictive ability of the model for those classes. To address this issue, they proposed a sampling strategy that considers both diversity and uncertainty. The diversity is calculated by computing pairwise Manhattan distance between the unlabeled observations. The uncertainty of a data point �" is computed by taking the difference between the largest two posterior probabilities as in ������(�") = �(� = �2|�") − �(� = �32|�") (15) where �2 and �32 are the classes with the highest posterior probabilities. This approach is also referred to as bestversus-second-best margin and, as highlighted by Joshi et al. [71], is a good indicator of uncertainty when a large number of classes are present in the data. It should be noted that the sampling strategy introduced by Qin et al. [68] is not suited for stream-based single-pass active learning as it requires computing similarity and uncertainty measures for all the unlabeled observations in the data batch. Another approach to deal with class imbalance in active learning was proposed by Ferdowsi et al. [72], who used linear SVMs and a sampling strategy that switches between multiple instance selection criteria online. This approach, however, is limited to a pool-based setting and requires predicting an unsupervised evaluation score for all available unlabeled instances. The impact of the last queried observations on the scores associated with the unlabeled data points is evaluated, and a greedy approach is used to decide which instance selection criterion to trust. SVMs have also been used by Ghassemi et al. [73], who proposed a differentially private approach to online active learning. The privacy concerns are tackled both during the instance selection and the training phase, by randomizing the strategy introduced by Tong and Koller [8]. The informativeness of a data point �" is measured by its closeness to the current hyperplane �" as in �(�) = exp8−�(�", �")9 ∈ [0, 1] (16) where the distance function �(�", �") is defined as 11 �(�", �") ≜ |〈�", �" 〉| ‖�"‖ (17)

In the traditional framework, the label �" is queried if we have �(�) > �, where � is a pre-defined threshold. It should be noted that �(�) > � is equivalent to �(�", �") ≤ log 1/Γ, which means that the observation �" is in a sampling region of width 2 log 1/Γ around �". However, to avoid a deterministic decision process on the labeling and ensure privacy, some randomness needs to be introduced. This can be done in two ways. First, the labeling decision can be modeled as a Bernoulli random variable of parameter � if �(�) < � or (1 − �) if �(�) ≥ �, where � < 1/2. Another approach is based on the exponential mechanism introduced by McSherry et al. [74]. According to this strategy, the algorithm sets a constant probability of labeling data points within a sampling region defined by �, and a decaying probability for points outside of it. The selection strategy is represented by a Bernoulli of parameter �(�) = ç �#�5/7 �(�", �") ≤ � �#!(�!,�!)5/7 �(�", �") > � (18) where � > 0 and Δ = (1 − �⁄�)�. The authors assumed all data points belonging to the stream to be bounded in norm by �, ‖�"‖ ≤ � for � = 1, … , �. To tackle the privacy concerns while training, the authors propose two minibatches strategies, to avoid the problem of slow convergence that may result from introducing noise according to the private stochastic gradient descent scheme [75–77].

Two different approaches have been proposed by Ma et al [78] and Shah and Manwani [79]. Ma et al [78] proposed a query-while-learning strategy for decision tree classifiers. They used entropy intervals extracted from the evidential likelihood to determine the dominant attributes, which are ordered based on the information gain ratio. When a new data point �" is observed, its label is queried only if there not exists a dominant attribute. This will help to identify one and narrow the entropy interval. Shah and Manwani [79] investigated the online active learning problem for reject option classifiers. Given the high cost that is sometimes associated with a misclassification error, these models are given the option of not predicting anything, for example when dealing with a very confusing example.

A typical application of reject option classifiers is in the medical field, when making a diagnosis with ambiguous symptoms might be particularly difficult. In this case, it could be more beneficial not to provide a prediction but suggest further tests instead. They proposed an approach based on a non-convex double ramp loss function [80] ℓ!<, where the label of the current example �= is queried only if it falls in the linear region of the loss given by |�"(�")| ∈ [�" − 1, �" + 1], which is the region where the parameter would be updated. Here, � refers to the bandwidth parameter of the reject option classifier that determines the rejection region.

So far, we discussed several single model approaches to active learning, which have shown promising results in various applications. However, it is important to note that single models have their limitations and can sometimes struggle to capture complex patterns and diverse representations present in the data. To address these limitations, researchers have proposed the use of ensembles or committees as an alternative. An ensemble or committee refers to a group of multiple models that collaborate to produce a more robust and accurate prediction by combining their individual predictions. The models in an ensemble or committee can be trained on different subsets of the data or with varying hyperparameters, and the final prediction is typically made through either voting or weighted averaging. Ensembles or committees can also be regarded as a collection of models that work together to make a prediction, either by exchanging information or learning from one another. Among this class of methods, a common sampling strategy is represented by disagreement-based active learning. A framework to perform disagreementbased active learning in online settings was recently introduced by Huang et al. [81]. Let us characterize the learner by a hypothesis space ℋ of Vapnik-Chervonenkis (VC) dimension �, which is composed of all the classifiers currently under consideration, and a Tsybakov noise model [82,83]. Each hypothesis ℎ ∈ ℋ is a measurable function mapping the observation �" to binary output �" = {0, 1}. The Bayes optimal classifier ℎ∗ that minimizes the error rate is assumed to be included in ℋ. The error rate of a hypothesis ℎ is given by the probability that ℎ misclassifies a random instance � ∈ � as in ϵℙ(ℎ) = ℙ[ℎ(�) ≠ �] (19) where ℙ = ℙ@ × ℙA|� is the joint distribution of the example (�, �). The error rate is used to define the pseudodistance �(ℎ$, ℎ,) = |ϵℙ(ℎ$) − ϵℙ(ℎ,)| between two hypotheses ℎ$ and ℎ, . The disagreement among two hypotheses is given by �(ℎ$, ℎ,) = ℙ[ℎ$(�) ≠ ℎ,(�)] and the disagreement region is defined as �(ℎ$, ℎ,) = {� ∈ �: ℎ$(�) ≠ ℎ,(�)} (20)

The online active learning strategy is represented by the policy � = ({�"},{�"}), where {�"}"B$ is the map of the queried data points, and {�"}"B$ is the sequence of prediction rules. Over the time horizon �, the performance of 12 the policy � is evaluated using the label complexity and the regret. The label complexity measures the expected number of labels queried, with respect to the stochastic process induced by �, and it is given by �[�(�)] = �ü†�[�" = 1] ' "C$ ¢ (21)

The regret, on the other hand, represents the expected number of excess classification errors with respect to ℎ∗, and it is obtained as �[�(�)] = � § † �[�" ≠ �"] − �[ℎ∗(�") ≠ �"] "D':F!C&
* (22)

The objective of the algorithm is to minimize the label complexity with a constraint on the regret. At the first round, the initial version space is the entire hypothesis space ℋ, while the initial region of disagreement is the whole input space �. Then, at time step �, the learner updates the version space ℋ" using the � collected labels, and computes a new region of disagreement as �(ℋ") = {� ∈ �: ∃ ℎ$, ℎ, ∈ ℋ", ℎ$(�) ≠ ℎ,(�)} (23)

If �" ∈ �(ℋ"), then the label of the current data point is queried, otherwise a prediction is produced using an arbitrary hypothesis in ℋ". At the end of the iteration �, the set �" of � collected labeled examples is used to estimate the empirical error of the hypotheses in ℋ as ϵ�! (ℎ) = 1 � † �[ℎ(�) ≠ �] (�,A)∈�! (24)

The best currently available hypothesis is given by ℎ" ∗ = ������I∈ℋ! ϵ�! (ℎ). Then, the version space is updated by removing all the suboptimal hypotheses whose empirical error exceeds the one obtained with ℎ" ∗ by a threshold

Δ�! (ℎ, ℎ" ∗). The threshold regulates the trade-off between reducing label complexity by narrowing the region of disagreement and increasing the regret by eliminating good classifiers.

The disagreement concept was also used by DeSalvo et al. [84], while proposing an approach to online active learning for binary classification tasks based on surrogate losses. The overall framework is similar to the disagreement-based one used by Huang et al. [81], with the main difference being the use of weak-labels to optimize the sampling strategy. At each time step �, the learner observes the unlabeled data point �" and either decide to request its label or it assigns it a pseudo-label �3". Then, the pseudo labels �3" and the true labels �" processed so far are used together to obtain an estimate of the empirical risk ϵK! (ℎ), where �" is obtained by combining the collected labeled examples �" with the pseudo-labeled ones �™ ". This represents an example of combining active learning and semi-supervised learning, as highlighted in section 2.3.

Loy et al. [85] presented a Bayesian framework that leverages the principle of committee consensus to balance exploration and exploitation in online active learning. The aim of exploration is to discover new, previously unknown classes, while exploitation focuses on refining the decision boundary for known classes. To address the issue of unknown classes, the framework uses a Pitman-Yor Processes (PYP) prior model [86] with a Dirichlet process mixture model (DPMM). A DPMM is a non-parametric clustering and classification model that models the data generating process using a mixture of probability distributions. Each data point is assigned to a cluster, which is associated with a probability distribution over the classes. The number of clusters is modeled using a Dirichlet process, which is a distribution over distributions that allows for an infinite number of clusters but ensures that the number of actual clusters is always finite. At each time step �, the learner samples two random hypotheses ℎ$ and ℎ, from the model. Then, it computes the posterior probability of the current class � corresponding to � , �(� = �|�"), for each of the two hypotheses. Finally, ℎ-(�") = arg max L � �(�|�") is calculated for � = 1,2. The label of the current data point is queried in two cases: first, if ℎ$(�") ≠ ℎ,(�"), meaning the two hypotheses disagree, and second, if ℎ-(�") = � + 1 ∀ �, where � is the number of currently known classes, meaning the current data point belongs to a new class. The DPMM has also been used by Mohamad et al. [87], who proposed a semi-supervised strategy for performing active learning in online human activity recognition with sensory data. To account for the possibility of dealing with different sensor network layouts, the authors proposed pre-training a conditional restricted Boltzmann machine [88,89] and used it to extract generic features from the sensory input. The instance selection strategy follows a Bayesian approach, in trying to minimize the uncertainty about the model parameters.

To assess the usefulness of labeling the data point �", they measure the discrepancy between the model uncertainty computed from the data observed until the time step � and the expected risk associated with �". This gives a hint 13 of how the current label would impact the current model uncertainty. A dynamically adaptive threshold Γ is finally used to the determine whether the current expected risk is greater than the current risk.

Fujii and Kassima [90] also investigated the problem of Bayesian online learning. They provided a general framework based on policy-adaptive submodularity to handle data streams in an online setting. The framework adapts to the changing distribution of data over time by adjusting a set function, leading to more efficient use of data and improved performance. The authors distinguish between the stream setting, where the labeling decision can be made within a given timeframe, and the secretary setting, introduced in section 2, where the labeling decision must be made immediately. The framework proposed by Fujii and Kassima can be applied in a variety of active learning scenarios, such as active classification, active clustering, and active feature selection. The framework is based on the concept of adaptive submodular maximization, which extends the idea of submodular maximization. A set function is considered to be submodular if it satisfies the property of diminishing returns, meaning that adding an element to a smaller set has a greater impact on the function value than adding the same element to a larger set.

Adaptive submodular maximization allows the model to adapt to the changing distribution of data over time, by adjusting the set function to reflect the current state of knowledge. This leads to more efficient use of available data and improved performance.

A different kind of committee has been considered by Hao et al. [91]. Indeed, they proposed a framework for minimizing the number of queries made by an online learner that is trying to make the best possible forecast, given the advice received from a pool of experts. To do so, they adapted the exponentially weighted average forecaster (EWAF) and the greedy forecaster (GF) to the online active learning scenario. A comprehensive analysis of forecasters to perform prediction with expert advice can be found in [92]. In general, at each time step �, the learner or forecaster has access to the predictions for the data point �" made by the � experts, �-,"(�"): ℝ! → [0, 1] with � = 1, … , �. Based on these predictions, it outputs its own prediction �" for the outcome �". Then, if the label is revealed, the predictions made by the forecaster and the experts are scored using a nonnegative loss function ℓ.

The objective of the learner is to minimize the cumulative regret over the time horizon �, which can be seen as the difference between its loss and the one obtained with each expert � as in �-,' = †°ℓ(�", �") − ℓ8�-,"(�"), �"9± = �²' − ' "C$ �-,' (25)

The most simple approach to obtain a prediction �" from the learner is to compute a weighted average of the experts predictions as in �" = ∑ �-,"�-,"(�") M -C$ ∑ �/," M /C$ (26) where �-," ≥ 0 is the weight assigned at time � to the �th expert. The weights are typically the results of a nonnegative, convex, and increasing function �: ℝ → ℝ of the expert regret. This approach is coherent with the idea of a potential-based forecaster, which works by assigning a potential value to each expert, based on its past performance, and selecting the expert with the highest potential value at each time step. The potential values capture the quality of an expert advice, based on its past performance, and allow the learner to balance exploration, trying out different experts to gather more information, and exploitation, using the best expert based on the current information. With the EWAF, the weight for the �th expert are obtained using �-," = �NO",!$% ∑ � M NO&,!$% /C$ (27) where � is a positive decay factor and �-,"#$ is the cumulative loss of expert � observed until step �. The exponential decay factor � determines the weight given to the past losses, with more recent losses having a higher weight and older losses having a lower weight. Instead, the GF works by minimizing, at each time step, the largest possible increase of the potential function for all the possible outcomes of �" as in �" = arg min* sup

A! ¸ℓ(�, �") + 1 � ��†�#NP",!

M -C$ º (28)

The potential function is the function that assigns a potential value to each expert. The idea behind the greedy forecaster is to make the prediction that minimizes the potential function in the worst-case scenario, where the worst-case scenario is defined as the scenario where the increase of the potential function is largest for all possible outcomes of �" . Hao et al. [91] extended the EWAF and GF introduced by Cesa-Bianchi and Lugosi [92] by proposing the active EWAF (AEWAF) and active GF (AGF). The key idea is that, while the standard EWAF and 14

GF assume the availability of the true label �" after each prediction, in the online active learning framework the loss ℓ can only be measured a limited number of times. To factor this in, a binary variable �" ∈ {0, 1} is introduced to decide whether or not at round � the label is requested. Consequently, the cumulative loss suffered by the �th expert on the instances queried by the active forecaster is given by �²-,' = †ℓ8�-,"(�"), �"9 ⋅ ' "C$ �" (29)

The sampling strategy is based on the determination of a confidence condition on the difference between the prediction �" of the fully supervised forecaster and the prediction �̂ " made by the active forecaster. For the active forecaster we have that �̂ " = �[&,$](�̅ "), where �̅ " depends on the chosen model. The AEWAF is based upon the observation that if we have max $D-,/DM ½�-,"(�") − �/,"(�")½ ≤ � (30) then |�" − �̂ "| ≤ �, where � is a tolerance threshold. This means that the prediction of the forecaster is close to the one obtained in the fully supervised setting if the maximum difference of advice between any two experts is not too large. This assumption might not hold in the presence of noisy or bad experts and, to tackle this problem, the authors proposed a robust variant of the AEWAF. The AGF uses instead a confidence condition based on the fact that if max $D-,/DM ½�-,"(�") − �̅ "½ ≤ � (31) then |�" − �̂ "| ≤ �. The general scheme for performing online active learning with expert advice is reported in

Algorithm 2.

Algorithm 2. Online active learning with expert advice

Require: a data stream �, a loss function ℓ, a time horizon �, a set of � experts, a tolerance threshold �, a sampling budget �. 1: Set � ← 1, � ← 0 2: while � ≤ � and � ≤ � do 3: Observe incoming data point �" ∈ � and the expert advice æ�-,"(�"): � = 1, … , �ø 4: Generate prediction �̅ " for the label �" and set �̂ " = �[&,$](�̂ ") 5: if Equations 25 or 26 are satisfied then // sampling decision 6: Discard �" 7: else 8: Ask for the true label �" 7: Update cumulative loss �²-," = �²-,"#$ + ℓ8�-,"(�"), �"9, for � = 1, … , � 8: � ← � + 1 9: � ← � + 1 10: end if 11: end while

A similar framework, in conjunction with multiple kernel learning (MKL), has been investigated by Chae and Hong [93]. They propose an active MKL (AMKL) algorithm based on random feature approximation. In general, online

MKL based on random feature approximation is a method for online learning and prediction that combines multiple kernel functions to improve the performance of a learning algorithm [94,95]. In MKL, multiple kernel functions are used to capture different aspects of the data, and the optimal combination of kernels is learned from the data. On the other hand, random feature approximation is a technique for approximating a high-dimensional feature space using a lower-dimensional space, which is easier to learn from [96]. The online version of MKL based on random feature approximation is designed to handle data that arrives sequentially, and the learning algorithm is updated after each new data point. In kernel-based learning, the target function �(�) is assumed to belong to a reproducing

Hilbert kernel space (RKHS), which can be defined as

ℋ ≜ ¿�: �(�) = †�-�(�, �-)

S -C$ ¬ (32) where �(�) is a function in the RKHS, �(�, �-) is the positive semidefinite kernel function, �- are coefficients, and �- are the data points in the input space. This means that the target function �(�) can be represented as a linear combination of kernel evaluations between � and �-. The coefficients �- determine the specific linear combination 15 that represents the target function. The kernel function is defined such that, given two points � and �- in the input space, �(�, �T ) = 〈�(�),�(�-) 〉, where 〈⋅,⋅〉 represent the inner product in the RKHS. This states that the dot product between the feature representations can be computed by evaluating the kernel function between the two points, thus avoiding the need to explicitly compute the feature representation �(�) of �. In the proposed AMKL the learner uses an ensemble of � kernel functions. At each time step �, two main steps are implemented. First, each kernel function �™ -,"(�"), with � = 1, … , �, is optimized independently of the other kernel functions. This is referred to as local step. Then, in the global step, the learner seeks the best function approximation �™ "(�=) by combining the � kernel functions as in �™ "(�") = †�ƒ-,"

M -C$ �™ -,"(�") (33) where �ƒ-," refers to the weight for the �th kernel function at round �. Similarly to the case with expert advice, the weights are determined by minimizing the regret over the time horizon �, which is defined as the difference between the loss of the learner and the one obtained with the best kernel function �-," ∗ �' = †ℓ8�™ "(�=), �"9 ' "C$ − min $D-DM†ℓ8�-," ∗ (�"), �"9 ' "C$ (34)

To do so, the weights are computed based on the past losses as �ƒ-," = expÅ−�U † ℓ8�™ -,V(�V), �V9

V∈�!$%

Æ (35) where �U > 0 is a tunable parameter and �"#$ is an index of time stamps � indicating the instances for which has label has been requested, thus permitting to measure the loss. Then, the weights �ƒ-," are obtained from �ƒ-," as follows �ƒ-," = �ƒ-," ∑ �ƒ-," M -C$ (36)

Finally, the instance selection criterion is based on a confidence condition, denoted by with � > 0, on the similarity of the learned kernel function, which is a similar to the condition used by Hao et al. [91] in the formulation of the

AEWAF max $D/DM†�ƒ-,"ℓ °�™ -,"(�"), �™ /,"(�")± ≤ �

M -C$ (37)

Finally, there have also been some applications of online active learning for classification in computer vision.

However, it should be noted that the most performing methods for deep active learning can hardly be adapted to a stream-based setting. Indeed, many methods require performing clustering or measuring pairwise similarity among image embeddings [97–101], which cannot be easily done in a single-pass manner. Hence, the online applications proposed so far are mostly relying on the use of traditional models with uncertainty-based sampling. Narr et al. [102] analyze the stream-based active learning problem for the classification of 3D objects. They used a mondrian forest classifier [103], which is an efficient alternative of random forest for the online learning scenario, and selected images with high classification uncertainty to be labeled. Rožanec et al. [51] used online active learning to reduce the data labeling effort while performing vision-based process monitoring. Initially, features are extracted from the images using a pre-trained ResNet-18 model [104] and then, using the mutual information criterion [105], only √� features [106] are retained to fit an online classifier, where � is the total number of observations in the training set.

The authors combine a simple active learning strategy based on model uncertainty with five streaming classification algorithms, including Hoeffding tree [107], Hoeffding adaptive tree [108], stochastic gradient tree [109], streaming logistic regression, and streaming k-nearest neighbors.

### 3.2 Drifting data stream classification approaches
Active learning strategies belonging to this category try to tackle online classification tasks in time-varying data streams affected by distribution shifts. We can classify distribution shifts into three main categories, depending on whether they concern the feature space � or the output dimension �. A shift that only affects the input distribution �(�), and not the conditional distribution �(�|�), is referred to as covariate shift [110–112] or virtual drift [113]. In these circumstances, for two different time steps, �- and �-X7 , we have that �"" (�) ≠ �""'((�) and �"" (�|�) = 16 �""'((�|�), meaning that the underlying model is not being altered by phenomena like class swaps or coefficients changes. Conversely, in the presence of a real concept drift [113,114], the conditional distribution changes, and we have �"" (�|�) ≠ �""'((�|�). In this scenario, the predictive performance of the fitted model dramatically deteriorates, and a model update or replacement become necessary. An example of this kind of distribution shift can be identified in the changes of the consumer behaviors over time, or following a major event as the COVID-19 pandemic [115].

However, it should be noted that virtual drifts and real concept drifts often occur together [116], leading to a situation where we have both �"" (�) ≠ �""'((�) and �"" (�|�) ≠ �""'( (�|�) [117]. Lastly, we can incur in a label distribution shift [111] when the shift only affects �(�), leading to �"" (�) ≠ �""'((�). This situation can be observed in many real-world scenarios where the target distribution changes over time. A typical example is the prediction of diseases like influenza, whose distribution can dramatically change depending on the season, or in the presence of sudden outbreaks. Another key characteristic of distribution shifts is represented by the change rate, namely how fast the new concept or distribution is introduced into the data stream. To this extent, we can identify four kinds of drifts [117,118], which are illustrated in Figure 6. A sudden or abrupt drift is a drift that can be immediately detected from two consecutive time steps, �- and �-X$. It refers to a sudden and clearly identifiable change in the data distribution. An example of this would be a sudden change in the weather, which would affect the behavior of customers at a retail store. The change is noticeable, and the model needs to be updated immediately. A gradual drift exhibits a transition phase, where a mixture or overlap between the two distributions �"" and �""'( exists. In this case, the change is slower and more difficult to detect, making it challenging to update the model. An example would be a change in consumer behavior over time, which is hard to detect but can have a significant impact on a business. Another type of drift is the incremental drift, which has an extremely low transition rate that makes it very difficult to detect changes between the data points observed in the transition period. This type of drift is often caused by changes in the data generating process that happen gradually over time, in small steps rather than all at once. An example would be changes in the types of products that are popular among customers, which happen gradually and are hard to detect. Finally, a data stream can also be affected by recurring concepts, which sequentially alternate over time. An example would be a retail store where the same types of products are popular at different times of the year, such as winter coats and summer dresses. The model needs to be able to detect and adapt to these recurring concepts in order to maintain good performance. In summary, abrupt drift refers to a sudden change in the data distribution, gradual drift refers to a slow change in the data distribution over time, incremental drift refers to a change in the data distribution that happens in small steps, and recurring concepts refer to situations where the same concept or pattern in the data reappears at different points in time. All these types of drifts require different approaches to detect and adapt to them to maintain a good performance of the model.

Figure 6. Different types of drifts that can affect the data stream: abrupt drift (a), gradual drift (b), incremental drift (c), recurring concepts (d). C1 and C2 indicates the two concepts that might characterize the data distribution.

In online active learning for drifting data streams, some approaches address the presence of concept drifts by combining active learning strategies with drift detectors [119,120]. Drift detectors are algorithms that try to detect distribution shifts and identify when the context is changing. They can be divided into three macro-categories [117].

The first group of methods is represented by the error-based drift detectors, which try to detect online changes in the error rate of a base classifier. Among these, one of the most commonly employed strategy is the drift detection method (DDM) proposed by Gama et al. [121]. Another popular approach is the adaptive window (ADWIN) strategy proposed by Bifet and Gavaldà [122]. The second class of drift detectors is called data distribution-based drift detection, and the third class is represented by multiple hypothesis testing strategies. While the first class contains the majority of the proposed approaches, it assumes that we are able to observe the labels of all the incoming data points to assess the error rate. Instead, the last two classes could be implemented even in an unsupervised manner. An exhaustive overview on unsupervised drift detection methods has been proposed by

Gemaque et al. [123]. While the unsupervised nature of the data distribution-based and multiple hypothesis testing strategies make them ideal for the active learning scenario, it should be noted that real concept drifts can hardly (a) (b) (c) (d) ! ! ! !

C1

C2

C1 C1 C1

C2 C2 C2 17 be detected in a completely unsupervised fashion. Indeed, in a circumstance when the input distribution �(�) remains unaltered while the underlying model relating the input variables � to the label � changes, it would not be possible to detect the change of concept without collecting labels. This is why Krawczyk et al. [120] propose to apply an error-based drift detector to the few labels collected during the online active learning routine. To this extent, they use the ADWIN [122] method to detect drifts and decide when the current model needs to be updated or replaced. The proposed general framework for dealing with online active learning with drifting data streams is reported in Algorithm 3.

Algorithm 3. Online active learning with drifting data streams

Require: a data stream �, a classifier �, a drift detector Θ, a sampling strategy Υ, a labeling rate �, a sampling budget �. 1: Set � ← 1 � ← 0 2: while � ≤ � and � ≤ |�| do 3: Observe incoming data point �" ∈ � 4: if Υ(�") = ���� then // sampling decision 5: Ask for the true label �" 6: � ← � + 1 7: Update classifier Ψ with the labeled example (�", �") 8: Update drift detector Θ with the labeled example (�", �") 9: if ����� ������� = ���� then 10: Start to train a new classifier ΨYZ[ 11: Increase labeling rate � 12: else 13: if ����� �������� = ���� then // a detection is always preceded by a warning 14: Replace � with CYZ[ 15: Further increase � 16: else 17: Return to initial labeling rate � 18: end if 19: end if 20: if CYZ[ exists then // keeps being updated in the background until replacement 21: Update classifier CYZ[ with the labeled example (�", �") 22: end if 23: � ← � + 1 24: end if 25: end while

Moreover, the authors proposed the use of a time-variable threshold to balance the budget use over time. Their approach is based on the intuition that, when a new concept is introduced, more labeling effort will be required to quickly collect representative observations belonging to the new concept and replace the outdated model. This is obtained by adjusting a time-variable threshold to balance the budget use over time. Given a threshold � on the uncertainty of the classifier and a labeling rate adjustment � ∈ [0,1], the threshold is reduced to Γ − � when ADWIN raises a warning and to Γ − 2� when a real drift is detected. Thus, when allocating the labeling budget, the key requirement is that the labeling rate employed when a drift is detected should be strictly larger than the one used in static conditions.

A similar thresholding idea has also been used by Castellani et al. [124], who proposed an active learning strategy for non-stationary data streams in the presence of verification latency. They used of a piece-wise constant budget function, where the labeling rate � is increased to �I-UI when a drift is detected and, after a while, reduced to �\][.

Finally, the labeling rate is restored to its nominal value �. A visual representation of the labeling approach is shown in Figure 7. The length of the time segments where the labeling rate is altered depends on the desired values for �I-UI and �\][, constraining the overall labeling rate to be equal to �. 18

Figure 7. Piece-wise constant budget function introduced by Castellani et al. [124]. The sampling rate � is increased to �!"#! when a drift is detected (�$%"&'), then reduced to �()* between �%! and �%", before being restored to its nominal value.

The authors also tackled the verification latency issue by considering the spatial information of a queried point for which the label has not been made available yet by the oracle. In this way, it is possible to avoid oversampling from regions where many close points have a high utility, namely a low classification confidence. While assessing the utility of the incoming data points the authors use real and pseudo-labels by propagating the information contained in the already labeled observations, as suggested by Pham et al. [125]. The idea is to use the spatial information of the queried labels by estimating the still missing labels with a weighted majority vote of the label of its k-nearest neighbors labels, where the weight for each nearest neighbor depends on the arrival time of the labels.

The verification latency issue in online active learning with drifting data streams was also extensively analyzed by

Pham et al. [125]. Consider the general case where at time �- @ we draw an instance �-, and find it interesting enough to send it to the oracle, which will send back the label �- only at time �-

A, where �-

A > �- @. Before the requested label arrives, we might encounter another instance similar to �- and ask again for its label, since the learner could not update its utility function or threshold. Similarly, we might use outdated information when updating the policy in a future window. To tackle these issues, the authors propose a forgetting and simulating strategy to avoid using soon-to-be outdated observations and prevent redundant labeling. The instance selection is based upon the variable uncertainty strategy proposed by Žliobaitė et al. [126] and the balanced incremental quantile filter by Kottke et al. [127]. If we denote the current sliding window at time �Y @ as �Y = [�Y @ − Δ,�Y @) and use windows of fixed size Δ, we know that the sliding window that would be used for training when the label �Y related to �Y arrives will be given by �Y = [�Y

A − Δ,�Y

A). The forgetting step is then implemented by discarding outdated labeled examples that are included in �Y but will not be included in �Y. If �- is a Boolean variable indicating whether the �th observation has been labeled, the set of instances selected to be forgotten is given by �Y = [(�-, �-) ∀ � < � ∶ �- = 1 ∧ �- @,�-

A ∈ �Y \ �Y (38)

Similarly, there is a second set of observations, with time stamps �Y

X = �Y\�Y = [�Y @,�Y

A), where there might be instances that have been queried but whose label is not currently available. To avoid losing such information and redundantly asking for the label of similar instances, the algorithm simulates incoming labels with a bagging approach by averaging across multiple utility estimations. They also consider an alternative simulation approach based on fuzzy labeling.

Similarly to Krawczyk et al. [120], the ADWIN drift detector has also been used by Zhang et al. [119] while proposing a method for dealing with online active learning in environments characterized by concept drifts and class imbalance. The instance selection criterion is based on the predictive uncertainty, which they estimate using the best-versus-second-best margin value (Equation 15), as they tackle a multi-class classification problem. An initial pool of � observations is passively collected from the stream to initialize the active learning strategy. Then, a threshold �- is estimated for each class as in �- = [ �� �-� � �-� ≥ 1 � � �-� < 1 (39) where � = 1, … , � is the number of classes and � is a pre-defined constant used to control the size of the threshold.

The model is represented by an ensemble of � classifiers and, when ADWIN detects a concept drift, the classifier with the higher error is replaced with a newly trained one. Finally, the class imbalance issue is also taken into account in two ways, during the training of the ensemble with the use of class-specific weights, and during the active learning routine, by dynamically adjusting the threshold to select more observations belonging to the minority class. Another ensemble-based approaches to perform active learning from data streams has been proposed by Zhu ! " !!"#$% !"! !"" "&'( ")#*) 19 et al. [128]. The authors partitioned the data stream � into chunks of observations, and then trained each of the � models composing the ensemble � on a different chunk of data. In this way, even if the previous observations become unavailable, the models can be used when taking the sampling decision in order to take into account a global uncertainty measure, which is a more robust approach than treating each chunk as a static dataset. At time step �, the learner receives a data chunk �", which is used to build the current classifier �". At this point, the ensemble is composed by �", together with the most recent � − 1 classifiers, �"#^X$, … , �"#$, trained on the labeled examples sampled from the previously observed data chunks, �"#^X$, … , �"#$. At each iteration, the objective is to predict the remaining unlabeled data points from the current chunk, �" . The ensemble-based active learning framework is depicted in Figure 8.

Figure 8. Ensemble-based active learning framework for data streams proposed by Zhu et al. [128].

The instances selected to be queried are the ones with the largest ensemble variance, and the predictions are obtained by combining the predictions of the single classifiers using the weights �"#^X$, … ,�"#$. Finally, a weight updating rule is used to adapt to dynamic data streams. Ensemble classifiers have also been used by Shan et al. [129,130], which is built upon the pairwise classifiers strategy introduced by Xu et al. [131]. The pairwise strategy makes use of two models, a stable classifier �3 and a dynamic classifier �!, and divides the data stream in batches as in [128]. The prediction for an incoming data point �" is obtained with a weighted average of the predictions obtained from the two classifiers as in �_(�") = �3�`) (�") + �!�`*(�") (40) where �3 and �! are the weights associated with the stable and the dynamic classifier, respectively. At time �, the stable classifier �3 is trained on the labeled portions of all the batches processed so far, �$, … , �"#$. Conversely, �! is trained exclusively on �"#$. The key idea is that whenever the reactive classifier starts to outperform the stable classifier, the stable classifier is replaced by the reactive one, which is eventually reset. This replacement allows the learner to adapt to the drift and focus on the most recent instances, forget the seemingly obsolete data points. The main drawback of this approach is that it cannot effectively address gradual drifts as the replacement with the classifier trained on the most recent observations makes the learner forget about observations away from the current window. Hence, similarly to the approach of Zhu et al. [128], Shan et al. [129] proposed an extension of this approach, based on an ensemble of classifiers in trying to contemporarily address gradual drifts and abrupt drifts. In their strategy, the stable classifier learns from all the labeled instances and the reactive classifier is replaced by an ensemble of dynamic classifiers, trained on multilevel sliding windows to capture changes in the data stream at different time intervals. The instance selection approach combines random sampling and uncertainty sampling, where the latter is based on the margin value of the predictions obtained by the ensemble. It should be noted that the prediction �_ for the data point �" is obtained as a weighted combination of the predictions obtained with the stable and dynamic classifiers as in �_(�") = �3�`) (�") + †�!�`*(�") a !C$ (41)

The stable classifier has a constant weight �3 = 0.5 and plays a crucial role in trying to learn the overall trend and direction of concept drift. Conversely, the dynamic classifiers have gradually decaying weights, according to a damped sliding winding approach where each weight is initialized at 1⁄� and then reduced according to its creation time �! = [ �! ×1 − 1 �ÿ � = 1, … ,� − 1 1 � � = � (42) 20

The most recent classifiers are useful in detecting sudden concept drifts and have highest weights while the old dynamic classifiers have lower weights and can help to identify gradual drifts. The same pairwise strategy based on an ensemble composed by a stable classifier a � dynamic classifiers was also used by Zhang et al. [132]. They modified the original strategy by introducing a reinforcement mechanism to adjust the weights �! according to the prediction performance and the class imbalance issue. The weights adjustment strategy is described by

Algorithm 4. It should be noted that this procedure is only implemented after the true label �" has been revealed by the oracle. The damped class imbalance ratio (DCIR) value is obtained by taking into account the number of observations for each class collected so far. This is expected to be useful when dealing with imbalanced classes.

With regards to the instance selection criterion, the authors consider a hybrid strategy combining uncertainty sampling and random sampling, since approaches solely based on uncertainty could ignore a concept change that is not close to the boundary.

Algorithm 4. Weight adjustment for dynamic classifiers

Require: a labeled observation (�", �"), number of classes Κ, number of dynamic classifiers �, current weights �! with � = 1, … ,�, DCIR for each class � ∈ Κ. 1: if DCIR[�"] < 1⁄Κ then // if it belongs to the minority class 2: for � in (1, �) do 3: if �!(�") = �" then // if the prediction made by �! is correct 4: �! = �!(1 + 1⁄�) // increase weight of classifier �! 5: else 6: �! = �!(1 − 1⁄�) // decrease weight of classifier �! 7: end if 8: end for 9: end if

Another way to perform online active learning in time-varying data streams is to use clustering-based approaches.

To this extent, Ienco et al. [25] investigated a clustering-based approach in a batch-based scenario, where only a fraction of the incoming block of observations can be labeled. To this extent, it is not implemented in a typical single-pass manner. They extend the pre-clustering approach [23], which had been previously studied in the poolbased scenario, to the stream-based case. The sampling strategy takes into account an extra-cluster metric, to sort the clusters, and an intra-cluster one, to sort the observations within each cluster. When a new batch arrives, observations are clustered, and clusters are sorted based on the homogeneity of the clusters, which is measured taking into account the number of (predicted) classes within each cluster. If a cluster is balanced in the number of expected classes, it is regarded to as an uncertain cluster that covers a more difficult area of the input space. Within each cluster, the certainty of an observations is determined by its representativeness, namely the distance from the centroid, and the uncertainty, meant as the maximum a posterior probability among all the predicted classes for �". When the clusters and observations are ranked, the learner starts to iteratively ask the observations label in an alternate fashion. Another approach that tries to exploit the clustering nature of the incoming observations has been proposed by Mohamad et al. [133], with the use of bi-criteria active learning algorithm that considers both density in the input space and label uncertainty. The density-based criterion makes use of the growing Gaussian mixture model proposed (GGMM) by [134], which is used to find clusters in the data and estimate its density. This model creates a new cluster when a new data point �" has a Mahalanobis distance greater than a given closeness threshold from the nearest cluster, among the currently available ones. A flowchart describing the main steps of the GMMM is depicted in Figure 9.

Figure 9. Main steps of the growing Gaussian mixture model used in [133].

Observe an unlabeled data point

Gaussian matches?

Yes

No

Update parameter of the Gaussian

Compute the probability of match with each Gaussian

Decay the weight of all Gaussians

Remove less contributing

Gaussian

Initialize new

Gaussian with the current data point 21

A Bayesian logistic regression model is used for addressing the label uncertainty criterion and the concept drift. As the classifier parameters �" are assumed to evolve over time, the model is incrementally updated using a discrepancy measure, which is computed as the difference between the uncertainty of the model in �" before and after the true label �" is added to the training set. The query strategy follows the �-sampling approach, in trying to sample, with high probability, the observations that contribute the most to the current error. The combination of density and uncertainty is also employed by Liu et al. [135], who proposed a cognitive dual query strategy for online active learning in the presence of concept drifts and noise. The local density measure is used to obtain representative instances and the uncertainty one to select data points where the classifier is less confident. The cognitive aspect takes into account the Ebbinghaus’s law of human memory [136] to determine an optimal replacement policy. The proposed strategy tries to tackle both gradual and abrupt drifts. The drift is generally considered as a change in the underlying joint probability distribution from one time step � to another, namely �"(�, �) ≠ �"X$(�, �). The local density of an observation �" is defined by the number of times that �" is the nearest neighbor of other instances [137]. Since we are in an online framework, the authors proposed to measure the local density using a sliding window model, referred to as cognition window. Based on the concept of memory strength, the model determines when the current window is full and needs to be updated. Finally, the labeling decision is taken by using two thresholds, one for the local density and one for the classifier uncertainty. A different sliding window-based online active learning strategy is the one proposed by Kurlej and Woźniak [138]. Indeed, the authors proposed a sliding window approach based on a nearest neighbors classifier. The reference set for the k-nearest neighbors model is a window, and it is updated in two ways: in a first-in-first-out manner or using the examples selected by the active learning strategy. Since the reference set is updated over time, this method can effectively deal with concept drift and time-varying data streams. The sampling strategy is also based on two criteria. The first one is similar to the margin-based approaches, an instance is queried if it has a low distance from two observations belonging to different classes. The second criterion, similar to the greedy sampling strategy, seeks observations that have a large minimum distance from the observations in the current reference set. Both criteria are implemented by setting a threshold on the distances. A simpler approach for taking into account the timevarying aspect of evolving data stream is to force the model to focus on the most recent observations. Along these lines, Chu et al. [139] propose a framework based on a Bayesian probit model and a time-decay variant. Online

Bayesian learning is used to maintain a posterior distribution of the weight vector of a linear classifier over time �", and the time-decay strategies is employed to tackle the concept drift and give more importance to recent observations. They also propose an online approximation technique that can handle weighted examples, which is based upon [140]. They tested different sampling strategies, built upon an online probit classifier. The instance selection criteria are based on entropy, function-value and random sampling.

### 3.3 Evolving fuzzy system approaches
An alternative way to take into account the time-varying nature of evolving data streams is the use of evolving fuzzy systems (EFS) [141], which are soft computing techniques that can efficiently deal with novelty and knowledge expansion. EFS are self-developing, self-learning fuzzy rule-based or neuro-fuzzy systems that self-adapt both their parameters and their structure on-the-fly. They try to mimic human-like reasoning by modeling it with a dynamically developing fuzzy rule-based structure and implementing it utilizing data streams using a formal learning process. The basic rule structure of a fuzzy model is given by ����-:��(�$ �� �-$) ��� . . . ���(�Y �� �-Y) ���� (�- = �-& + �-$�$ + . . . + �-Y�Y) (43) where ����- is one of several (� = 1,2, . . . , �) fuzzy rules in the rule base; �/ (� = 1,2, . . . , �) are input variables; �- denotes output of the i-th fuzzy rule; �-/ denotes the j-th prototype (focal point) of the i-th fuzzy rule; �-/ denotes the j-th parameter of the i-th fuzzy rule. For a more thorough discussion on EFS and their use in online learning, we suggest referring to [6,141–143].

The main components of an EFS are shown in Figure 10. The two key components of an EFS are the structure evolving scheme, which contains the rule generation and simplification modules, and the parameters updating scheme. The rule generation module defines when a new rule needs to be added to the current model. The rule merging and pruning steps simplify the models by removing redundant rules and combining two rules when their similarity is larger than a given threshold. The parameters updating modules are used to keep track of the model evolution. These learning modules are used to update the EFS every time a new labeled example (�", �") is made available. 22

Figure 10. Learning modules of an EFS [142].

The first single-pass active learning approach based on the use of evolving classification models has been proposed by Lughofer [144]. The proposed algorithm is based on two key concepts, conflict and ignorance. The former is related to an incoming data point lying close to the boundary between any two classes; the latter considers the distance of the incoming observation from the currently labeled training set, in the feature space. This suggests that the data point falls within a region that has not been thoroughly explored by the learner. Later on, Lughofer and Pratama [145] also proposed the first online active learning approach for evolving regression models. Similarly to their previous work [144], the authors consider the ignorance about the input space in the instance selection criterion. Moreover, they also consider the uncertainty in the model outputs and in the model parameters. The predictive uncertainty is assessed in terms of confidence intervals using locally adaptive error bars. The error bars are inspired by [146] and the authors propose a new merging approach for dealing with the case of overlapping fuzzy rules. The uncertainty in the model parameters in instead evaluated using the A-optimality criterion, which will be discussed in section3.4. Instead of leveraging the uncertainty about the output, Pratama et al. [147] set a dynamic threshold based on the variable uncertainty strategy introduced by [126] while trying to address the whatto-learn question in the training of a recurrent fuzzy classifier. The key idea is that the model is iteratively retrained using data points that fall within rules with low support, which were formed using a smallest amount of observations.

A different kind of threshold, based on the spherical potential theory, has been suggested by Subramanian et al. [148], with the proposal of a meta-cognitive component that evaluates the novelty content of incoming data points.

This is done using a knowledge measure represented by the spherical potential, which has been thoroughly investigated in kernel-based approaches [149]. The spherical potential is used to set a threshold and decide whether to add a new rule to capture the knowledge in the current sample. It should be noted that the authors also used a threshold based on the prediction error, which could not be used with scarcity of labels. The prediction error is assessed using the hinge loss error function [150,151].

Fuzzy models have also been used to solve computer vision tasks. Weigl et al. [152] analyze the visual inspection quality control case, which is also considered by Rožanec et al [51]. They assess the usefulness of the images in a single-pass manner, but the instances that are selected to be queried are accumulated in a buffer, which is later on assigned to an oracle for labeling. Choosing the size of the buffer represents a trade-off problem between timely updating the classifier and requiring continuous interventions from a human annotator. The active learning strategy is certainty-based, meaning that the objective is to increase the classifier’s certainty on future predictions. The strategy is implemented via thresholding the certainty of the incoming data points and takes into account two model classes, a random forest classifier and an evolving fuzzy classifier. When using random forest, certainty is computed using the best-versus-second-best margin score. Instead, when using evolving fuzzy classifiers, the sample selection criterion takes into account the conflict and ignorance concepts as in [144].

Finally, Cernuda et al. [153] combine the use of fuzzy models with a sampling approach inspired by the multivariate statistical process control literature. Indeed, using a latent structure model, they propose a query strategy based on the Hotelling �, and the squared prediction error (SPE) statistics, which have been extensively used in anomaly detection problems [154–157]. Ge [21] used these statistics for pool-based active learning in conjunction with a principal component regression model. The key idea is to use the Hotelling �, and the SPE statistics to measure the distance between the currently labeled training set and a new unlabeled data point. A high value in one of the two statistics would most likely suggest that the new observation is violating the current model, and thus its inclusion in the training set could bring some valuable information. Similarly, Cernuda et al. [153] use the Hotelling �, and the SPE statistics with a partial least squares model. Then, when a new observation is added to the training set, they retrain a TS fuzzy model using a sliding window approach.

Structure evolving

Rule generation Rule merging Rule pruning

Parameters updating

Antecedent Consequent 23

### 3.4 Experimental design and bandit approaches
Optimal experimental design [158] is a research field that is closely related to active learning. It deals with the design of experiments that allow for efficient estimation of model parameters or improved prediction performance while minimizing the number of required labeled examples, also referred to as the number of runs �. Many optimality criteria have been developed in thriving to strike a balance between efficient use of resources and ensuring good performance of the model. The traditional framework of optimal experimental designs focuses on linear regression models of the form � = �� + � (44) where, given � input variables, � is a � × 1 vector of response variables, � is a � × � model matrix, � is a � × 1 vector of regression coefficients, and � is a � × 1 vector representing the noise, with covariance matrix �,�. When � ≥ � observations are available, an ordinary least square (OLS) estimator for � can be obtained using �È = (�%�)#$�%� (45)

In general, design optimality criteria leverage the information contained in the moment matrix, which is defined as � = �%�⁄�. The matrix �%� plays a crucial role in the estimation of the model coefficients �, and it is important to perceive information about the design geometry. Indeed, with Gaussian noise characterized by � ∼ �(�, �,�), and independent and identically distributed observations, we know that �È|� ∼ �(�, (�%�)#$�,) (46) and we can define a 100(1 − �)% confidence ellipsoid related to the solutions of � using 8� − �È9 % (�%�)8� − �È9 ��, ≤ �b,!,Y#! (47) where �, represents the residual mean square, �b,!,Y#! is the 100(1 − �) percentile derived from the Fisher distribution, and � indicates all the possible vectors that could be the true model parameter �. The ellipsoid can also be expressed as 8� − �È9 % (�%�)8� − �È9 ≤ �, where � = ��,�b,!,Y#!. The volume of this ellipsoid is inversely proportional to the square root of the determinant of �%�, and the length of its axes is proportional to 1/�-, where �- represents the �th eigenvalue of �%�, with � = 1, … , �. The so-called alphabetic optimality criteria pursuit efficient designs by exploiting these properties [159]. The most commonly employed optimality criteria for good parameter estimation are A-, D- and E-optimality:
* A-optimality: this criterion pursues good model parameter estimation by minimizing the variances of the regression coefficients. Knowing that the coefficients variances appear on the diagonal of the matrix (�%�)#$, it can be shown that an A-optimal design is given by a design �∗ that satisfies min � tr[�(�)]#$ = tr[�(�∗)]#$.

Geometrically, this coincides with shrinking the hyperrectangular enclosing the confidence ellipsoid [160,161].
* D-optimality: this criterion minimizes both the variance and covariance of the regression coefficients, directly minimizing the total volume of the confidence ellipsoid [162]. A D-optimal design is given by a design �∗ that satisfies max � │�(�)│ = │�(�∗)│ [163].
* E-optimality: this strategy tries to shrink the ellipsoid by minimizing the maximum eigenvalue of the covariance matrix, which can be seen as trying to reduce the length of the longest axis [164].

The geometrical intuition behind these criteria is illustrated, in the two-dimensional case, in Figure 11.

Figure 11. Confidence ellipsoid around the model parameters and optimality criteria: A-optimality (a), D-optimality (b), and

E-optimality (c). (a) !! !" "# (b) !! !" "# (c) !! !" "# 24

Finally, there are also optimality criteria that focus on developing models with good predictive properties. Within this class, G-optimality represents a criterion that is used to seek protection against the worst-case prediction variance in a region of interest ℛ. This is achieved by solving min � [max �∈ℛ �(�)] (48) where �(�) represents the variance of the current model in the data point �, which can be computed as �(�) = � �(e)%(�%�)#$�(e) (49) where �(e) represents the data point where the variance is being estimated, expanded to the model form. It should be noted that G-optimality can be highly influenced by anomalous observations, as it protects against the highest possible variance over all the region ℛ. This issue can be tackled by using I- or V-optimality, which estimate the overall prediction variance over ℛ by integrating or averaging, respectively. For a more extensive discussion on optimal designs, readers may wish to consult Montgomery [165] or Myers et al. [162].

The use of optimality criteria has proven to be highly beneficial in offline experiments design, allowing practitioners to pre-determine the location of each design point with ease. However, these methods require modification to be applied in a stream-based scenario where data points arrive sequentially. A common approach for obtaining a nearoptimal design with streaming observational data is represented by thresholding. Riquelme et al. [166] proposed a thresholding algorithm for online active linear regression, which is related to the A-optimality criterion. Their approach uses a norm-thresholding algorithm, where only observations with large, scaled norms are selected. The design is augmented with the observations � whose norm exceeds a threshold � given by

Pf(‖�‖ ≥ �) = � (50) where � is the ratio of observations we are willing to label out of the incoming data stream. Another approach related to the A-optimality criterion was proposed by Fontaine et al. [167], who studied online optimal design under heteroskedasticity assumptions, with the objective of optimally allocating the total labeling budget between covariates in order to balance the variance of each estimated coefficient. Cacciarelli et al. [168] further extended the thresholding approach introduced by Riquelme et al. [166] by proposing a conditional D-optimality (CDO) algorithm. The terms conditional refers to the fact the design is marginally optimal, given an initial set of labeled observations to be augmented. The main steps of the CDO approach are reported in Algorithm 5. The authors exploited the connection between D-optimality and prediction variance previously highlighted by Myers et al. [162].

The sampling strategy selects observations by setting a threshold � given by

Pf8�" %(�%�)#$�" ≥ �9 = � (51) where � is the current set of labeled observations and �" is the data point that is currently under evaluation. It can be seen how the threshold is estimated using kernel density estimation (KDE) on a set of � unlabeled observations, which are taken passively from the data stream without querying any label. This provides an initial set of data, referred to as warm-up set, that can be used to estimate the covariance matrix and the informativeness threshold.

Algorithm 5. Online active learning using CDO

Require: an initial random design �; a data stream �; a warm-up length �; a sampling rate �; a budget � 1: Set � = ∅ // warm-up set to estimate � and � 2: � ← 1, � ← 0 //� represents the currently used budget 3: while � ≤ � do 4: Observe incoming data point �" ∈ � 5: Select �":� = � ∪ �" 6: � ← � + 1 7: end while 8: Estimate the covariance matrix � of � and perform eigendecomposition � = ���% 9: Whiten the initial design by computing � = �#�⁄� ��� 10: Whiten the warm-up observations by computing � = �#�⁄� ��� 11: Estimate � using KDE on � with the desired sampling rate � using Equation 15 with � and � 12: while � ≤ � and � ≤ |�| do 13: Observe incoming data point �" ∈ � 14: Whiten �- by computing �" = �#�⁄� ���" 15: if �" %(�%�)#$�" ≥ � then 16: Ask for the label �- and augment the labeled dataset � = � ∪ �" 17: � ← � + 1 25 18: Update threshold � to measure the prediction variance of the enlarged design (step 11) 19: else 20: Discard �" 21: � ← � + 1 22: end if 23: end while

Cacciarelli et al. [169] also investigated how the presence of outliers affect the performance of online active linear regression strategies. They showed how the design optimality-based sampling strategies might be attracted to outliers, whose inclusion in the design eventually degrades the predictive performance of the model. This issue can be tackled by bounding the search area of the learner with two thresholds, as in

Pf8�$ ≤ �" %(�%�)#$�" ≤ �,9 = � (52) where the choice of �, represents a trade-off between seeking protection against outliers and exploring uncertain regions of the input space. The norm-thresholding approach was also extended by Riquelme et al. [170] to the case where the learner tries to estimate uniformly well a set of models, given a shared budget. This scenario is similar to a multi-armed bandit (MAB) problem where the learner wants to estimate the mean of a finite set of arms by setting a budget on the number of allowed pulls [171–174]. The authors propose a trace upper confidence bound (UCB) algorithm to simultaneously estimate the difficulty of each model and allocate the shared labeling budget proportionally to these estimates. UCB is a common algorithm used in MAB problems to balance exploration and exploitation [175,176], which takes into account the predicted mean value and the predicted standard deviation, weighted by an adjustable parameter [177]. This allows to balance the exploitation of data points with a high predicted value and the exploration of areas with high uncertainty. In general, MAB problems can be seen as a special case of sequential experimental design, where the goal is to sequentially choose experiments to perform with the aim of maximizing some outcome. The typical framework of a MAB problem can be regarded as an optimization problem where the learner must identify the option or arm with the highest reward, among a set of available arms characterized by different reward distributions. Both MAB and active learning paradigms involve a sequential decision-making process where the learner aims to maximize a reward or improve model accuracy by selecting an arm to pull or a data point to label, respectively, and receiving feedback (in the form of a reward or label request) for each selection. There are two main approaches to tackle MAB problems:
* Regret minimization: this approach is coherent with the objective of maximizing the cumulative reward observed over many trials. In this case, the learner must balance exploration, namely trying out different arms to learn more about the reward distributions, with exploitation, i.e., using current knowledge to choose the most promising arm. These kinds of algorithms strike a balance between learning a good model and obtaining high rewards. A few examples might be treatment design, online advertising and recommender systems.
* Pure exploration: in this case we are interested in finding the most promising arm, with a certain confidence or given a fixed budget on the number of pulls. To do so, the objective is to learn a good model while minimizing the number of measurements or labels required. This scenario is suggested in circumstances where, due to safety constraints, we are not given complete freedom to change the variable levels and we are mostly interested in understanding the underlying model governing the system. Possible examples include drug discovery or soft sensor development [178–181].

The pure exploration approach is particularly useful when coupled with the study of linear bandits, which are a type of contextual bandit algorithm [182] that assumes a linear relationship between the features of the context and the expected reward of each arm. In this type of problem, when an arm � ∈ � is pulled, the learner observes a reward �(�) that depends on an unknown parameter �∗ ∈ ℝ! according to the linear model �(�) = �'�∗ + e (53) where e is a zero-mean i.i.d. noise. This is similar to active linear regression in that, in both cases, the learner aims to select the most informative data points to learn about the underlying model or system [172,173]. Soare et al. [183], investigated this problem, in the offline setting, using the G-optimality criterion and a newly proposed ��- allocation algorithm. Jedra and Proutiere [184] proposed a fixed-confidence algorithm for the same problem, while

Azizi et al. [185] analyzed the fixed-budget case. Azizi et al. [185] also extended the framework to the case where the underlying model is represented by a generalized linear model [183,186].

An interesting variant of this problem is represented by the study of transductive experimental designs. A transductive design is a problem where we can pull arms from a set � ∈ ℝ!, with the objective of identifying the best arm or improve the predictions over a separate set of observations � ∈ ℝ!, which is given, in an unlabeled form, beforehand. A practical example of this case is when we are trying to infer the user preferences over a set of 26 products, but we can only do that by pulling arms from a limited set of free trials. Alternatively, we might be interested in estimating the efficacy of a drugs over a certain population, while doing experiments on a population with different characteristics. This problem has been tackled with an active learning approach by Yu et al. [187], with the idea of exploiting unlabeled data points in � while evaluating the informativeness of the data points in �.

The transductive case of sequential experimental design has been explored by Fiez et al. [188], but instead of performing active learning, they were interested in inferring the best reward over �, only pulling the arms in �.

Finally, this has been extended to the online scenario by Camilleri et al. [189], balancing the trade-off between time complexity and label complexity, namely between the number of unlabeled observations spanned and the number of labels queried in order to stop the learning procedure and declare the best-arm.

In addition to MAB, reinforcement learning-based approaches can also be applied to active learning in order to optimize a decision-making policy that balances exploration of uncertain data with exploitation of information learned from previous observations. This can be particularly useful in applications where the goal is to maximize the expected cumulative reward over time, such as in robotics or game playing. Compared to MAB, reinforcement learning-based approaches offer a more general and flexible framework for active learning, allowing for a wider range of problem formulations and feedback signals [190–192]. One approach to combining active learning and reinforcement learning is through modeling the sampling routine as a contextual-bandit problem, as proposed by

Wasserman et al. [193]. In this approach, the rewards are based on the usefulness of the query behavior of the learner. The key intuition behind the use of reinforcement learning in online active learning is that the learner gets feedback after the requested label, based on how useful the request actually was. In contrast to the traditional active learning view, where most of the effort is dedicated to the instance selection phase, the learner is penalized ex-post for querying useless instances. The learner gets a positive reward �X if it asks for the label when it would have otherwise predicted the wrong class, and a negative reward �# when querying was unnecessary as the model would have predicted the right label. The contextual bandit problem is implemented by building an ensemble of different models, with each expert suggesting whether to query or not based on whether its prediction certainty exceeds a threshold �. The models are assigned a decision power based on how past suggestions were rewarded and how coherent they were with the other experts' suggestions. When an observation is sent to the oracle for labeling, the reward is computed, and the objective function of the learner is to maximize the total reward over a time horizon �. Another reinforcement learning-based approach has been proposed by Woodward and Finn [194]. They considered the case where, at each time step � the learner needs to decide whether to predict the label of the unlabeled data point �" or pay to request its label �". The reinforcement learning framework is used to find an optimal policy �∗(�") that takes into account the cost of asking a label and the cost of making an incorrect prediction, where �" represents the state that is given in input at the time � to a policy �(�") that outputs the suggested action �". The authors approximate the action-value function using a long short-term memory (LSTM) neural network with a linear output layer. The optimal policy is determined by maximizing the long-term reward, after assigning a reward to a label request �<Zk, a correct prediction �L]<< and an incorrect prediction �-YL. It should be noted that �L]<< and �-YL should be negative reward, as they are associated to costly actions. 

## 4 Evaluation strategies
The use of active learning approaches is becoming increasingly common in machine learning, allowing models to be trained more efficiently by selecting the most informative examples for labeling. To evaluate the performance of these approaches, it is typical to compare them to a passive random sampling strategy by generating learning curves that plot the model performance (e.g., accuracy, F1 score, or root mean square error) on a held-out test set over the number of labeled examples used for training. Learning curves are a useful tool for comparing the asymptotic performance of different strategies and their sample efficiency, with the slope of the curve reflecting the rate at which the model performance improves with additional labeled examples. A steeper slope indicates a more sampleefficient strategy.

When multiple sampling strategies are being compared, a visual inspection of the learning curves may not be sufficient, and more rigorous statistical tests may be necessary. Reyes et al. [195] recommend the use of nonparametric statistical tests to analyze the effectiveness of active learning strategies for classification tasks. The sign test [196] or the Wilkinson signed-ranks test [197] can be used to compare two strategies, while the Friedman test [198], the Friedman aligned-ranks test [199], the Friedman test with Iman-Davenport correction [200], or the Quade test [201] can be used when evaluating more than two strategies. These statistical tests can provide insight into whether the difference in performance between the active learning and passive random sampling strategies is statistically significant. Overall, the use of learning curves and statistical tests can provide valuable insights into the effectiveness and efficiency of different active learning strategies. By understanding the statistical significance 27 of differences in performance between these strategies, researchers can make informed decisions about which approaches are more effective for a particular task or dataset.

Furthermore, the choice of the evaluation scheme is crucial when assessing the performance of active learning approaches. If we use an evaluation scheme based on a held-out test set, at each leaning step � the performance of the model is assessed using the same test set. This can be a reasonable approach if we are dealing with a stationary data stream, which does not evolve over time. Under these assumptions, using the same test set we might be able to better assess the prediction improvement as more labeled examples are included to the design. However, this approach might not be ideal when dealing with drifting data streams. In these circumstances, a prequential evaluation scheme can be more useful to monitor the evolution of the prediction error over time [202–205]. In online learning, prequential evaluation is also referred to as test-then-train approach, and it involves using each incoming instance first to measure the prediction error, and then to be included in the training set [114]. For imbalanced data streams, a specific prequential variant of the area under the curve metric has been proposed [206,207]. In the online active learning framework, the prequential evaluation scheme is extensively used [87,124,125,131,208], and its main steps are described in Algorithm 6. The key idea is that at each time step �, we first test the model by making a prediction, then we decide whether to query the true labels, and finally we update our model.

Algorithm 6. Prequential evaluation for online active learning

Require: an initial model; a data stream �; a budget �; an active learning strategy �. 1: Set � = ∅, � ← 0 // storing predictions 2: while � ≤ � and � ≤ |�| do 3: Observe the � "I data point �- ∈ � 4: Predict the label �3- and store it, � = � ∪ �3- 5: if �(�-) = True then 6: Ask for the true label �- and update the model 7: � ← � + 1 8: else 9: Discard �- 10: � ← � + 1 11: end if 12: end while 

## 5 Summary and future directions

This survey has described the problem of performing online active learning with data streams, and explored various strategies for selecting the most informative data points. Table 1 provides a summary of the relevant state-of-theart approaches, highlighting their main properties and settings. Our survey reveals that previous research has mainly focused on the development of online classification models, both for stationary and drifting data streams.

However, there has been relatively little work done on online active linear regression or dedicated to the development of online regression models in general.

Method Data processing Sampling strategy Instance selection criterion Model

Cesa-Bianchi et al. [55,56] Single-pass �-sampling Uncertainty (margin-based) Classifier (Linear)

Dasgupta et al. [58] Single-pass Thresholding Uncertainty (margin-based) Classifier (Linear)

Sculley [60] Single-pass

Multiple: thresholding, �-sampling Uncertainty (margin-based) Classifier (Perceptron, SVM)

Lu et al. [7] Single-pass �-sampling Uncertainty (margin-based) Classifier (Perceptron)

Hao et al. [63] Single-pass �-sampling Multiple: uncertainty (marginbased), variance

Classifier (Perceptron)

Ferdowsi et al. [72] Window Best out of window Multiple: uncertainty (marginbased), density Classifier (SVM)

Zhu et al. [128] Window Best out of window Uncertainty (committee-based) Classifier (decision tree)

Huang et al. [81] Single-pass Thresholding Disagreement Classifier (Ensemble) 28

Hao et al. [91] Single-pass Thresholding Disagreement Classifier (Ensemble)

DeSalvo et al. [84] Single-pass Thresholding Disagreement Classifier (Ensemble)

Shah and Manwani [79] Single-pass Thresholding (bounded region) Expected model change Reject option classifiers

Ghassemi et al. [73] Single-pass Thresholding Uncertainty (margin-based) Classifier (SVM)

Loy et al. [85] Single-pass Rule-based Disagreement Classifier (Bayesian)

Fujii and Kashima [90] Window Best out of window Submodularity Classifier (Bayesian)

Chae and Hong [93] Single-pass Thresholding Disagreement Classifier

Mohamad et al. [87] Single-pass Thresholding Uncertainty (about model parameters) Classifier (Bayesian)

Krawczyk et al. [120] Single-pass Thresholding (dynamic) Multiple: uncertainty, random Classifier (Hoeffding tree)

Castellani et al. [124] Single-pass Thresholding (piecewise constant)

Uncertainty (classifier’s confidence) Classifier

Zhang et al. [119] Single-pass Thresholding (classspecific) Uncertainty (margin-based) Classifier

Shan et al. [129,130] Single-pass Thresholding (dynamic) Multiple: uncertainty (marginbased), random Classifier (Ensemble)

Zhang et al. [132] Single-pass Thresholding (dynamic) Multiple: uncertainty (marginbased), random Classifier (Ensemble)

Ienco et al. [25] Window Best out of window Multiple: density, uncertainty (max. posterior probability) Classifier

Mohamad et al. [133] Single-pass �-sampling Multiple: density (clustering), uncertainty Classifier (Bayesian)

Liu et al. [135] Single-pass Thresholding (multiple) Multiple: density (NN), uncertainty Classifier

Kurlej and Woźniak [138] Window Best out of window Multiple: density (NN), uncertainty (margin-based) Classifier

Chu et al. [139] Single-pass Thresholding Multiple: entropy, function-value, random Classifier (Bayesian)

Riquelme et al. [166] Single-pass Thresholding (dynamic) Uncertainty (A-optimality) Regression (linear)

Cacciarelli et al. [168] Single-pass Thresholding Uncertainty (D-optimality) Regression (linear)

Lughofer [144] Single-pass Thresholding Multiple: diversity (novelty), uncertainty (conflict) Classification (fuzzy)

Lughofer and

Pratama [145] Single-pass Thresholding

Multiple: diversity (novelty), uncertainty (error bars + Aoptimality)

Regression (fuzzy)

Pratama et al. [147] Single-pass Thresholding (dynamic) Uncertainty Classification (fuzzy)

Subramanian et al. [148] Window Thresholding Diversity (novelty) Classification (fuzzy)

Weigl et al. [152] Single-pass Thresholding Uncertainty Classification (fuzzy)

Cernuda et al. [153] Window Thresholding Diversity (�++SPE) Regression (fuzzy)<br/>
Table 1. State-of-the-art approaches for online active learning.

We believe that there are several promising directions for future research in this field. First, we recommend further investigation into online active learning strategies specifically designed for regression models. Given the limited work in this area, there is a need for more advanced methods that can be applied to nonlinear models, beyond linear models or linear bandits. For example, there has been a recent spark of interest toward the use of Bayesian optimization for active learning in nonlinear regression problems [209,210]. Additionally, model-agnostic methods that can be applied to a variety of regression models could be valuable as they would provide a more general solution to the problem. Second, we believe that there is potential for research into single-pass online sampling strategies for dynamic data streams. Ensemble models and batch-based approaches have been the dominant methods in online classification, but some of their assumptions or requirements may not hold in many real-world applications.

For instance, in some applications, data may arrive in a continuous stream, and it may not be possible to divide it into batches due to time or memory constraints. In such cases, single-pass online sampling strategies that do not require the use or update of multiple models would be more practical. Moreover, it could be beneficial to develop online active learning strategies that are able tackle all the types of distribution shifts introduced in section 3.2.

Finally, the combination of reinforcement learning and active learning in pool-based scenarios is an area of ongoing 29 research. We believe that the study of online reinforcement learning to optimize sampling strategies could provide valuable insights into how to best perform active learning in dynamic environments. 

## 6 Conclusion

In conclusion, the field of online active learning with data streams is a rapidly evolving and highly relevant area of research in machine learning. The ability to effectively learn from data streams in real-time is becoming increasingly important, as the amount of data generated by modern applications continues to grow at an exponential rate.

However, obtaining annotated data to train complex prediction and decision-making models presents a major roadblock. This hinders the proper integration of artificial intelligence models with real-world applications such as healthcare, autonomous driving and industrial production. Our survey provides a comprehensive overview of the current state of the art in this field and highlights the challenges and opportunities that researchers face when developing methods for online active learning. We have reviewed a wide range of strategies for selecting the most informative data points in online active learning, including methods based on uncertainty sampling, diversity sampling, query by committee, and reinforcement learning, among others. Our analysis has shown that these strategies have been applied in a variety of contexts, including online classification, online regression, and online semi-supervised learning. We hope that this survey will inspire further research in the field of online active learning with data streams and encourage the development of new and advanced methods for handling this type of data. In particular, we believe that there is significant potential for the development of model-agnostic and single-pass online active learning strategies that can be applied in practical settings.

## References
1. C.C. Aggarwal, X. Kong, Q. Gu, J. Han, P.S. Yu, Chapter 22 Active Learning: A Survey, n.d.
2. B. Settles, Computer Sciences Department Active Learning Literature Survey, 2009.
3. Y. Fu, X. Zhu, B. Li, A survey on instance selection for active learning, Knowl Inf Syst. 35 (2013) 249–283. https://doi.org/10.1007/s10115-012-0507-8.
4. P. Kumar, A. Gupta, Active Learning Query Strategies for Classification, Regression, and Clustering: A Survey, J Comput Sci Technol. 35 (2020) 913–945. https://doi.org/10.1007/s11390-020-9487-4.
5. Y. Fu, X. Zhu, B. Li, A survey on instance selection for active learning, Knowl Inf Syst. 35 (2013) 249–283. https://doi.org/10.1007/s10115-012-0507-8.
6. E. Lughofer, On-line active learning: A new paradigm to improve practical useability of data stream modeling methods, Inf Sci (N Y). 415–416 (2017) 356–376. https://doi.org/10.1016/j.ins.2017.06.038.
7. J. Lu, P. Zhao, S.C.H. Hoi, Online Passive-Aggressive Active learning, Mach Learn. 103 (2016) 141–183. https://doi.org/10.1007/s10994-016-5555-y.
8. S. Tong, D. Koller, Support vector machine active learning with applications to text classification, The Journal of Machine Learning Research. 2 (2002). https://doi.org/10.1162/153244302760185243.
9. D. Roth, K. Small, Margin-Based Active Learning for Structured Output Spaces, in: 2006: pp. 413–424. https://doi.org/10.1007/11871842_40.
10. M.-F. Balcan, A. Broder, T. Zhang, Margin Based Active Learning, in: Lecture Notes in Computer Science, 2007.
11. D.A. Cohn, Z. Ghahramani, M.I. Jordan, Active Learning with Statistical Models, 1996.
12. N. Roy, A. McCallum, Toward optimal active learning through sampling estimation of error reduction, in: Proceedings of the Eighteenth International Conference on Machine Learning (ICML), 2001.
13. W. Cai, Y. Zhang, J. Zhou, Maximizing expected model change for active learning in regression, in: Proceedings - IEEE International Conference on Data Mining, ICDM, 2013: pp. 51–60. https://doi.org/10.1109/ICDM.2013.104.
14. S. Hanneke, Theory of Disagreement-Based Active Learning, Foundations and Trends® in Machine Learning. 7 (2014) 131–309. https://doi.org/10.1561/2200000037.
15. L. Wang, Smoothness, Disagreement Coefficient, and the Label Complexity of Agnostic Active Learning, The Journal of Machine Learning Research. (2011).
16. S. Hanneke, L. Yang, Minimax Analysis of Active Learning, (2014).
17. V.S. Sheng, F. Provost, P.G. Ipeirotis, Get another label? improving data quality and data mining using multiple, noisy labelers, in: Proceeding of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 08, ACM Press, New York, New York, USA, 2008: p. 614. https://doi.org/10.1145/1401890.1401965.
18. H.S. Seung, M. Opper, H. Sompolinsky, Query by committee, in: Proceedings of the Fifth Annual Workshop on Computational Learning Theory - COLT ’92, ACM Press, New York, New York, USA, 1992: pp. 287–294. https://doi.org/10.1145/130385.130417.
19. Y. Freund, H.S. Seung, E. Shamir, N. Tishby, Selective Sampling Using the Query by Committee Algorithm, Kluwer Academic Publishers, 1997.
20. R. Burbidge, J.J. Rowland, R.D. King, Active Learning for Regression based on Query by Committee, n.d.
21. Z. Ge, Active learning strategy for smart soft sensor development under a small number of labeled data samples, J Process Control. 24 (2014) 1454–1461. https://doi.org/10.1016/j.jprocont.2014.06.015.
22. D. Cacciarelli, M. Kulahci, J. Tyssedal, Online Active Learning for Soft Sensor Development using Semi-Supervised Autoencoders, in: ICML 2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World, Baltimore, US, 2022. https://arxiv.org/abs/2212.13067.
23. H.T. Nguyen, A. Smeulders SMEULDERS, Active Learning Using Pre-clustering, 2004.
24. F. Min, S.M. Zhang, D. Ciucci, M. Wang, Three-way active learning through clustering selection, International Journal of Machine Learning and Cybernetics. 11 (2020) 1033–1046. https://doi.org/10.1007/s13042-020-01099-2. 30
25. D. Ienco, A. Bifet, ˇ Zliobait, B. Pfahringer, LNAI 8140 - Clustering Based Active Learning for Evolving Data Streams, n.d. https://www.mturk.com.
26. H. Zhang, S.S. Ravi, I. Davidson, A Graph-Based Approach for Active Learning in Regression, (2020). http://arxiv.org/abs/2001.11143.
27. J. Long, J. Yin, W. Zhao, E. Zhu, Graph-Based Active Learning Based on Label Propagation, in: 2008: pp. 179–190. https://doi.org/10.1007/978-3-540-88269-5_17.
28. P. Donmez, J. Carbonell, P. Bennet, Dual Strategy Active Learning, in: Lecture Notes in Computer Science, 2007.
29. S.-J. Huang, R. Jin, Z.-H. Zhou, Active Learning by Querying Informative and Representative Examples, IEEE Trans Pattern Anal Mach Intell. 36 (2014) 1936–1949. https://doi.org/10.1109/TPAMI.2014.2307881.
30. E.B. Baum, K. Lang, Query learning can work poorly when a human oracle is used, in: Proceedings of the IEEE International Joint Conference on Neural Networks, 1992.
31. P. Ren, Y. Xiao, X. Chang, P.-Y. Huang, Z. Li, B.B. Gupta, X. Chen, X. Wang, A Survey of Deep Active Learning, ACM Comput Surv. 54 (2022) 1–40. https://doi.org/10.1145/3472291.
32. I.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative Adversarial Networks, (2014).
33. J.-J. Zhu, J. Bento, Generative Adversarial Active Learning, (2017).
34. D.P. Kingma, M. Welling, Auto-Encoding Variational Bayes, (2013).
35. T. Tran, T.-T. Do, I. Reid, G. Carneiro, Bayesian Generative Active Deep Learning, in: Proceedings of the 36th International Conference on Machine Learning, 2019.
36. T. Tran, T. Pham, G. Carneiro, L. Palmer, I. Reid, A Bayesian Data Augmentation Approach for Learning Deep Models, (2017).
37. X. Li, Y. Guo, Adaptive Active Learning for Image Classification, in: 2013 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2013: pp. 859–866. https://doi.org/10.1109/CVPR.2013.116.
38. A. Vahdat, M. Belbahri, V.P. Nia, Active Learning for High-Dimensional Binary Features, (2019).
39. E.C. Shannon, A Mathematical Theory of Communication, The Bell System Technical Journal. (1948).
40. J. Wu, J. Chen, D. Huang, Entropy-based Active Learning for Object Detection with Progressive Diversity Constraint, in: CVPR 2022, 2022.
41. E. Haussmann, M. Fenzi, K. Chitta, J. Ivanecky, H. Xu, D. Roy, A. Mittel, N. Koumchatzky, C. Farabet, J.M. Alvarez, Scalable Active Learning for Object Detection, (2020).
42. S. Schmidt, Q. Rao, J. Tatsch, A. Knoll, Advanced Active Learning Strategies for Object Detection, in: 2020 IEEE Intelligent Vehicles Symposium (IV), IEEE, 2020: pp. 871–876. https://doi.org/10.1109/IV47402.2020.9304565.
43. P.R. Freeman, The Secretary Problem and Its Extensions: A Review, 1983.
44. C. Riquelme, ONLINE ACTIVE LEARNING WITH LINEAR MODELS A DISSERTATION SUBMITTED TO THE DEPARTMENT OF MATHEMATICAL AND COMPUTATIONAL ENGINEERING AND THE COMMITTEE ON GRADUATE STUDIES OF STANFORD UNIVERSITY IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF PHILOSOPHY, 2017. http://purl.stanford.edu/rp382fv8012.
45. V. Souza, T. Pinho, G. Batista, Evaluating Stream Classifiers with Delayed Labels Information, in: 2018 7th Brazilian Conference on Intelligent Systems (BRACIS), IEEE, 2018: pp. 408–413. https://doi.org/10.1109/BRACIS.2018.00077.
46. R. Polikar, L. Upda, S.S. Upda, V. Honavar, Learn++: an incremental learning algorithm for supervised neural networks, IEEE Transactions on Systems, Man and Cybernetics, Part C (Applications and Reviews). 31 (2001) 497–508. https://doi.org/10.1109/5326.983933.
47. Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, Y. Fu, Large Scale Incremental Learning, (2019).
48. A. Shilton, M. Palaniswami, D. Ralph, A.C. Tsoi, Incremental Training of Support Vector Machines, IEEE Trans Neural Netw. 16 (2005) 114–131. https://doi.org/10.1109/TNN.2004.836201.
49. R. Istrate, A.C.I. Malossi, C. Bekas, D. Nikolopoulos, Incremental Training of Deep Convolutional Neural Networks, (2018).
50. R. v. Kulkarni, S.H. Patil, R. Subhashini, An overview of learning in data streams with label scarcity, in: Proceedings of the International Conference on Inventive Computation Technologies, ICICT 2016, Institute of Electrical and Electronics Engineers Inc., 2016. https://doi.org/10.1109/INVENTIVE.2016.7824874.
51. J.M. Rožanec, E. Trajkova, P. Dam, B. Fortuna, D. Mladenic, Streaming Machine Learning and Online Active Learning for Automated Visual Inspection., in: IFAC-PapersOnLine, Elsevier B.V., 2022: pp. 277–282. https://doi.org/10.1016/j.ifacol.2022.04.206.
52. C. Baykal, K. Trinh, F. Iliopoulos, G. Menghani, E. Vee, Robust Active Distillation, (2022).
53. T.N. Hoang, S. Hong, C. Xiao, B. Low, J. Sun, AID: Active Distillation Machine to Leverage Pre-Trained Black-Box Models in Private Data Settings, in: Proceedings of the Web Conference 2021, ACM, New York, NY, USA, 2021: pp. 3569–3581. https://doi.org/10.1145/3442381.3449944.
54. B. Kwak, Y. Kim, Y.J. Kim, S. Hwang, J. Yeo, TrustAL: Trustworthy Active Learning using Knowledge Distillation, in: AAAI 2022, 2022.
55. N. Cesa-Bianchi, C. Gentile, L. Zaniboni, Worst-Case Analysis of Selective Sampling for Linear-Threshold Algorithms, in: Adv Neural Inf Process Syst, 2004.
56. N. Cesa-Bianchi, C. Gentile, L. Zaniboni, Worst-Case Analysis of Selective Sampling for Linear Classification, The Journal of Machine Learning Research. 7 (2006).
57. F. Rosenblatt, The perceptron: A probabilistic model for information storage and organization in the brain., Psychol Rev. 65 (1958) 386–408. https://doi.org/10.1037/h0042519.
58. S. Dasgupta, A.T. Kalai, C. Monteleoni, Analysis of Perceptron-Based Active Learning, in: 2005: pp. 249–263. https://doi.org/10.1007/11503415_17.
59. S. Hampson, D. Kibler, Minimum generalization via reflection: A fast linear threshold learner, Mach Learn. 37 (1999).
60. D. Sculley, Online active learning methods for fast label efficient spam filtering, in: Proceedings of the Fourth Conference on Email and AntiSpam, Mountain View, California, 2007.
61. K. Crammer, O. Dekel, J. Keshet, S. Shalev-Schwartz, Y. Singer, Online passive-aggressive algorithms, The Journal of Machine Learning Research. (2006).
62. A. Bordes, S. Ertekin, J. Weston, L. Bottou, Fast kernel classifiers with online and active learning, The Journal of Machine Learning Research. 6 (2005).
63. S. Hao, J. Lu, P. Zhao, C. Zhang, S.C.H. Hoi, C. Miao, Second-Order Online Active Learning and Its Applications, IEEE Trans Knowl Data Eng. 30 (2018) 1338–1351. https://doi.org/10.1109/TKDE.2017.2778097.
64. N. Houlsby, J. Miguel Hernandez-Lobato, Z. Ghahramani, Cold-start active learning with robust ordinal matrix factorization, in: 31st International Conference on Machine Learning, 2014. 31
65. M. Yuan, H.-T. Lin, J. Boyd-Graber, Cold-start Active Learning through Self-supervised Language Modeling, (2020).
66. Q. Jin, M. Yuan, S. Li, H. Wang, M. Wang, Z. Song, Cold-start active learning for image classification, Inf Sci (N Y). 616 (2022) 16–36. https://doi.org/10.1016/j.ins.2022.10.066.
67. J.M. Joyce, Kullback-Leibler Divergence, in: International Encyclopedia of Statistical Science, Springer Berlin Heidelberg, Berlin, Heidelberg, 2011: pp. 720–722. https://doi.org/10.1007/978-3-642-04898-2_327.
68. J. Qin, C. Wang, Q. Zou, Y. Sun, B. Chen, Active learning with extreme learning machine for online imbalanced multiclass classification, Knowl Based Syst. 231 (2021) 107385. https://doi.org/10.1016/j.knosys.2021.107385.
69. G.-B. Huang, Q.-Y. Zhu, C.-K. Siew, Extreme learning machine: Theory and applications, Neurocomputing. 70 (2006) 489–501. https://doi.org/10.1016/j.neucom.2005.12.126.
70. H. Yu, C. Sun, W. Yang, X. Yang, X. Zuo, AL-ELM: One uncertainty-based active learning algorithm using extreme learning machine, Neurocomputing. 166 (2015) 140–150. https://doi.org/10.1016/j.neucom.2015.04.019.
71. A.J. Joshi, F. Porikli, N. Papanikolopoulos, Multi-class active learning for image classification, in: 2009 IEEE Conference on Computer Vision and Pattern Recognition, IEEE, 2009: pp. 2372–2379. https://doi.org/10.1109/CVPR.2009.5206627.
72. Z. Ferdowsi, R. Ghani, R. Settimi, Online Active Learning with Imbalanced Classes, in: 2013 IEEE 13th International Conference on Data Mining, IEEE, 2013: pp. 1043–1048. https://doi.org/10.1109/ICDM.2013.12.
73. M. Ghassemi, A.D. Sarwate, R.N. Wright, Differentially private online active learning with applications to anomaly detection, in: AISec 2016 - Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security, Co-Located with CCS 2016, Association for Computing Machinery, Inc, 2016: pp. 117–128. https://doi.org/10.1145/2996758.2996766.
74. F. McSherry, K. Talwar, Mechanism design via differential privacy, in: 48th Annual IEEE Symposium on Foundations of Computer Science (FOCS’07), IEEE, 2007: pp. 94–103. https://doi.org/10.1109/FOCS.2007.41.
75. R. Bassily, A. Smith, A. Thakurta, Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds, in: 2014 IEEE 55th Annual Symposium on Foundations of Computer Science, IEEE, 2014: pp. 464–473. https://doi.org/10.1109/FOCS.2014.56.
76. S. Song, K. Chaudhuri, A.D. Sarwate, Stochastic gradient descent with differentially private updates, in: 2013 IEEE Global Conference on Signal and Information Processing, IEEE, 2013: pp. 245–248. https://doi.org/10.1109/GlobalSIP.2013.6736861.
77. J.C. Duchi, M.I. Jordan, M.J. Wainwright, Local Privacy and Statistical Minimax Rates, in: 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, IEEE, 2013: pp. 429–438. https://doi.org/10.1109/FOCS.2013.53.
78. L. Ma, S. Destercke, Y. Wang, Online active learning of decision trees with evidential data, Pattern Recognit. 52 (2016) 33–45. https://doi.org/10.1016/j.patcog.2015.10.014.
79. K. Shah, N. Manwani, Online Active Learning of Reject Option Classifiers, Proceedings of the AAAI Conference on Artificial Intelligence. 34 (2020) 5652–5659. https://doi.org/10.1609/aaai.v34i04.6019.
80. N. Manwani, K. Desai, S. Sasidharan, R. Sundararajan, Double Ramp Loss Based Reject Option Classifier, in: 19th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining (PAKDD), 2013. https://doi.org/10.1007/978-3-319-57454- 7_53.
81. B. Huang, S. Salgia, Q. Zhao, Disagreement-Based Active Learning in Online Settings, IEEE Transactions on Signal Processing. 70 (2022) 1947–1958. https://doi.org/10.1109/TSP.2022.3159388.
82. E. Mammen, A.B. Tsybakov, Smooth discrimination analysis, The Annals of Statistics. 27 (1999). https://doi.org/10.1214/aos/1017939240.
83. A.B. Tsybakov, Optimal Aggregation of Classifiers in Statistical Learning, The Annals of Statistics. (2004).
84. G. Desalvo, C. Gentile, T.S. Thune, Online Active Learning with Surrogate Loss Functions, n.d.
85. C.C. Loy, T.M. Hospedales, T. Xiang, S. Gong, Stream-based joint exploration-exploitation active learning, in: Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2012: pp. 1560–1567. https://doi.org/10.1109/CVPR.2012.6247847.
86. J. Pitman, M. Yor, The Two-Parameter Poisson-Dirichlet Distribution Derived from a Stable Subordinator., The Annals of Probability. 25 (1997).
87. S. Mohamad, M. Sayed-Mouchaweh, A. Bouchachia, Online active learning for human activity recognition from sensory data streams, Neurocomputing. 390 (2020) 341–358. https://doi.org/10.1016/j.neucom.2019.08.092.
88. G. Taylor, G. Hinton, Factored Conditional Restricted Boltzmann Machines for Modeling Motion Style, in: Proceedings of the 26th International Conference on Machine Learning, Montreal, Canada, 2009, 2009.
89. G. Taylor, G. Hinton, S. Roweis, Modeling Human Motion Using Binary Latent Variables, in: Advances in Neural Information Processing Systems 19 (NIPS 2006), 2006.
90. K. Fujii, H. Kashima, Budgeted stream-based active learning via adaptive submodular maximization, n.d.
91. S. Hao, P. Hu, P. Zhao, S.C.H. Hoi, C. Miao, Online active learning with expert advice, ACM Trans Knowl Discov Data. 12 (2018). https://doi.org/10.1145/3201604.
92. N. Cesa-Bianchi, G. Lugosi, Prediction, Learning, and Games, Cambridge University Press, 2006. https://doi.org/10.1017/CBO9780511546921.
93. J. Chae, S. Hong, Stream-Based Active Learning with Multiple Kernels, in: 2021 International Conference on Information Networking (ICOIN), IEEE, 2021: pp. 718–722. https://doi.org/10.1109/ICOIN50884.2021.9333940.
94. R. Jin, S. Hoi, T. Yang, Online Multiple Kernel Learning: Algorithms and Mistake Bounds, in: Proceedings of the 21st International Conference on Algorithmic Learning Theory, 2010.
95. S.C.H. Hoi, R. Jin, P. Zhao, T. Yang, Online Multiple Kernel Classification, Mach Learn. 90 (2013) 289–316. https://doi.org/10.1007/s10994-012-5319-2.
96. Y. Shen, T. Chen, G.B. Giannakis, Random Feature-based Online Multi-kernel Learning in Environments with Unknown Dynamics, (2017).
97. O. Sener, S. Savarese, Active Learning for Convolutional Neural Networks: A Core-Set Approach, (2017). http://arxiv.org/abs/1708.00489.
98. S. Agarwal, H. Arora, S. Anand, C. Arora, Contextual Diversity for Active Learning, (2020). http://arxiv.org/abs/2008.05723.
99. J.T. Ash, C. Zhang, A. Krishnamurthy, J. Langford, A. Agarwal, Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds, (2019). http://arxiv.org/abs/1906.03671.
100. G. Citovsky, G. DeSalvo, C. Gentile, L. Karydas, A. Rajagopalan, A. Rostamizadeh, S. Kumar, Batch Active Learning at Scale, (2021). http://arxiv.org/abs/2107.14263.
101. V. Prabhu, A. Chandrasekaran, K. Saenko, J. Hoffman, G. Tech, Active Domain Adaptation via Clustering Uncertainty-weighted Embeddings, n.d. https://github.com/virajprabhu/CLUE. 32
102. A. Narr, R. Triebel, D. Cremers, Stream-based Active Learning for efficient and adaptive classification of 3D objects, in: Proc IEEE Int Conf Robot Autom, Institute of Electrical and Electronics Engineers Inc., 2016: pp. 227–233. https://doi.org/10.1109/ICRA.2016.7487138.
103. B. Lakshminarayanan, D. Roy, Y. Whye Teh, Mondrian Forests: Efficient Online Random Forests, in: Advances in Neural Information Processing Systems (NIPS), 2014.
104. K. He, X. Zhang, S. Ren, J. Sun, Deep Residual Learning for Image Recognition, (2015).
105. A. Kraskov, H. Stögbauer, P. Grassberger, Estimating mutual information, Phys Rev E. 69 (2004) 066138. https://doi.org/10.1103/PhysRevE.69.066138.
106. J. Hua, Z. Xiong, J. Lowey, E. Suh, E.R. Dougherty, Optimal number of features as a function of sample size for various classification rules, Bioinformatics. 21 (2005) 1509–1515. https://doi.org/10.1093/bioinformatics/bti171.
107. G. Hulten, L. Spencer, P. Domingos, Mining time-changing data streams, in: Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’01, ACM Press, New York, New York, USA, 2001: pp. 97–106. https://doi.org/10.1145/502512.502529.
108. A. Bifet, R. Gavaldà, Adaptive Learning from Evolving Data Streams, in: 2009: pp. 249–260. https://doi.org/10.1007/978-3-642- 03915-7_22.
109. H. Gouk, B. Pfahringer, E. Frank, Stochastic Gradient Trees, (2019).
110. C. Zhou, X. Ma, P. Michel, G. Neubig, Examining and Combating Spurious Features under Distribution Shift, in: Proceedings of the 38 Th International Conference on Machine Learning, 2021. https://github.com/violet-zct/.
111. R. Wu, C. Guo, Y. Su, K.Q. Weinberger, Online Adaptation to Label Distribution Shift, in: 35th Conference on Neural Information Processing Systems (NeurIPS 2021), 2021. https://www.kaggle.com/Cornell-University/arxiv.
112. A. Li, A. Boyd, P. Smyth, S. Mandt, Detecting and Adapting to Irregular Distribution Shifts in Bayesian Online Learning, in: 35th Conference on Neural Information Processing Systems (NeurIPS 2021), 2021.
113. L. Baier, T. Schlör, J. Schöffer, N. Kühl, Detecting Concept Drift With Neural Network Model Uncertainty, (2021). http://arxiv.org/abs/2107.01873.
114. A.L. Suárez-Cetrulo, D. Quintana, A. Cervantes, A survey on machine learning for recurring concept drifting data streams, Expert Syst Appl. 213 (2023) 118934. https://doi.org/10.1016/j.eswa.2022.118934.
115. R.J. Zwanka, C. Buff, COVID-19 Generation: A Conceptual Framework of the Consumer Behavioral Shifts to Be Caused by the COVID-19 Pandemic, J Int Consum Mark. 33 (2021) 58–67. https://doi.org/10.1080/08961530.2020.1771646.
116. A. Tsymbal, M. Pechenizkiy, P. Cunningham, S. Puuronen, Dynamic integration of classifiers for handling concept drift, Information Fusion. 9 (2008) 56–68. https://doi.org/10.1016/j.inffus.2006.11.002.
117. J. Lu, A. Liu, F. Dong, F. Gu, J. Gama, G. Zhang, Learning under Concept Drift: A Review, IEEE Trans Knowl Data Eng. (2018) 1–1. https://doi.org/10.1109/TKDE.2018.2876857.
118. M. Lima, M. Neto, T.S. Filho, R.A. de A. Fagundes, Learning Under Concept Drift for Regression—A Systematic Literature Review, IEEE Access. 10 (2022) 45410–45429. https://doi.org/10.1109/ACCESS.2022.3169785.
119. H. Zhang, W. Liu, L. Sun, L. Chen, Z. Ding, Q. Liu, Analyzing network traffic for protocol identification: An ensemble online active learning method, in: Proceedings - 2020 6th International Conference on Big Data and Information Analytics, BigDIA 2020, Institute of Electrical and Electronics Engineers Inc., 2020: pp. 167–172. https://doi.org/10.1109/BigDIA51454.2020.00035.
120. B. Krawczyk, B. Pfahringer, M. Wozniak, Combining active learning with concept drift detection for data stream mining, in: 2018 IEEE International Conference on Big Data (Big Data), IEEE, 2018: pp. 2239–2244. https://doi.org/10.1109/BigData.2018.8622549.
121. J. Gama, P. Medas, G. Castillo, P. Rodrigues, Learning with drift detection, Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 3171 (2004) 286–295. https://doi.org/10.1007/978-3- 540-28645-5_29.
122. A. Bifet, R. Gavaldà, Learning from Time-Changing Data with Adaptive Windowing, in: Proceedings of the 2007 SIAM International Conference on Data Mining, Society for Industrial and Applied Mathematics, Philadelphia, PA, 2007: pp. 443–448. https://doi.org/10.1137/1.9781611972771.42.
123. R.N. Gemaque, A.F.J. Costa, R. Giusti, E.M. dos Santos, An overview of unsupervised drift detection methods, Wiley Interdiscip Rev Data Min Knowl Discov. 10 (2020). https://doi.org/10.1002/widm.1381.
124. A. Castellani, S. Schmitt, B. Hammer, Stream-based Active Learning with Verification Latency in Non-stationary Environments, (2022). https://doi.org/10.1007/978-3-031-15937-4_22.
125. T. Pham, D. Kottke, G. Krempl, B. Sick, Stream-based active learning for sliding windows under the influence of verification latency, Mach Learn. 111 (2022) 2011–2036. https://doi.org/10.1007/s10994-021-06099-z.
126. I. Zliobaite, A. Bifet, B. Pfahringer, G. Holmes, Active Learning With Drifting Streaming Data, IEEE Trans Neural Netw Learn Syst. 25 (2014) 27–39. https://doi.org/10.1109/TNNLS.2012.2236570.
127. D. Kottke, G. Krempl, M. Spiliopoulou, Probabilistic Active Learning in Datastreams, in: 2015: pp. 145–157. https://doi.org/10.1007/978-3-319-24465-5_13.
128. X. Zhu, P. Zhang, X. Lin, Y. Shi, Active learning from data streams, in: Proceedings - IEEE International Conference on Data Mining, ICDM, 2007: pp. 757–762. https://doi.org/10.1109/ICDM.2007.101.
129. J. Shan, H. Zhang, W. Liu, Q. Liu, Online Active Learning Ensemble Framework for Drifted Data Streams, IEEE Trans Neural Netw Learn Syst. 30 (2019) 486–498. https://doi.org/10.1109/TNNLS.2018.2844332.
130. H. Zhang, W. Liu, J. Shan, Q. Liu, Online Active Learning Paired Ensemble for Concept Drift and Class Imbalance, IEEE Access. 6 (2018) 73815–73828. https://doi.org/10.1109/ACCESS.2018.2882872.
131. W. Xu, F. Zhao, Z. Lu, Active learning over evolving data streams using paired ensemble framework, in: 2016 Eighth International Conference on Advanced Computational Intelligence (ICACI), IEEE, 2016: pp. 180–185. https://doi.org/10.1109/ICACI.2016.7449823.
132. H. Zhang, W. Liu, Q. Liu, Reinforcement Online Active Learning Ensemble for Drifting Imbalanced Data Streams, IEEE Trans Knowl Data Eng. 34 (2022) 3971–3983. https://doi.org/10.1109/TKDE.2020.3026196.
133. S. Mohamad, A. Bouchachia, M. Sayed-Mouchaweh, A Bi-Criteria Active Learning Algorithm for Dynamic Data Streams, IEEE Trans Neural Netw Learn Syst. 29 (2018) 74–86. https://doi.org/10.1109/TNNLS.2016.2614393.
134. A. Bouchachia, C. Vanaret, GT2FC: An Online Growing Interval Type-2 Self-Learning Fuzzy Classifier, IEEE Transactions on Fuzzy Systems. 22 (2014) 999–1018. https://doi.org/10.1109/TFUZZ.2013.2279554.
135. S. Liu, S. Xue, J. Wu, C. Zhou, J. Yang, Z. Li, J. Cao, Online Active Learning for Drifting Data Streams, IEEE Trans Neural Netw Learn Syst. (2021). https://doi.org/10.1109/TNNLS.2021.3091681.
136. H. Ebbinghaus, Memory: A Contribution to Experimental Psychology, Ann Neurosci. 20 (2013). https://doi.org/10.5214/ans.0972.7531.200408. 33
137. D. Ienco, B. Pfahringer, I. Žliobaitė, High density-focused uncertainty sampling for active learning over evolving stream data, in: BIGMINE’14: Proceedings of the 3rd International Conference on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications, 2014.
138. B. Kurlej, M. Woźniak, Learning Curve in Concept Drift While Using Active Learning Paradigm, in: 2011: pp. 98–106. https://doi.org/10.1007/978-3-642-23857-4_13.
139. W. Chu, M. Zinkevich, L. Li, A. Thomas, B. Tseng, Unbiased online active learning in data streams, in: Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’11, ACM Press, New York, New York, USA, 2011: p. 195. https://doi.org/10.1145/2020408.2020444.
140. T.P. Minka, A family of algorithms for approximate Bayesian inference, Ph.D. Thesis, Massachusetts Institute of Technology, 2001.
141. E. Lughofer, Evolving Fuzzy Systems – Methodologies, Advanced Concepts and Applications, Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. https://doi.org/10.1007/978-3-642-18087-3.
142. D. Ge, X.-J. Zeng, Learning data streams online — An evolving fuzzy system approach with self-learning/adaptive thresholds, Inf Sci (N Y). 507 (2020) 172–184. https://doi.org/10.1016/j.ins.2019.08.036.
143. X. Gu, J. Han, Q. Shen, P.P. Angelov, Autonomous learning for fuzzy systems: a review, Artif Intell Rev. (2022). https://doi.org/10.1007/s10462-022-10355-6.
144. E. Lughofer, Single-pass active learning with conflict and ignorance, Evolving Systems. 3 (2012) 251–271. https://doi.org/10.1007/s12530-012-9060-7.
145. E. Lughofer, M. Pratama, Online Active Learning in Data Stream Regression Using Uncertainty Sampling Based on Evolving Generalized Fuzzy Models, IEEE Transactions on Fuzzy Systems. 26 (2018) 292–309. https://doi.org/10.1109/TFUZZ.2017.2654504.
146. I. Škrjanc, Confidence interval of fuzzy models: An example using a waste-water treatment plant, Chemometrics and Intelligent Laboratory Systems. 96 (2009) 182–187. https://doi.org/10.1016/j.chemolab.2009.01.009.
147. M. Pratama, S.G. Anavatti, J. Lu, Recurrent Classifier Based on an Incremental Metacognitive-Based Scaffolding Algorithm, IEEE Transactions on Fuzzy Systems. 23 (2015) 2048–2066. https://doi.org/10.1109/TFUZZ.2015.2402683.
148. K. Subramanian, A.K. Das, S. Sundaram, S. Ramasamy, A meta-cognitive interval type-2 fuzzy inference system and its projection based learning algorithm, Evolving Systems. 5 (2014) 219–230. https://doi.org/10.1007/s12530-013-9102-9.
149. H. Hoffmann, Kernel PCA for novelty detection, Pattern Recognit. 40 (2007) 863–874. https://doi.org/10.1016/j.patcog.2006.07.009.
150. S. Suresh, N. Sundararajan, P. Saratchandran, Risk-sensitive loss functions for sparse multi-category classification problems, Inf Sci (N Y). 178 (2008) 2621–2638. https://doi.org/10.1016/j.ins.2008.02.009.
151. T. Zhang, Statistical behavior and consistency of classification methods based on convex risk minimization, The Annals of Statistics. 32 (2004). https://doi.org/10.1214/aos/1079120130.
152. E. Weigl, W. Heidl, E. Lughofer, T. Radauer, C. Eitzinger, On improving performance of surface inspection systems by online active learning and flexible classifier updates, Mach Vis Appl. 27 (2016) 103–127. https://doi.org/10.1007/s00138-015-0731-9.
153. C. Cernuda, E. Lughofer, G. Mayr, T. Röder, P. Hintenaus, W. Märzinger, J. Kasberger, Incremental and decremental active learning for optimized self-adaptive calibration in viscose production, Chemometrics and Intelligent Laboratory Systems. 138 (2014) 14–29. https://doi.org/10.1016/j.chemolab.2014.07.008.
154. D. Cacciarelli, M. Kulahci, A novel fault detection and diagnosis approach based on orthogonal autoencoders, Comput Chem Eng. 163 (2022) 107853. https://doi.org/10.1016/j.compchemeng.2022.107853.
155. S. Gajjar, M. Kulahci, A. Palazoglu, Real-time fault detection and diagnosis using sparse principal component analysis, J Process Control. 67 (2018) 112–128. https://doi.org/10.1016/j.jprocont.2017.03.005.
156. E. Vanhatalo, M. Kulahci, B. Bergquist, On the structure of dynamic principal component analysis used in statistical process monitoring, Chemometrics and Intelligent Laboratory Systems. 167 (2017) 1–11. https://doi.org/10.1016/j.chemolab.2017.05.016.
157. E. Vanhatalo, M. Kulahci, Impact of autocorrelation on principal components and their use in statistical process control, Qual Reliab Eng Int. 32 (2016) 1483–1500. https://doi.org/10.1002/qre.1858.
158. S. Karlin, William J. Studden, Optimal Experimental Designs, The Annals of Mathematical Statistics. 37 (1966) 783–815.
159. J. Kiefer, Optimum Experimental Designs, Journal of the Royal Statistical Society. Series B (Methodological). (1959).
160. S.P. Asprey, S. Macchietto, Designing robust optimal dynamic experiments, J Process Control. 12 (2002) 545–556. https://doi.org/10.1016/S0959-1524(01)00020-8.
161. F. Galvanin, Optimal model-based design of experiments in dynamic systems: novel techniques and unconventional applications, Università degli Studi di Padova, 2010.
162. R.H. Myers, D. Montgomery, C.M. Anderson-Cook, Response surface methodology: process and product optimization using designed experiments, 2016.
163. R.C. st. John, N.R. Draper, D-Optimality for Regression Designs: A Review, Technometrics. 17 (1975) 15–23. https://doi.org/10.1080/00401706.1975.10489266.
164. K. Jamieson, Online and Adaptive Machine Learning. Regression (Part 7), Washington University. (2018).
165. D.C. Montgomery, Design and Analysis of Experiments, John Wiley & Sons, Inc., Hoboken, NJ, USA, 2012. https://doi.org/10.1002/9781118147634.
166. C. Riquelme, R. Johari, B. Zhang, Online Active Linear Regression via Thresholding, in: Thirty-First AAAI Conference on Artificial Intelligence, 2017. www.aaai.org.
167. X. Fontaine, P. Perrault, M. Valko, V. Perchet, Online A-Optimal Design and Active Linear Regression, 2021.
168. D. Cacciarelli, M. Kulahci, J.S. Tyssedal, Stream-based active learning with linear models, Knowl Based Syst. 254 (2022) 109664. https://doi.org/10.1016/j.knosys.2022.109664.
169. D. Cacciarelli, M. Kulahci, J.S. Tyssedal, Robust online active learning, (2023).
170. C. Riquelme, M. Ghavamzadeh, A. Lazaric, Active Learning for Accurate Estimation of Linear Models, in: Proceedings of the 34th International Conference on Machine Learning, 2017.
171. Y. Ruan, J. Yang, Y. Zhou, Linear Bandits with Limited Adaptivity and Learning Distributional Optimal Design, (2020).
172. J.-Y. Audibert, R. Munos, Best Arm Identification in Multi-Armed Bandits, (2010).
173. K. Jamieson, R. Nowak, Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting, in: 2014 48th Annual Conference on Information Sciences and Systems (CISS), IEEE, 2014: pp. 1–6. https://doi.org/10.1109/CISS.2014.6814096.
174. M. Soare, A. Lazaric, R. Munos, Active Learning in Linear Stochastic Bandits, in: Bayesian Optimization in Theory and Practice, 2013.
175. A. Carpentier, A. Lazaric, M. Ghavamzadeh, R. Munos, P. Auer, A. Antos, Upper-Confidence-Bound Algorithms for Active Learning in Multi-Armed Bandits, (2015).
176. A. Garivier, E. Moulines, On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems, (2008). 34
177. J. Thompson, W.P. Walters, J.A. Feng, N.A. Pabon, H. Xu, B.B. Goldman, D. Moustakas, M. Schmidt, F. York, Optimizing active learning for free energy calculations, Artificial Intelligence in the Life Sciences. 2 (2022) 100050. https://doi.org/10.1016/j.ailsci.2022.100050.
178. L. Fortuna, S. Graziani, A. Rizzo, M.G. Xibilia, Soft sensors for monitoring and control of industrial processes, Springer, 2007.
179. X. Shi, W. Xiong, Approximate linear dependence criteria with active learning for smart soft sensor design, Chemometrics and Intelligent Laboratory Systems. 180 (2018) 88–95. https://doi.org/10.1016/j.chemolab.2018.07.009.
180. L.L.T. Chan, Q.Y. Wu, J. Chen, Dynamic soft sensors with active forward-update learning for selection of useful data from historical big database, Chemometrics and Intelligent Laboratory Systems. 175 (2018) 87–103. https://doi.org/10.1016/j.chemolab.2018.01.015.
181. Q. Tang, D. Li, Y. Xi, A new active learning strategy for soft sensor modeling based on feature reconstruction and uncertainty evaluation, Chemometrics and Intelligent Laboratory Systems. 172 (2018) 43–51. https://doi.org/10.1016/j.chemolab.2017.11.001.
182. P. Auer, N. Cesa-Bianchi, Y. Freund, R.E. Schapire, The Nonstochastic Multiarmed Bandit Problem, SIAM Journal on Computing. 32 (2002) 48–77. https://doi.org/10.1137/S0097539701398375.
183. M. Soare, A. Lazaric, R. Munos, Best-Arm Identification in Linear Bandits, in: 27th Conference on Neural Information Processing Systems (NeurIPS 2014), 2014.
184. Y. Jedra, A. Proutiere, Optimal Best-arm Identification in Linear Bandits, in: 34th Conference on Neural Information Processing Systems (NeurIPS 2020), 2020.
185. M.J. Azizi, B. Kveton, M. Ghavamzadeh, Fixed-Budget Best-Arm Identification in Structured Bandits, in: Proceedings of the ThirtyFirst International Joint Conference on Artificial Intelligence (IJCAI-22), 2022.
186. S. Filippi, O. Cappe, A. Garivier, C. Szepesvári, Parametric bandits: The generalized linear case, in: Advances in Neural Information Processing Systems 23 (NIPS 2010), 2010.
187. K. Yu, J. Bi, V. Tresp, Active Learning via Transductive Experimental Design, in: Proceedings of the 23rd International Conference on Machine Learning, 2006.
188. T. Fiez, L. Jain, K. Jamieson, L. Ratliff, Sequential Experimental Design for Transductive Linear Bandits, in: 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), 2019.
189. R. Camilleri, Z. Xiong, M. Fazel, L. Jain, K. Jamieson, Selective Sampling for Online Best-arm Identification, in: 35th Conference on Neural Information Processing Systems (NeurIPS 2021)., 2021. http://arxiv.org/abs/2110.14864.
190. P. Menard, O.D. Domingues, A. Jonsson, E. Kaufmann, E. Leurent, M. Valko, Fast active learning for pure exploration in reinforcement learning, in: Proceedings of the 38th International Conference on Machine Learning, 2021.
191. M. Fang, Y. Li, T. Cohn, Learning how to Active Learn: A Deep Reinforcement Learning Approach, (2017).
192. O. Rudovic, M. Zhang, B. Schuller, R. Picard, Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach, in: 2019 International Conference on Multimodal Interaction, ACM, New York, NY, USA, 2019: pp. 6–15. https://doi.org/10.1145/3340555.3353742.
193. S. Wassermann, T. Cuvelier, P. Casas, RAL-Improving Stream-Based Active Learning by Reinforcement Learning, n.d. https://hal.archives-ouvertes.fr/hal-02265426.
194. M. Woodward, C. Finn, Active One-shot Learning, (2017). http://arxiv.org/abs/1702.06559.
195. O. Reyes, A.H. Altalhi, S. Ventura, Statistical comparisons of active learning strategies over multiple datasets, Knowl Based Syst. 145 (2018) 274–288. https://doi.org/10.1016/j.knosys.2018.01.033.
196. R.G.D. Steel, A Multiple Comparison Sign Test: Treatments Versus Control, J Am Stat Assoc. 54 (1959) 767. https://doi.org/10.2307/2282500.
197. F. Wilcoxon, Individual Comparisons by Ranking Methods, Biometrics Bulletin. 1 (1945) 80. https://doi.org/10.2307/3001968.
198. M. Friedman, A Comparison of Alternative Tests of Significance for the Problem of $m$ Rankings, The Annals of Mathematical Statistics. 11 (1940) 86–92. https://doi.org/10.1214/aoms/1177731944.
199. J.L. Hodges, E.L. Lehmann, Rank Methods for Combination of Independent Experiments in Analysis of Variance, The Annals of Mathematical Statistics. (1962).
200. R.L. Iman, J.M. Davenport, Approximations of the critical region of the fbietkan statistic, Commun Stat Theory Methods. 9 (1980) 571–595. https://doi.org/10.1080/03610928008827904.
201. D. Quade, Using Weighted Rankings in the Analysis of Complete Blocks with Additive Block Effects, J Am Stat Assoc. 74 (1979) 680. https://doi.org/10.2307/2286991.
202. A.L. Suárez-Cetrulo, A. Kumar, L. Miralles-Pechuán, Modelling the COVID-19 virus evolution with Incremental Machine Learning, (2021).
203. V. Cerqueira, L. Torgo, I. Mozetič, Evaluating time series forecasting models: an empirical study on performance estimation methods, Mach Learn. 109 (2020) 1997–2028. https://doi.org/10.1007/s10994-020-05910-7.
204. E. Tieppo, R.R. dos Santos, J.P. Barddal, J.C. Nievola, Hierarchical classification of data streams: a systematic literature review, Artif Intell Rev. 55 (2022) 3243–3282. https://doi.org/10.1007/s10462-021-10087-z.
205. D. Cacciarelli, M. Boresta, What drives a donor? A machine learning‐based approach for predicting responses of nonprofit direct marketing campaigns, International Journal of Nonprofit and Voluntary Sector Marketing. (2021). https://doi.org/10.1002/nvsm.1724.
206. D. Brzezinski, J. Stefanowski, Prequential AUC for Classifier Evaluation and Drift Detection in Evolving Data Streams, in: 2015: pp. 87–101. https://doi.org/10.1007/978-3-319-17876-9_6.
207. D. Brzezinski, J. Stefanowski, Prequential AUC: properties of the area under the ROC curve for data streams with concept drift, Knowl Inf Syst. 52 (2017) 531–562. https://doi.org/10.1007/s10115-017-1022-8.
208. B. Krawczyk, Active and adaptive ensemble learning for online activity recognition from data streams, Knowl Based Syst. 138 (2017) 69–78. https://doi.org/10.1016/j.knosys.2017.09.032.
209. S. Mohamadi, H. Amindavar, Deep Bayesian Active Learning, A Brief Survey on Recent Advances, (2020).
210. C. Riis, F. Antunes, F.B. Hüttel, C.L. Azevedo, F.C. Pereira, Bayesian Active Learning with Fully Bayesian Gaussian Processes, In Proceedings of Advances in Neural Information Processing Systems 35 (NeurIPS 2022). (2022).
