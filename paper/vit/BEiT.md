# BEiT: BERT Pre-Training of Image Transformers
2021.06.15 https://arxiv.org/abs/2106.08254

## Abstract 
We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains 86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%). The code and pretrained models are available at https://aka.ms/beit .

我们介绍了一种自监督视觉表示模型BEiT，它代表来自图像变换器的双向编码器表示。在自然语言处理领域开发的BERT之后，我们提出了一种掩码图像建模任务来预训练视觉变换器。具体来说，在我们的预训练中，每个图像都有两个视图，即图像块(例如16x16像素)和视觉tokens(即离散tokens)。我们首先将原始图像“tokens”为视觉tokens。然后，我们随机掩码一些图像块，并将它们输入主干变压器。预训练目标是基于损坏的图像块恢复原始视觉tokens。在预训练BEiT之后，我们通过在预训练的编码器上添加任务层来直接微调下游任务的模型参数。对图像分类和语义分割的实验结果表明，我们的模型与以前的预训练方法取得了竞争性的结果。例如，基本尺寸的BEiT在ImageNet-1K上实现了83.2%的顶级精度，在相同设置下显著优于从头开始的DeiT训练(81.8%)。此外，大型BEiT仅使用ImageNet-1K获得86.3%的数据，甚至超过了在ImageNet-22K上进行监督预训练的ViT-L(85.2%)。代码和预训练模型在 https://aka.ms/beit .