# A Comparative Survey of Deep Active Learning
深度主动学习的比较调查 2022.3.25 https://arxiv.org/abs/2203.13450

## Abstract
While deep learning (DL) is data-hungry and usually relies on extensive labeled data to deliver good performance, Active Learning (AL) reduces labeling costs by selecting a small proportion of samples from unlabeled data for labeling and training. Therefore, Deep Active Learning (DAL) has risen as a feasible solution for maximizing model performance under a limited labeling cost/budget in recent years. Although abundant methods of DAL have been developed and various literature reviews conducted, the performance evaluation of DAL methods under fair comparison settings is not yet available. Our work intends to fill this gap. In this work, We construct a DAL toolkit, DeepAL+, by re-implementing 19 highlycited DAL methods. We survey and categorize DAL-related works and construct comparative experiments across frequently used datasets and DAL algorithms. Additionally, we explore some factors (e.g., batch size, number of epochs in the training process) that influence the efficacy of DAL, which provides better references for researchers to design their DAL experiments or carry out DALrelated applications. 

虽然深度学习 (DL) 需要大量数据并且通常依赖于大量标注数据来提供良好的性能，但主动学习 (AL) 通过从未标注数据中选择一小部分样本进行标注和训练来降低标注成本。 因此，近年来，深度主动学习 (DAL) 已成为在有限的标注成本/预算下最大化模型性能的可行解决方案。 尽管已经开发出丰富的 DAL 方法并进行了各种文献综述，但尚无公平比较设置下 DAL 方法的性能评估。 我们的工作旨在填补这一空白。 在这项工作中，我们通过重新实现 19 种被高度引用的 DAL 方法构建了一个 DAL 工具包 DeepAL+。 我们对与 DAL 相关的工作进行调查和分类，并在常用数据集和 DAL 算法之间构建比较实验。 此外，我们探索了一些影响 DAL 有效性的因素(例如，批量大小、训练过程中的 epoch 数)，这为研究人员设计 DAL 实验或开展 DAL 相关应用提供了更好的参考。

## 1 Introduction
Blessed by the capacity of representation learning in an over-parameterized architecture, Deep Neural Networks (DNNs) have been used as significant workhorses in various machine learning (ML) tasks. While DNNs can work with extensive training datasets and deliver decent performance, collecting and annotating data to feed DNNs training becomes extremely expensive and time-consuming. On the other hand, given a large pool of unlabeled data, AL improves learning efficiency by selecting small subsets of samples for annotating and training [76]. In this way, a sweet spot appears at the intersection of DNNs and AL, where representation learning can be achieved with reduced labeling costs. Deep Active Learning (DAL) has been employed in various tasks, e.g., named entity recognition [9, 62], semantic parsing [16], object detection [23, 53], image segmentation [6, 55], counting [81], etc. Besides these applications, multiple unified DAL frameworks have been designed and perform well on various tasks [2, 50, 57, 63].

得益于过参数化架构中表示学习的能力，深度神经网络 (DNN) 已被用作各种机器学习 (ML) 任务中的重要主力。 虽然 DNN 可以使用广泛的训练数据集并提供不错的性能，但收集和标注数据以提供 DNN 训练变得极其昂贵和耗时。 另一方面，给定大量未标注数据，AL 通过选择小样本子集进行标注和训练来提高学习效率 [76]。 这样，一个最佳点出现在 DNN 和 AL 的交叉点，在那里可以通过降低标注成本来实现表示学习。 深度主动学习 (DAL) 已被用于各种任务，例如命名实体识别 [9, 62]、语义解析 [16]、目标检测 [23, 53]、图像分割 [6, 55]、计数 [81] 等。除了这些应用程序之外，还设计了多个统一的 DAL 框架，并且在各种任务上表现良好 [2、50、57、63]。

DAL originated from AL for classical ML tasks, which has been well studied in past years. The application of AL to classical ML tasks appear in a wealth of literature surveys [1, 17, 19, 39, 59, 71, 74] and comparative studies [7, 36, 44, 49, 51, 56, 60, 65, 68, 80]. Some traditional AL methods for classical ML have been generalized to DL tasks [3, 20, 69]. Adapting AL methods to work well on classical ML tasks has several issues to overcome [52]: 1) different from traditional AL methods that use fixed pre-processed features to calculate uncertainty/representativeness, in DL tasks, feature representations are jointly learned with DNNs. Therefore, feature representations are dynamically changing during DAL processes, and thus pairwise distances/similarities used by representativenessbased measures need to be re-computed in every stage, whereas for AL with classical ML tasks, these pairwise terms can be pre-computed. 2) DNNs are typically over-confident with their predictions and thus evaluating the uncertainty of unlabeled data might be unreliable. Ren et al. [52] conducted a comprehensive review of DAL, which systematically summarizes and categorizes 189 existing works. Indeed it is a comprehensive study of DAL and guides new and experienced researchers who want to use it. However, due to the lack of experimental comparisons among various branches of DAL algorithms across different datasets/tasks, it is difficult for researchers to distinguish which DAL algorithms are suitable for which task. Our work aims to fill this gap.

DAL 起源于经典 ML 任务的 AL，在过去几年得到了很好的研究。 大量文献调查 [1、17、19、39、59、71、74] 和比较研究 [7、36、44、49、51、56、60、65、 68、80]。 一些用于经典 ML 的传统 AL 方法已被推广到 DL 任务 [3、20、69]。 适应 AL 方法以在经典 ML 任务上很好地工作有几个问题需要克服 [52]：
1. 与使用固定预处理特征来计算不确定性/代表性的传统 AL 方法不同，在 DL 任务中，特征表示是与 DNN 共同学习的 . 因此，特征表示在 DAL 过程中动态变化，因此需要在每个阶段重新计算基于代表性的度量所使用的成对距离/相似性，而对于具有经典 ML 任务的 AL，可以预先计算这些成对项。 
2. DNN 通常对其预测过于自信，因此评估未标注数据的不确定性可能不可靠。 Ren et al. [52] 对 DAL 进行了全面审查，系统地总结和分类了 189 个现有的工作。 事实上，它是对 DAL 的全面研究，并为想要使用它的新手和有经验的研究人员提供指导。 然而，由于缺乏跨不同数据集/任务的 DAL 算法的各个分支之间的实验比较，研究人员很难区分哪些 DAL 算法适用于哪些任务。 我们的工作旨在填补这一空白。
<!--52,A survey of deep active learning -->

In this work, we construct a DAL toolkit, called DeepAL+, by re-implementing 19 DAL methods surveyed in this paper. DeepAL+ is sequel to our previous work DeepAL [27]. Compared to DeepAL, which includes 11 highly-cited DAL approaches prior to 2018, in DeepAL+, 1) we upgraded and optimized some algorithms that already were implemented in DeepAL; 2) we re-implemented more highly-cited DAL algorithms, most of which are proposed after 2018; 3) besides well-studied datasets adopted in DeepAL like MNIST [14], CIFAR [38] and SVHN [45], we integrated more complicated tasks in DeepAL+ like medical image analysis [31, 66] and object recognition with correlated backgrounds (containing spurious correlations) [54]. We conduct comparative experiments between a variety of DAL approaches based on DeepAL+ on multiple tasks and also explore factors of interest to researchers, such as the influence of batch size and the number of training epochs in each AL iteration, and timing-cost comparison. More descriptions of DeepAL+ are in Section B in Appendix.

在这项工作中，我们通过重新实施本文调查的 19 种 DAL 方法，构建了一个名为 DeepAL+ 的 DAL 工具包。 DeepAL+ 是我们之前工作 DeepAL [27] 的续集。 与包含 2018之前的 11 个高引用 DAL 方法的 DeepAL 相比，在 DeepAL+ 中，
1. 我们升级和优化了一些已经在 DeepAL 中实现的算法;  
2. 我们重新实现了更多高引用的DAL算法，其中大部分是2018年之后提出的;  
3. 除了在 DeepAL 中采用经过充分研究的数据集，如 MNIST[14]、CIFAR[38] 和 SVHN[45]，我们在 DeepAL+ 中集成了更复杂的任务，如医学图像分析[31、66] 和具有相关背景的目标检测(包含 虚假相关)[54]。 
我们在多个任务上对基于 DeepAL+ 的多种 DAL 方法进行了比较实验，还探索了研究人员感兴趣的因素，例如每次 AL 迭代中批量大小和训练周期数的影响，以及时间成本比较。 DeepAL+ 的更多描述在附录 B 部分。

We hope that our comparative study/benchmarking test brings authentic comparative evaluation for DAL, provides a quick look at which DAL models are more effective and what are the challenges and possible research directions in DAL, as well as offering guidelines for conducting fair comparative experiments for future DAL methods. More importantly, we expect that our DeepAL+ will contribute to the development of DAL since DeepAL+ is extensible, allowing easy incorporation of new basic tasks/datasets, new DAL algorithms, and new basic learned models. This makes the application of DAL to downstream tasks, and designing new DAL algorithms becomes easier. DeepAL+ is an ongoing process. We will keep expanding it by incorporating more basic tasks, models, and DAL algorithms. Our DeepAL+ is available on https://github.com/SineZHAN/deepALplus. 

我们希望我们的比较研究/基准测试能够为 DAL 带来真实的比较评估，让您快速了解哪些 DAL 模型更有效以及 DAL 中的挑战和可能的研究方向是什么，并为进行公平的比较实验提供指导方针 未来的 DAL 方法。 更重要的是，我们期望我们的 DeepAL+ 将有助于 DAL 的发展，因为 DeepAL+ 是可扩展的，允许轻松合并新的基本任务/数据集、新的 DAL 算法和新的基本学习模型。 这使得 DAL 应用于下游任务，设计新的 DAL 算法变得更加容易。 DeepAL+ 是一个持续的过程。 我们将通过合并更多的基本任务、模型和 DAL 算法来不断扩展它。 我们的 DeepAL+ 可在 https://github.com/SineZHAN/deepALplus 上获得。

## 2 DAL Approaches
This section provides an overview of highly-cited DAL methods in recent years, including the perspectives of querying strategies and techniques for enhancing DAL methods.

本节概述了近年来被高度引用的 DAL 方法，包括查询策略的观点和增强 DAL 方法的技术。

Problem Definition. We only discuss pool-based AL, since most DAL approaches belong to this category. Pool-based AL selects most informative data iteratively from a large pool of unlabeled i.i.d. data samples until either the basic learner(s) reaches a certain level of performance or a fixed budget is exhausted [11]. We consider a general process of DAL, taking classification tasks as example, where other tasks (e.g., image segmentation) follow the common definition of their tasks domain. We have an initial labeled set Dl = {(xj , yj )}Mj=1 and a large unlabeled data pool Du = {xi, }Ni=1, where M  N, yi ∈ {0, 1} is the class label of xi for binary classification, or yi ∈ {1, ..., k} for multi-class classification. In each iteration, we select batch of samples Dq with batch size b from Du based on basic learned model M and an acquisition function α(x,M), and query their labels from the oracle. Data samples are selected by Dq∗ = arg maxbx∈Du α(x,M), where the superscript b indicates selection of the top b points. Dl and Du are then updated, and M is retrained on Dl . DAL terminates when the budget Q is exhausted or a desired model performance is reached.

问题定义。 我们只讨论基于池的 AL，因为大多数 DAL 方法都属于这一类。 基于池的 AL 从大量未标注的 i.i.d 中迭代地选择信息最丰富的数据。 数据样本，直到基本学习器达到一定的性能水平或固定预算用尽 [11]。 我们考虑 DAL 的一般过程，以分类任务为例，其中其他任务(例如图像分割)遵循其任务域的通用定义。 我们有一个初始标注集 Dl = {(xj , yj )}Mj=1 和一个大的未标注数据池 Du = {xi, }Ni=1，其中 M N, yi ∈ {0, 1} 是类标签 用于二进制分类的 xi 或用于多类分类的 yi ∈ {1, ..., k}。 在每次迭代中，我们基于基本学习模型 M 和采集函数 α(x,M) 从 Du 中选择批量大小为 b 的样本批次 Dq，并从 oracle 中查询它们的标签。 数据样本通过Dq∗ = arg maxbx∈Du α(x,M)选择，其中上标b表示选择前b个点。 然后更新 Dl 和 Du，并在 Dl 上重新训练 M。 当预算 Q 耗尽或达到所需的模型性能时，DAL 终止。

#### 2.1 Querying Strategies
DAL can be categorized into 3 branches from the perspective of querying strategy: uncertainty-based, representativeness/diversity-based and combined strategies, as shown in Figure 1. 

DAL从查询策略的角度可以分为3个分支：基于不确定性、基于代表性/多样性和组合策略，如图1所示。

Figure 1: Categorization of DAL sampling/querying strategies.
图 1：DAL 采样/查询策略的分类。

#### 2.1.1 Uncertainty-based Querying Strategies 基于不确定性的查询策略
Uncertainty-based DAL selects data samples with high aleatoric uncertainty or epistemic uncertainty, where aleatoric uncertainty refers to the natural uncertainty in data due to influences on data generation processes that are inherently random. Epistemic uncertainty comes from the modeling/learning process and is caused by a lack of knowledge [30, 46, 58, 59]. Many uncertainty-based DAL measures are adapted from pool-based AL techniques for classical ML tasks. Typical methods include:
1. Maximum Entropy (Entropy) [61] selects data x that maximize the predictive entropy HM[y|x]: αentropy(x,M) = HM[y|x] = −P k pM(y = k|x) log pM(y = k|x), where pM(y|x) is the posterior label probability from the classifier M.
2. Margin [45] selects data x whose two most likely labels (ˆy1, yˆ2) have smallest difference in posterior probabilities: αmargin(x,M) = −[pM(ˆy1|x) − pM(ˆy2|x)].
3. Least Confidence (LeastConf) [69] selects data x whose most likely label yˆ has lowest posterior probability: αLeastConf(x,M) = −pM(ˆy|x). A similar method is Variation Ratios (VarRatio) [18], which measures the lack of confidence like LeastConf: αVarRatio(x,M) = 1 − pM(ˆy|x).
4. Bayesian Active Learning by Disagreements (BALD) [20, 26] chooses data points that are expected to maximize the information gained from the model parameters ω, i.e. the mutual information between predictions and model posterior: αBALD(x,M) = HM[y|x] − Ep(ω|Dl)[HM[y|x, ω]].
5. Mean Standard Deviation (MeanSTD) [29] maximizes the mean standard deviation of the predicted probabilities over all k classes: αMeanSTD(x,M) = 1k P k p Varq(ω)[p(y = k|x, ω)].

基于不确定性的 DAL 选择具有高任意不确定性或认知不确定性的数据样本，其中任意不确定性是指由于对本质上随机的数据生成过程的影响而导致的数据自然不确定性。 认知不确定性来自建模/学习过程，是由缺乏知识引起的 [30、46、58、59]。 许多基于不确定性的 DAL 度量是从用于经典 ML 任务的基于池的 AL 技术改编而来的。 典型的方法包括：
1. 最大熵(Entropy)[61]选择最大化预测熵HM[y|x]的数据x：αentropy(x,M) = HM[y|x] = −P k pM(y = k|x) log pM(y = k|x)，其中 pM(y|x) 是来自分类器 M 的后验标签概率。
2. Margin [45] 选择数据 x，其两个最可能的标签 (^y1, y^2) 的后验概率差异最小：αmargin(x,M) = −[pM(^y1|x) − pM(^y2|x)]。
3. 最低置信度(LeastConf)[69] 选择最有可能的标签 y^ 具有最低后验概率的数据 x：αLeastConf(x,M) = −pM(^y|x)。 一种类似的方法是变异比率 (VarRatio) [18]，它像 LeastConf 一样衡量缺乏信心：αVarRatio(x,M) = 1 − pM(^y|x)。
4. 基于分歧的贝叶斯主动学习 (BALD) [20, 26] 选择预期最大化从模型参数 ω 获得的信息的数据点，即预测和模型后验之间的互信息：αBALD(x,M) = HM [y|x] − Ep(ω|Dl)[HM[y|x, ω]]。
5. 平均标准偏差 (MeanSTD) [29] 最大化所有 k 个类别的预测概率的平均标准偏差：αMeanSTD(x,M) = 1k P k p Varq(ω)[p(y = k|x, ω) ].

Inspired by recent advances in generating adversarial examples, some DAL methods utilize adversarial attacks to rank the uncertainty/informativeness of each unlabeled data sample. The DeepFool Active Learning method (AdvDeepFool) [15] queries the unlabeled samples that are closest to their adversarial attacks (DeepFool). Specifically, αAdvDeepFool(x,M) = rx, where rx is the minimal perturbation that causes the changing of labels, e.g., for binary classification, rx = argminr, M(x)6=M(x+r) − M(x+r) ||∇M(x+r)||22 ∇M(x+r). DeepFool attack can be replaced by other attack methods, e.g., Basic Interactive Method (BIM) [40], called AdvBIM.

受生成对抗性样本的最新进展的启发，一些 DAL 方法利用对抗性攻击对每个未标注数据样本的不确定性/信息量进行排名。 DeepFool 主动学习方法 (AdvDeepFool) [15] 查询最接近其对抗性攻击 (DeepFool) 的未标注样本。 具体来说，αAdvDeepFool(x,M) = rx，其中 rx 是导致标签变化的最小扰动，例如，对于二进制分类，rx = argminr，M(x)6=M(x+r) − M(x +r) ||∇M(x+r)||22∇M(x+r)。 DeepFool 攻击可以用其他攻击方法代替，例如基本交互方法 (BIM) [40]，称为 AdvBIM。

Generative Adversarial Active Learning (GAAL) [83] synthesizes queries via Generative Adversarial Networks (GANs). In contrast to regular AL that selects points from the unlabeled data pool, GAAL generates images from GAN for querying human annotators. However, the generated data very close to the classifier decision boundary may be meaningless, and even human annotators could not distinguish its category. An improved approach called Bayesian Generative Active Deep Learning (BGADL) [67] combines active learning with data augmentation. BGADL utilizes typical Bayesian DAL approaches for its acquisition function (e.g., αBALD) and then trains a VAE-ACGAN to generate synthetic data samples to enlarge the training set. Other practical uncertainty-based measures include i) utilizing gradient: Wang et al. [72] found that gradient norm can effectively guide unlabeled data selection; that is, selecting unlabeled data of higher gradient norm can reduce the upper bound of the test loss. Another work that utilizes gradient is Batch Active learning by Diverse Gradient Embeddings (BADGE)[2] measures uncertainty as the gradient magnitude with respect to parameters in the output layer since DNNs are optimized using gradient-based methods like SGD. ii) Loss Prediction Loss (LPL) [78] uses a loss prediction strategy by attaching a small parametric module that is trained to predict the loss of unlabeled inputs with respect to the target model, by minimizing the loss prediction loss between predicted loss and target loss. LPL picks the top b data samples with the highest predicted loss.

生成对抗主动学习 (GAAL) [83] 通过生成对抗网络 (GAN) 合成查询。 与从未标注的数据池中选择点的常规 AL 不同，GAAL 从 GAN 生成图像以查询人类标注者。 然而，非常接近分类器决策边界的生成数据可能毫无意义，甚至人工标注者也无法区分其类别。 一种称为贝叶斯生成主动深度学习 (BGADL) [67] 的改进方法将主动学习与数据增广相结合。 BGADL 利用典型的贝叶斯 DAL 方法实现其采集功能(例如，αBALD)，然后训练 VAE-ACGAN 生成合成数据样本以扩大训练集。 其他基于不确定性的实用措施包括 
1. 利用梯度：Wang et al. [72] 发现梯度范数可以有效地指导未标注数据的选择;  即,选择更高梯度范数的未标注数据可以降低测试损失的上限。 另一项利用梯度的工作是通过多样化梯度嵌入 (BADGE) [2] 进行批量主动学习，测量不确定性作为输出层中参数的梯度幅度，因为 DNN 使用基于梯度的方法(如 SGD)进行了优化。 
2. Loss Prediction Loss (LPL) [78] 通过附加一个小的参数模块来使用损失预测策略，该模块被训练来预测未标注输入相对于目标模型的损失，通过最小化预测损失和目标之间的损失预测损失 损失。 LPL 选择预测损失最高的前 b 个数据样本。

#### 2.1.2 Representative/Diversity-based Querying Strategies 基于代表性/多样性的查询策略
Representative/diversity-based strategies select batches of samples representative of the unlabeled set and are based on the intuition that the selected representative examples, once labeled, can act as a surrogate for the entire dataset [2]. Clustering methods are widely used in representative-based strategies. A typical method is KMeans, which selects centroids by iteratively sampling points in proportion to their squared distances from the nearest previously selected centroid. Another widely adopted approach [21, 57] selects a batch of representative points based on a core set, which is a subsample of a dataset that can be used as a proxy for the full set. CoreSet is measured in the penultimate layer space h(x) of the current model. Firstly, given Dl , an example xu is selected with the greatest distance to its nearest neighbor in the hidden space u = arg maxxi∈Du minxj∈Dl ∆(h(xi, xj )). Sampling is then repeated until batch size b is reached. In another method, Cluster-Margin [12] selects a diverse set of examples on which the model is least confident. It first runs hierarchical agglomerate clustering with average-linkage as pre-processing, and then selects an unlabeled subset with lowest margin scores (Margin), which is then filtered down to a diverse set with b samples. In contrast to CoreSet, Cluster-Margin only runs clustering once as pre-processing.

代表性/基于多样性的策略 选择代表未标注集的样本批次，并且基于这样的直觉，即所选代表性样本一旦被标注，就可以充当整个数据集的智能体 [2]。 聚类方法广泛用于基于代表的策略。 一种典型的方法是 KMeans，它通过迭代采样点来选择质心，这些点与它们距离最近的先前选择的质心的平方距离成比例。 另一种广泛采用的方法 [21、57] 基于核心集选择一批代表点，核心集是数据集的子样本，可用作完整集的智能体。 CoreSet 是在当前模型的倒数第二层空间 h(x) 中测量的。 首先，给定 Dl ，在隐藏空间 u = arg maxxi∈Du minxj∈Dl ∆(h(xi, xj )) 中选择与其最近邻居距离最大的样本 xu。 然后重复采样，直到达到批量大小 b。 在另一种方法中，Cluster-Margin [12] 选择模型最不自信的一组不同的例子。 它首先运行具有平均链接的层次凝聚聚类作为预处理，然后选择具有最低边缘分数(边缘)的未标注子集，然后将其过滤为具有 b 个样本的多样化集合。 与 CoreSet 相比，Cluster-Margin 只运行一次聚类作为预处理。

Point Processes are also adopted in representative-based DAL, e.g., Active-DPP [4]. A determinantal point process (DPP) captures diversity by constructing a pair-wise (dis)similarity matrix and calculating its determinant. BADGE also utilizes DPPs as a representative measure. Discriminative AL (DiscAL) [22] is a representative measure that, reminiscent of GANs, attempts to fool a discriminator that tries to distinguish between data coming from two different distributions (unlabeled/labeled). Variational Adversarial AL (VAAL) [64] learns a distribution of labeled data in latent space using a VAE and an adversarial network trained to discriminate between unlabeled and labeled data. The network is optimized using both reconstruction and adversarial losses. αVAAL is formed with the discriminator that estimates the probability that the data comes from the unlabeled data. Wasserstein Adversarial AL (WAAL) [63] searches the diverse unlabeled batch that also has larger diversity than the labeled samples through adversarial training by H-divergence.

基于代表的 DAL 也采用点过程，例如 Active-DPP [4]。 行列式点过程 (DPP) 通过构建成对(非)相似性矩阵并计算其行列式来捕获多样性。 BADGE 还利用 DPP 作为代表性措施。 判别 AL (DiscAL) [22] 是一种代表性措施，让人联想到 GAN，它试图愚弄试图区分来自两种不同分布(未标注/标注)的数据的鉴别器。 变分对抗性 AL (VAAL) [64] 使用 VAE 和经过训练以区分未标注和标注数据的对抗网络来学习标注数据在潜在空间中的分布。 该网络使用重建和对抗性损失进行了优化。 αVAAL 由估计数据来自未标注数据的概率的鉴别器组成。 Wasserstein Adversarial AL (WAAL) [63] 通过 H-divergence 的对抗训练搜索多样化的未标注批次，这些批次也比标注样本具有更大的多样性。

#### 2.1.3 Combined Querying Strategies 组合查询策略
Due to the demand for larger batch size (representative/diversity) and more precise decision boundaries for higher model performance (uncertainty) in DAL, combined strategies have become the dominant approaches to DAL. It aims to achieve a trade-off between uncertainty and representativeness/diversity in query selection. We mainly discuss the optimization methods with respect to multiple objectives (uncertainty, diversity, etc.) in this paper, including weighted-sum and two-stage optimization.

由于 DAL 中对更大批量大小(代表性/多样性)和更精确的决策边界以提高模型性能(不确定性)的需求，组合策略已成为 DAL 的主要方法。 它旨在在查询选择中实现不确定性和代表性/多样性之间的权衡。 本文主要讨论针对多目标(不确定性、多样性等)的优化方法，包括加权求和和两阶段优化。

Weighted-sum optimization is both simple and flexible, where the objective functions are summed up with weight β: αweighted-sum = αuncertainty + βαrepresentative. However, two factors limit its usage in Combined DAL: 1) it introduces extra hyper-parameter β for tuning; 2) unlike uncertainty-based measure that provide a single score per sample, representativeness is usually expressed in matrix form, which is not straightforward to convert into a single per-sample score. A example of weighted-sum optimization is Exploitation-Exploration [77] selects samples that are most uncertain and least redundant (exploitation), as well as most diverse (exploration). Specifically, in the exploitation step, αexploitation = αentropy(Dq,M) − |D βq|αsimilarity(Dq). Using DPPs is a natural way to balance uncertainty score and pairwise diversity well without introducing additional hyper-parameters [2, 4 4, 79]. However, sampling from DPPs in DAL is not trivial since DPPs have a time complexity of O(N3).

加权和优化既简单又灵活，其中目标函数用权重 β 求和：αweighted-sum = αuncertainty + βαrepresentative。 然而，有两个因素限制了它在 Combined DAL 中的使用：
1. 它引入了额外的超参数 β 用于调整;  
2. 与基于不确定性的度量为每个样本提供单个分数不同，代表性通常以矩阵形式表示，这不容易转换为每个样本的单个分数。 
加权和优化的一个例子是开发-探索[77]选择最不确定和最少冗余(开发)以及最多样化(探索)的样本。 具体来说，在开发步骤中，αexploitation = αentropy(Dq,M) − |D βq|αsimilarity(Dq)。 使用 DPP 是在不引入额外超参数的情况下很好地平衡不确定性分数和成对多样性的自然方式 [2, 4, 4, 79]。 然而，由于 DPP 的时间复杂度为 O(N3)，因此从 DAL 中的 DPP 进行采样并非微不足道。

Two-stage (multi-stage) optimization is a popular combined strategy, Each stage refines the previous stage’s selections using different criteria. E.g., stage 1 selects an informative subset with a size larger than b, and then stage 2 selects b samples with maximum diversity. WAAL uses two stage optimization for implementing discriminative learning via training a DNN for discriminative features in stage 1, and making batch selections in stage 2 [63]. BADGE computes gradient embeddings for each unlabeled data samples in stage 1, then clusters by KMeans++ in stage 2 [2]. Diverse mini-Batch Active Learning (DBAL) [82] first pre-filters unlabeled data pool to the top ρb most informative/uncertain examples (ρ is pre-filter factor), then clusters these samples to b clusters with (weighted) KMeans and selects b samples closest to the cluster centers.

两阶段(多阶段)优化是一种流行的组合策略，每个阶段使用不同的标准细化前一阶段的选择。 例如，第 1 阶段选择一个大小大于 b 的信息子集，然后第 2 阶段选择 b 个具有最大多样性的样本。 WAAL 使用两阶段优化来实施判别学习，方法是在第 1 阶段训练 DNN 以获得判别特征，并在第 2 阶段进行批量选择 [63]。 BADGE 在第 1 阶段为每个未标注的数据样本计算梯度嵌入，然后在第 2 阶段通过 KMeans++ 聚类 [2]。 Diverse mini-Batch Active Learning (DBAL) [82] 首先将未标注的数据池预过滤到顶部 ρb 个信息量最大/不确定的样本(ρ 是预过滤因子)，然后将这些样本聚类到具有(加权)KMeans 和 选择最接近聚类中心的 b 个样本。

### 2.2 Enhancing of DAL Methods
In Section 2.1, many highly-cited DAL methods have designed acquisition functions, e.g., Entropy, CoreSet and BADGE. These methods are easily adapted to various tasks since they only involve the data selection process, not the training process of the backbone. However, how well these DAL methods can perform is limited, e.g., one might not exceed the performance of training on full data. Some DAL models are proposed for enhancing DAL methods that can break the limitation, which can be categorized into two branches: data and model aspect. The data aspect includes data augmentation and pseudo labeling, while the model aspect includes attaching extra networks, modifying loss functions, and ensemble. Due to limited space, we exclude related joint tasks that modify DAL methods like semi-/ self-/un-/supervised, transfer, or reinforcement learning.

在 2.1 节中，许多高引用的 DAL 方法都设计了采集函数，例如 Entropy、CoreSet 和 BADGE。 这些方法很容易适应各种任务，因为它们只涉及数据选择过程，而不涉及骨干的训练过程。 然而，这些 DAL 方法的性能是有限的，例如，可能不会超过对完整数据进行训练的性能。 提出了一些 DAL 模型来增强可以打破限制的 DAL 方法，这些模型可以分为两个分支：数据和模型方面。 数据方面包括数据增广和伪标签，而模型方面包括附加额外网络、修改损失函数和集成。 由于空间有限，我们排除了修改 DAL 方法的相关联合任务，如半/自/非/监督、迁移或强化学习。

Data aspect. Pseudo-labeling utilizes large-scale unlabeled data for training. Cost-Effective AL (CEAL) [70] assigns high-confident (low entropy HM[y|x]) pseudo labels predicted by M for training in the next iteration. However, this introduces new hyperparameters to threshold the prediction confidence, which, if badly tuned, can corrupt the training set with wrong labels [15]. Data augmentation uses labeled samples for enlarging the training set. However, data augmentation might waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative. AdvDeepFool adds adversarial samples to the training set [15], while BGADL employs ACGAN and Bayesian Data Augmentation for producing new artificial samples that are as informative as the selected samples [67].

数据方面。 伪标注利用大规模未标注数据进行训练。 Cost-Effective AL (CEAL) [70] 分配由 M 预测的高置信度(低熵 HM[y|x])伪标签用于下一次迭代的训练。 然而，这引入了新的超参数来阈值预测置信度，如果调整不当，可能会破坏带有错误标签的训练集 [15]。 数据增广使用标注样本来扩大训练集。 但是，数据增广可能会浪费计算资源，因为它会不加选择地生成不能保证提供信息的样本。 AdvDeepFool 将对抗样本添加到训练集 [15]，而 BGADL 使用 ACGAN 和贝叶斯数据增广来生成与所选样本一样信息丰富的新人工样本 [67]。

Model aspect. Some researchers utilize extra modules to improve target model performance and make selections in DAL. For instance, LPL jointly learns the target backbone model and loss prediction model, which can predict when the target model is likely to produce a wrong prediction. Choi et al. [10] constructs mixture density networks to estimate a probability distribution for each localization and classification head’s output for the object detection task. Revising the loss function of the target model is also promising. WAAL adopts min-max loss by leveraging the unlabeled data for better distinguishing labeled and unlabeled samples [63]. Another approach is ensemble learning. DNNs use a softmax layer to obtain the label’s posterior probability and tend to be overconfident when calculating the uncertainty. To increase uncertainty, Gal et al. [20] leverages Monte-Carlo (MC) Dropout, where uncertainty in the weights ω induces prediction uncertainty by marginalizing over the approximate posterior using MC integration. It can be viewed as an ensemble of models sampled with dropouts. Beluch et al. [3] found that ensembles of multiple classifiers perform better than MC Dropout for calculating uncertainty scores. 

模型方面。 一些研究人员利用额外的模块来提高目标模型的性能并在 DAL 中进行选择。 例如，LPL 联合学习目标主干模型和损失预测模型，可以预测目标模型何时可能产生错误预测。 崔et al. [10] 构建混合密度网络来估计每个定位和分类头的目标检测任务输出的概率分布。 修改目标模型的损失函数也很有希望。 WAAL 通过利用未标注数据采用最小-最大损失来更好地区分标注和未标注样本 [63]。 另一种方法是集成学习。 DNN 使用 softmax 层来获取标签的后验概率，并且在计算不确定性时往往过于自信。 为了增加不确定性，Gal et al. [20] 利用 Monte-Carlo (MC) Dropout，其中权重 ω 的不确定性通过使用 MC 积分边缘化近似后验来引起预测不确定性。 它可以被看作是用 dropouts 采样的模型的集合。 贝鲁奇et al. [3] 发现多个分类器的集成在计算不确定性分数方面比 MC Dropout 表现更好。

## 3 Comparative Experiments of DAL
We conduct comparisons on 19 methods across 10 public available datasets, in which these datasets are selected with reference of [52] (see Table 2 in [52]) and highly-cited DAL papers.

我们对 10 个公共可用数据集的 19 种方法进行了比较，其中这些数据集是参考 [52](参见 [52] 中的表 2)和高引用 DAL 论文选择的。

### 3.1 Experimental Settings
Datasets. Considering some DAL approaches currently only support computer vision tasks like VAAL, for consistency and fairness of our experiments, we adopt the image classification tasks, similar to most DAL papers. We use: MNIST [14], FashionMNIST [75], EMNIST [13], SVHN [45], CIFAR10, CIFAR100 [38] and TinyImageNet [41]. We construct an imbalanced dataset based on CIFAR10, called CIFAR10-imb, which sub-samples the training set with ratios of 1:2:· · · :10 for classes 0 through 9. We also consider medical imaging analysis tasks, including Breast Cancer Histopathological Image Classification (BreakHis) [66] and Chest X-Ray Pneuomonia classification (Pneumonia-MNIST) [31]. Additionally, we adopted an object recognition dataset with correlated backgrounds (Waterbird) [35, 54]. This dataset contains waterbird and landbird classes, which are manually mixed to water and land backgrounds. It is challenging since DNNs might spuriously rely on the background instead of learning to recognize the object semantics.

数据集。 考虑到一些 DAL 方法目前仅支持 VAAL 等计算机视觉任务，为了我们实验的一致性和公平性，我们采用图像分类任务，类似于大多数 DAL 论文。 我们使用：MNIST [14]、FashionMNIST [75]、EMNIST [13]、SVHN [45]、CIFAR10、CIFAR100 [38] 和 TinyImageNet [41]。 我们构建了一个基于 CIFAR10 的不平衡数据集，称为 CIFAR10-imb，它以 1:2:···:10 的比例对 0 到 9 类的训练集进行子采样。我们还考虑了医学影像分析任务，包括乳腺癌 组织病理学图像分类 (BreakHis) [66] 和胸部 X 射线肺炎分类 (Pneumonia-MNIST) [31]。 此外，我们采用了具有相关背景的目标检测数据集(水鸟)[35、54]。 该数据集包含水鸟和陆鸟类，它们是手动混合到水和陆地背景中的。 这具有挑战性，因为 DNN 可能虚假地依赖背景而不是学习识别对象语义。

DAL methods. We test Random Sampling (Random), Entropy, Margin, LeastConf and their MC Dropout versions [3] (denoted as EntropyD, MarginD, LeastConfD, respectively), BALD, MeanSTD, VarRatio, CEAL(Entropy), KMeans, the greedy version of CoreSet (denoted as KCenter), BADGE, AdversarialBIM, WAAL, VAAL, and LPL. For KMeans, considering that we need to cluster large amounts of data, the original KMeans implementation based on the scikitlearn library [48] will be too time-consuming on large-scale unlabeled data pools. Thus, to save the time cost and let our DeepAL+ be more adaptable to DL tasks, we implemented KMeans with GPU (KMeans (GPU)) based on the faiss library [28].

DAL 方法。 我们测试了Random Sampling(随机)、Entropy、Margin、LeastConf及其MC Dropout版本[3](分别表示为EntropyD、MarginD、LeastConfD)、BALD、MeanSTD、VarRatio、CEAL(Entropy)、KMeans、贪心版 CoreSet(表示为 KCenter)、BADGE、AdversarialBIM、WAAL、VAAL 和 LPL。 对于 KMeans，考虑到我们需要对大量数据进行聚类，基于 scikitlearn 库 [48] 的原始 KMeans 实现在大规模未标注数据池上将过于耗时。 因此，为了节省时间成本并让我们的 DeepAL+ 更适应 DL 任务，我们基于 faiss 库 [28] 实现了带 GPU 的 KMeans(KMeans(GPU))。

For all AL methods, we employed ResNet18 (w/o pre-training) [24] as the basic learner. For a fair comparison, consistent experimental settings of the basic classifier are used across all DAL methods. We run these experiments using DeepAL+ toolkit.

对于所有 AL 方法，我们采用 ResNet18(无预训练)[24] 作为基本学习器。 为了公平比较，基本分类器的一致实验设置用于所有 DAL 方法。 我们使用 DeepAL+ 工具包运行这些实验。

Experimental protocol. We repeat each experiment for 3 trials with random splits of the initial labeled and unlabeled pool (using the same random seed) and report average testing performance. For evaluation metrics, for brevity, we report overall performance using area under the budget curve (AUBC) [79, 80], where the performance-budget curve is generated by evaluating the DAL method for varying budgets (e.g., accuracy vs. budget). Higher AUBC values indicate better overall performance. We also report the final accuracy (F-acc), which is the accuracy after the budget Q is exhausted. More details of experimental settings (i.e., datasets, implementations) are in Section D in Appendix.

实验方案。 我们重复每个实验 3 次试验，随机拆分初始标注和未标注池(使用相同的随机种子)并报告平均测试性能。 对于评估指标，为简洁起见，我们使用预算曲线下面积 (AUBC) [79、80] 报告整体性能，其中性能预算曲线是通过评估不同预算的 DAL 方法生成的(例如，准确性与预算) . AUBC 值越高表示整体性能越好。 我们还报告了最终精度(F-acc)，这是预算 Q 耗尽后的精度。 有关实验设置(即数据集、实现)的更多详情，请参见附录 D 部分。

### 3.2 Analysis of Comparative Experiments
For analyzing the experiment results, we roughly divided our tasks into three groups: 1) standard image classification, including MNIST, FashionMNIST, EMNIST, SVHN, CIFAR10, CIFAR10-imb, CIFAR100 and TinyImageNet; 2) medical image analysis, including BreakHis and PneumoniaMNIST; 3) comparative studies, including MNIST and Waterbird, which would be introduced in Section 3.4. We report AUBC (acc) and F-acc performance in Table 1. We provided overall accuracy-budget curves and summarizing tables, as shown in Figure 5 and Tables 4 to 9 in Appendix.

为了分析实验结果，我们大致将任务分为三组：1)标准图像分类，包括 MNIST、FashionMNIST、EMNIST、SVHN、CIFAR10、CIFAR10-imb、CIFAR100 和 TinyImageNet;  2)医学图像分析，包括BreakHis和PneumoniaMNIST;  3)比较研究，包括MNIST和Waterbird，将在3.4节介绍。 我们在表 1 中报告了 AUBC (acc) 和 F-acc 性能。我们提供了总体准确度预算曲线和汇总表，如图 5 和附录中的表 4 至表 9 所示。

The typical uncertainty-based strategies on group 1, standard image classification tasks (in Table 1, from LeastConf to CEAL) are generally 1% ∼ 3% higher than Random on average performance across the whole AL process (AUBC). Among these uncertainty-based methods, we have conducted paired t-tests of each method with the other methods comparing AUBCs across group 1, standard image classification tasks, and no method performs significantly better than the others (all p-value are larger than 0.05). Considering dropout, there are only negligible effects (or even counter-intuitive) compared with the original versions (e.g., EntropyD vs. Entropy) on the normal image classification task, which is consistent with the observations in [3], except for TinyImageNet. On TinyImageNet, dropout versions are generally 1% ∼ 3% higher than the original versions. One possible explanation is that it is not accurate to use the feature representations provided by a single backbone model for calculating uncertainty score, while the dropout technique could help increase the uncertainty, and the differences among the uncertainty scores of unlabeled data samples will be increased. Therefore dropout versions provide better acquisition functions on TinyImageNet. Another comparison group is CEAL(Entropy) and Entropy, CEAL improved Entropy by an average of 0.5% on 8 datasets with threshold of confidence/entropy 1e-5. This idea seems effective, but the threshold must be carefully tuned to get better performance. On medical image analysis tasks (e.g., PneumoniaMNIST), performances are slightly different; VarRatio is even 4.5% lower than Random. Additionally, we observed the F-acc of many DAL algorithms are higher than full performance (e.g., F-acc is 0.9189 on KCenter and 0.9039 on full data), and the performances of dropout versions are worse than normal methods, e.g., 0.857 on Entropy and 0.8177 on EntropyD. These abnormal phenomena could be caused by the distribution shift between training/test sets and data redundancy in AL processes. The detailed explanations are in Section E.1 in Appendix.

第 1 组的典型基于不确定性的策略，标准图像分类任务(在表 1 中，从 LeastConf 到 CEAL)在整个 AL 过程(AUBC)的平均性能上通常比随机高 1% ∼ 3%。 在这些基于不确定性的方法中，我们对每种方法与其他方法进行了配对 t 检验，比较了第 1 组的 AUBC，标准图像分类任务，没有一种方法的性能明显优于其他方法(所有 p 值均大于 0.05 ). 考虑到 dropout，与原始版本(例如，EntropyD vs. Entropy)相比，对正常图像分类任务的影响可以忽略不计(甚至违反直觉)，这与 [3] 中的观察结果一致，TinyImageNet 除外。 在 TinyImageNet 上，dropout 版本通常比原始版本高 1% ∼ 3%。 一种可能的解释是，使用单一主干模型提供的特征表示来计算不确定性分数是不准确的，而 dropout 技术可能有助于增加不确定性，并且会增加未标注数据样本的不确定性分数之间的差异。 因此，dropout 版本在 TinyImageNet 上提供了更好的采集功能。 另一个比较组是 CEAL(Entropy) 和 Entropy，CEAL 在 8 个置信度/熵阈值为 1e-5 的数据集上将熵平均提高了 0.5%。 这个想法似乎有效，但必须仔细调整阈值以获得更好的性能。 在医学图像分析任务(例如 PneumoniaMNIST)上，表现略有不同;  VarRatio 甚至比 Random 低 4.5%。 此外，我们观察到许多 DAL 算法的 F-acc 高于完整性能(例如，F-acc 在 KCenter 上为 0.9189，在完整数据上为 0.9039)，而 dropout 版本的性能比普通方法差，例如，在 KCenter 上为 0.857 EntropyD 上的熵和 0.8177。 这些异常现象可能是由 AL 过程中训练/测试集之间的分布偏移和数据冗余引起的。 详细说明在附录的 E.1 节中。

Table 1: Overall results of DAL comparative experiments. We bold F-acc values that are higher than full performance. We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively. “∗” refers to the experiment needed too much memory, e.g., KCenter on EMNIST. “#” refers to the experiment that has not been completed yet. Completed tables of all tasks are shown in Tables 4, 5, 6, 7, and 8 in Appendix. 
表 1：DAL 比较实验的总体结果。 我们将高于全性能的 F-acc 值加粗。 我们将每个任务的 F-acc 和 AUBC 分别排在第一、第二和第三位，分别为红色、青色和蓝色。 “*”表示实验需要太多内存，例如 EMNIST 上的 KCenter。 “#”表示尚未完成的实验。 所有任务的完成表格显示在附录中的表 4、5、6、7 和 8 中。


Compared with uncertainty-based measures, the performances of representativeness/diversity-based methods (KMeans, KCenter and VAAL) do not show much advantage. Furthermore, they have relatively high time and memory costs since the pairwise distance matrix used by KMeans and KCenter need to be re-computed in each iteration with current feature representations, while VAAL requires re-training a VAE. Also, a high memory load is needed for storing the pairwise distance matrix for large unlabeled data pools like EMNIST).

与基于不确定性的措施相比，基于代表性/多样性的方法(KMeans、KCenter 和 VAAL)的性能没有显示出太多优势。 此外，它们具有相对较高的时间和内存成本，因为 KMeans 和 KCenter 使用的成对距离矩阵需要在每次迭代中使用当前特征表示重新计算，而 VAAL 需要重新训练 VAE。 此外，需要高内存负载来存储大型未标注数据池(如 EMNIST)的成对距离矩阵。

Compared with the performance of representativeness-based AL strategies on classical ML tasks [80], we believe that good representativeness-based DAL performance is based on good feature representations. Our analysis is consistent with the implicit analyses in [43]. Compared with the CPU-version KMeans, KMeans (GPU) is more time-efficient (see Section E.1 in Appendix) and performs better (see Table 1).

与基于代表性的 AL 策略在经典 ML 任务上的表现相比 [80]，我们认为良好的基于代表性的 DAL 性能是基于良好的特征表示。 我们的分析与 [43] 中的隐式分析一致。 与 CPU 版本的 KMeans 相比，KMeans (GPU) 的时间效率更高(见附录 E.1 节)并且性能更好(见表 1)。

Combined strategy BADGE shows its advantage on multiple datasets, where it consistently has relatively better performance. BADGE has 1% ∼ 3% higher AUBC performance compared with single KMeans and achieves comparable performance with uncertainty-based strategies and higher AUBC on CIFAR100 dataset. 

组合策略 BADGE 在多个数据集上显示出其优势，它始终具有相对更好的性能。 与单个 KMeans 相比，BADGE 的 AUBC 性能高出 1% ∼ 3%，并且在 CIFAR100 数据集上与基于不确定性的策略和更高的 AUBC 实现了相当的性能。

For enhanced DAL methods like LPL, WAAL, AdvBIM and CEAL, we are delighted to see their potential over typical DAL methods. For instance, LPL improves F-acc over full training on SVHN (0.9452 vs. 0.8793), CIFAR10 (0.9028 vs. 0.8793), CIFAR100 (0.6369 vs. 0.6062), and CIFAR10- imb (0.8478 vs. 0.8036). However, LPL is sensitive to hyper-parameters in the LossNet used to predict the target loss, e.g., the feature size determined by FC layer in LossNet. The LPL results on EMNIST and TinyImageNet indicate that it does not work on all datasets (we have tried many hyper-parameter settings on LossNet but did not work). A similar phenomenon occurs with WAAL. A potential explanation is that both EMNIST and TinyImageNet contain too many categories, which brings difficulty to the loss prediction in LPL and extracting diversified features in WAAL. This explanation is further verified in Section 3.4 – we adopt a pre-trained ResNet18 as the basic classifier, which obtains better feature representations for loss prediction, yielding better performance of LPL compared to the non-pre-trained version (0.9923 vs. 0.8913). The performance comparison on CIFAR100 also supported this explanation. AdvBIM, which adds adversarial samples for training, does not achieve ground-breaking performances like LPL. These adversarial samples are learned by the current backbone model, thus the improvement provided by AdvBIM is marginal. Moreover, AdvBIM is extremely time-consuming since it requires calculating adversarial distance r for every unlabeled data sample in every iteration. AdvBIM on EMNIST and TinyImageNet could not be completed due to the prohibitive computing requirements.

对于 LPL、WAAL、AdvBIM 和 CEAL 等增强型 DAL 方法，我们很高兴看到它们优于典型 DAL 方法的潜力。 例如，LPL 在 SVHN(0.9452 对比 0.8793)、CIFAR10(0.9028 对比 0.8793)、CIFAR100(0.6369 对比 0.6062)和 CIFAR10-imb(0.8478 对比 0.8036)上提高了 F-acc。 然而，LPL 对用于预测目标损失的 LossNet 中的超参数敏感，例如，LossNet 中由 FC 层确定的特征大小。 LPL 在 EMNIST 和 TinyImageNet 上的结果表明它并不适用于所有数据集(我们在 LossNet 上尝试了很多超参数设置但没有奏效)。 WAAL 也会出现类似的现象。 一个可能的解释是 EMNIST 和 TinyImageNet 都包含太多类别，这给 LPL 中的损失预测和 WAAL 中提取多样化特征带来困难。 这一解释在 3.4 节中得到进一步验证——我们采用预训练的 ResNet18 作为基本分类器，它获得了更好的损失预测特征表示，与非预训练版本相比，产生更好的 LPL 性能(0.9923 对 0.8913) . CIFAR100 上的性能比较也支持了这个解释。 添加对抗样本进行训练的 AdvBIM 并没有像 LPL 那样取得突破性的表现。 这些对抗性样本是由当前的主干模型学习的，因此 AdvBIM 提供的改进是微不足道的。 此外，AdvBIM 非常耗时，因为它需要在每次迭代中为每个未标注的数据样本计算对抗距离 r。 由于计算要求过高，无法在 EMNIST 和 TinyImageNet 上完成 AdvBIM。

### 3.3 Ablation Study – numbers of training epochs and batch size
Compared to classical ML tasks [80] that are typically convex optimization problems and have globally optimal solutions, DL typically involves non-convex optimization problems with local minima. Different hyper-parameters like learning rate, optimizer, number of training epochs, and AL batch size lead to other solutions with various performances. Here we conduct ablation studies on the effect of the number of training epochs in each AL iteration and batch size b. Figure 2 presents the results (see Figure 6 in the Appendix for more figures). Compared with Random, Entropy achieves better performance when the model is trained using more epochs, e.g., Entropy boosts AUBC by around 1.5 when the model is trained 30 epochs. We also see that using more training epochs results in better performance, e.g., AUBC gradually increases from around 0.66 to 0.80 for Random. It is worth noting that the improvement of AUBC brought by increasing the number of epochs has diminishing returns. Some researchers prefer to use a vast number of epochs during DAL training processes, e.g., Yoo and Kweon [78] used 200 epochs. However, others like [32] suggest that increasing the number of training epochs will not effectively improve testing performance due to generalization problems. Therefore, selecting an optimal number of training epochs is vital for reducing computation costs while maintaining good model performance. Interestingly, AL batch size has less impact on the performance, e.g., Entropy achieves similar performance using different AL batch sizes of 500, 1000, and 2000, which is important for DAL since we can use a relatively large batch size to reduce the number of training cycles.

与通常是凸优化问题并具有全局最优解的经典 ML 任务 [80] 相比，DL 通常涉及具有局部最小值的非凸优化问题。 学习率、优化器、训练时期数和 AL 批量大小等不同的超参数会导致其他具有不同性能的解决方案。 在这里，我们对每次 AL 迭代中训练时期数和批量大小 b 的影响进行消融研究。 图 2 显示了结果(有关更多数据，请参见附录中的图 6)。 与 Random 相比，Entropy 在使用更多 epoch 训练模型时获得更好的性能，例如，当模型训练 30 epoch 时，Entropy 将 AUBC 提高了 1.5 左右。 我们还看到使用更多的训练 epoch 会带来更好的性能，例如，对于随机，AUBC 从大约 0.66 逐渐增加到 0.80。 值得注意的是，增加epochs数量带来的AUBC的提升是收益递减的。 一些研究人员更喜欢在 DAL 训练过程中使用大量的 epoch，例如，Yoo 和 Kweon [78] 使用了 200 个 epoch。 然而，其他人如 [32] 表明，由于泛化问题，增加训练时期的数量不会有效地提高测试性能。 因此，选择最佳数量的训练时期对于降低计算成本同时保持良好的模型性能至关重要。 有趣的是，AL 批量大小对性能的影响较小，例如，熵使用 500、1000 和 2000 的不同 AL 批量大小实现了相似的性能，这对 DAL 很重要，因为我们可以使用相对较大的批量大小来减少 培训周期。

WAAL performs consistently better than Entropy and BADGE and the number of training epochs has less impact on its performance, e.g., training with 5 epochs, WAAL achieves AUBC 0.825 when training with 5 epochs and the AUBC remains consistent when increasing to 30 epochs. The possible reason is that WAAL considers diversity among samples and constructs a good representation through leveraging the unlabeled data information, thus reducing data bias. Moreover, we present the accuracy-budget curves using different batch sizes and training epochs in Figure A3. We also conclude that training epochs have less impact on WAAL, which is important for active learning approaches since fewer training epochs will save training time. In addition, WAAL outperforms its counterparts (i.e., Badge and Entropy).

WAAL 的表现始终优于 Entropy 和 BADGE，并且训练时期的数量对其性能的影响较小，例如，训练 5 个时期，WAAL 在训练 5 个时期时达到 AUBC 0.825，而当增加到 30 个时期时，AUBC 保持一致。 可能的原因是WAAL考虑了样本之间的多样性，通过利用未标注的数据信息构建了一个良好的表示，从而减少了数据偏差。 此外，我们在图 A3 中展示了使用不同批量大小和训练时期的准确性预算曲线。 我们还得出结论，训练时期对 WAAL 的影响较小，这对于主动学习方法很重要，因为较少的训练时期将节省训练时间。 此外，WAAL 优于其对应物(即 Badge 和 Entropy)。

### 3.4 How pre-training influence DAL performance?
Pre-training has become a central technique in research and applications of DNNs in recent years [25]. In our work, we selected Waterbird and MNIST datasets to conduct comparative experiments, with non pre-trained ResNet18 and pre-trained ResNet18 (pre-trained on ImageNet-1K data, ResNet18P for short). Waterbird and MNIST have completely different natures. Waterbird dataset contains spurious correlations, and non pre-trained model will focus more on backgrounds, e.g., the classifier will wrongly classify a landbird as a waterbird when the background is water. Pre-trained models provide better feature representations and allow better object semantics (see Figure 3). Waterbird is separated to four groups based on object and background: {(waterbird, water), (waterbird, land), (landbird, land), (landbird, land)}. Besides overall accuracy, we also report mismatch and worst group accuracy [42, 54], which refers to the accuracy of groups {(waterbird, land), (landbird, water)} and the lowest accuracy among four groups respectively. On MNIST, there is no valid background information. Therefore both models w/ and w/o pre-training would focus on semantic itself. The goal of this experiment is to observe how the pre-training technique influences DAL on hard tasks (i.e., Waterbird) and easy/well-studied tasks (i.e., MNIST) and how feature representations generated by basic learners influence DAL methods.

近年来，预训练已成为 DNN 研究和应用的核心技术 [25]。 在我们的工作中，我们选择了 Waterbird 和 MNIST 数据集进行对比实验，分别使用非预训练的 ResNet18 和预训练的 ResNet18(在 ImageNet-1K 数据上预训练，简称 ResNet18P)。 Waterbird 和 MNIST 有着完全不同的性质。 水鸟数据集包含虚假相关性，非预训练模型将更多地关注背景，例如，当背景为水时，分类器会将陆鸟错误地分类为水鸟。 预训练模型提供更好的特征表示并允许更好的对象语义(参见图 3)。 水鸟根据对象和背景分为四组：{(waterbird, water), (waterbird, land), (landbird, land), (landbird, land)}。 除了整体准确度外，我们还报告了不匹配和最差组准确度 [42, 54]，分别指组 {(waterbird, land), (landbird, water)} 的准确度以及四组中最低的准确度。 在 MNIST 上，没有有效的背景信息。 因此，带和不带预训练的模型都将关注语义本身。 本实验的目的是观察预训练技术如何影响 DAL 在困难任务(即 Waterbird)和简单/充分研究的任务(即 MNIST)上的影响，以及基本学习器生成的特征表示如何影响 DAL 方法。

Figure 2: Ablation studies on varying number of epochs and batch size on CIFAR10.
图 2：CIFAR10 上不同时期数和批量大小的消融研究。

Figure 3: Activation maps of ResNet18 w/ and w/o ImageNet1K pre-training. 
图 3：带和不带 ImageNet1K 预训练的 ResNet18 激活图。

Figure 4: Overall (mismatch, worst group) accuracy vs. budget curves on MNIST and Waterbird datasets. 
图 4：MNIST 和 Waterbird 数据集的总体(不匹配，最差组)准确度与预算曲线。

Figure 4 presents overall (also mismatch and worst group in Waterbird) accuracy vs. budget curves for MNIST and Waterbird. On MNIST, pre-training does enhance overall DAL performance, but the ranking across these methods is not change (except for LPL), e.g., Entropy > EntropyD > KMeans in both MNIST with ResNet18 and ResNet18P. LPL performs far better based on ResNet18P, since loss prediction is more accurate with better feature representations. In Waterbird, considering ResNet18 w/o pre-training, normal DAL methods like Entropy, EntropyD, CEAL and KMeans are affected by the quality of backbone, which influences the DAL selections. Moreover, selecting more data even induces more biases (Waterbird is imbalanced among four groups) and cause performance reduction. These DAL methods return to normal when using the pre-trained ResNet18P, since it helps generate predictions with accurate directions (i.e., focus on the object itself, as shown in Figure 3). On Waterbird, LPL and WAAL w/o pre-training has better performance, possibly because they acquire more information with help of enhancing techniques (i.e., loss prediction and collecting unlabeled data information). However, LPL and WAAL could not well learn worst group under both w/ and w/o pre-training situations. A possible explanation is that they are affected by the imbalance problems of these groups, which induces bias problems in loss prediction and collecting unlabeled data information, and results in poor performance of the worst group. 

图 4 显示了 MNIST 和 Waterbird 的总体(也是 Waterbird 中的不匹配和最差组)精度与预算曲线。 在 MNIST 上，预训练确实提高了整体 DAL 性能，但这些方法的排名没有变化(LPL 除外)，例如，在 MNIST 中，熵 > 熵 D > KMeans 与 ResNet18 和 ResNet18P。 LPL 基于 ResNet18P 表现得更好，因为损失预测更准确，具有更好的特征表示。 在 Waterbird 中，考虑到没有预训练的 ResNet18，像 Entropy、EntropyD、CEAL 和 KMeans 这样的普通 DAL 方法会受到骨干质量的影响，这会影响 DAL 选择。 此外，选择更多的数据甚至会导致更多的偏差(Waterbird 在四组之间不平衡)并导致性能下降。 当使用预训练的 ResNet18P 时，这些 DAL 方法恢复正常，因为它有助于生成具有准确方向的预测(即关注对象本身，如图 3 所示)。 在 Waterbird 上，没有预训练的 LPL 和 WAAL 具有更好的性能，可能是因为它们在增强技术(即损失预测和收集未标注数据信息)的帮助下获得了更多信息。 然而，LPL 和 WAAL 在有和没有预训练的情况下都不能很好地学习最差组。 一种可能的解释是，它们受到这些组的不平衡问题的影响，导致损失预测和收集未标注数据信息的偏差问题，导致最差组的表现不佳。

## 4 Challenges and Future Directions of DAL
Since there is little room for improving DAL by only designing acquisition functions as shown in Sections 2.2 and 3.2, researchers focus on proposing effective ways to enhance DAL methods like LPL and enlarging batch size in each round to reduce the time and computation cost. However, enhancing methods might not work well on all tasks (as shown in Table 1). Better and more universal enhancement methods are needed in DAL. Cluster-Margin have scaled to batch sizes (100K-1M) several orders of magnitude larger than previous studies [12], it is hard to be transcended.

由于仅通过设计第 2.2 节和第 3.2 节所示的采集功能来改进 DAL 的空间很小，因此研究人员专注于提出有效的方法来增强 LPL 等 DAL 方法，并在每一轮中扩大批量大小以减少时间和计算成本。 然而，增强方法可能不适用于所有任务(如表 1 所示)。 DAL 需要更好、更通用的增强方法。 Cluster-Margin 已经扩展到批量大小(100K-1M)，比之前的研究[12]大几个数量级，很难被超越。

Another notable situation is the research trend shifting towards developing new methodologies to utilize better unlabeled data like semi- and self-supervised learning (S4L). Chan et al. [8] integrated S4L and DAL and conducted extensive experiments with contemporary methods, demonstrating that much of the benefit of DAL is subsumed by S4L techniques. A clear direction is to better leverage the unlabeled data during training in the DAL process under the very few label regimes.

另一个值得注意的情况是研究趋势转向开发新方法以利用更好的未标注数据，如半监督和自监督学习 (S4L)。 陈等。 [8] 集成了 S4L 和 DAL，并使用现代方法进行了广泛的实验，证明了 DAL 的大部分好处都包含在 S4L 技术中。 一个明确的方向是在极少数标签制度下的 DAL 过程中的训练期间更好地利用未标注的数据。

Recently, some researchers employed DAL on more complex tasks like Visual Question Answering (VQA) [30] and observed that DAL methods might not perform well.

最近，一些研究人员将 DAL 用于视觉问答 (VQA) [30] 等更复杂的任务，并观察到 DAL 方法可能表现不佳。

Many potential reasons limit DAL performance: i) task-specific DAL may be needed for those specific tasks; ii) better feature representations are needed; and iii) various dataset properties need to be considered, like collective outliers in VQA tasks [30]. These are possible research directions that demand prompt solutions. Therefore, with the increasing demand for dealing with larger and more complex data in realistic scenarios, e.g., out-of-distribution (OOD), rare classes, imbalanced data, and larger unlabeled data pool [12, 37], DAL under different data types are becoming more popular. E.g., Kothawade et al. [37] let DAL work on rare classes, redundancy, imbalanced, and OOD data scenarios.

许多潜在的原因限制了 DAL 性能：i)那些特定任务可能需要特定于任务的 DAL;  ii) 需要更好的特征表示;  iii)需要考虑各种数据集属性，例如 VQA 任务中的集体异常值 [30]。 这些是需要迅速解决的可能的研究方向。 因此，随着现实场景中处理更大更复杂数据的需求不断增加，例如分布外(OOD)、稀有类、不平衡数据和更大的未标注数据池 [12, 37]，不同数据下的 DAL 类型越来越受欢迎。 例如，Kothawade et al. [37] 让 DAL 处理稀有类、冗余、不平衡和 OOD 数据场景。

Another possible research direction is to apply DAL techniques with new data-insufficient tasks like automatic driving, medical image analysis, etc. Haussmann et al. [23] have applied AL in an autonomous driving setting of nighttime detection of pedestrians and bicycles to improve nighttime detection of pedestrians and bicycles. It has shown improved detection accuracy of self-driving DNNs over manual curation – data selected with AL yields relative improvements in mean average precision of 3× on pedestrian detection and 4.4× on detection of bicycles over manually-selected data. Budd et al. [5] presents a survey on AL adoption in the medical domain. Different tasks have different concerns when integrating DAL techniques. For instance, In medical imaging, there are many rare yet important diseases (e.g., various forms of cancers), while non-cancerous images are much more than compared to the cancerous ones [37]. Therefore, rare classes must be considered when designing AL strategies in medical image analysis. More importantly, labeling medical images require expertise, and annotation costs and effort remain significant. Task-specific DAL is also worthy of research in recent years.

另一个可能的研究方向是将 DAL 技术应用于新的数据不足的任务，如自动驾驶、医学图像分析等。Haussmann et al. [23] 已将 AL 应用于行人和自行车夜间检测的自动驾驶设置，以改善行人和自行车的夜间检测。 它表明，自动驾驶 DNN 的检测精度高于手动管理——与手动选择的数据相比，使用 AL 选择的数据在行人检测和自行车检测方面的平均精度分别提高了 3 倍和 4.4 倍。 巴德et al. [5] 介绍了一项关于 AL 在医学领域的采用情况的调查。 在集成 DAL 技术时，不同的任务有不同的关注点。 例如，在医学成像中，有许多罕见但重要的疾病(例如，各种形式的癌症)，而与癌性图像相比，非癌性图像要多得多 [37]。 因此，在医学图像分析中设计 AL 策略时必须考虑稀有类别。 更重要的是，标注医学图像需要专业知识，并且标注成本和工作量仍然很大。 Task-specific DAL也是近年来值得研究的。

## Acknowledgments and Disclosure of Funding
Parts of experiments (including experiments on PneumoniaMNIST and BreakHis datasets) in this paper were carried out on Baidu Data Federation Platform (Baidu FedCube). For usages, please contact us via {fedcube,shubang}@baidu.com.

## References
1. Charu C Aggarwal, Xiangnan Kong, Quanquan Gu, Jiawei Han, and S Yu Philip. Active learning: A survey. In Data Classification, pages 599–634. Chapman and Hall/CRC, 2014.
2. Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batch active learning by diverse, uncertain gradient lower bounds. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.
3. William H Beluch, Tim Genewein, Andreas Nürnberger, and Jan M Köhler. The power of ensembles for active learning in image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 9368–9377, 2018. 10
4. Erdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using determinantal point processes. arXiv preprint arXiv:1906.07975, 2019.
5. Samuel Budd, Emma C Robinson, and Bernhard Kainz. A survey on active learning and human-in-the-loop deep learning for medical image analysis. Medical Image Analysis, 71: 102062, 2021.
6. Arantxa Casanova, Pedro O Pinheiro, Negar Rostamzadeh, and Christopher J Pal. Reinforced active learning for image segmentation. arXiv preprint arXiv:2002.06583, 2020.
7. Gavin C Cawley. Baseline methods for active learning. In Active Learning and Experimental Design workshop In conjunction with AISTATS 2010, pages 47–57. JMLR Workshop and Conference Proceedings, 2011.
8. Yao-Chun Chan, Mingchen Li, and Samet Oymak. On the marginal benefit of active learning: Does self-supervision eat its cake? In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3455–3459. IEEE, 2021.
9. Yukun Chen, Thomas A Lasko, Qiaozhu Mei, Joshua C Denny, and Hua Xu. A study of active learning methods for named entity recognition in clinical text. Journal of biomedical informatics, 58:11–18, 2015.
10. Jiwoong Choi, Ismail Elezi, Hyuk-Jae Lee, Clement Farabet, and Jose M Alvarez. Active learning for deep object detection via probabilistic modeling. arXiv preprint arXiv:2103.16130, 2021.
11. Wei Chu, Martin Zinkevich, Lihong Li, Achint Thomas, and Belle Tseng. Unbiased online active learning in data streams. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 195–203, 2011.
12. Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin Rostamizadeh, and Sanjiv Kumar. Batch active learning at scale. Advances in Neural Information Processing Systems, 34, 2021.
13. Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pages 2921–2926. IEEE, 2017.
14. Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine, 29(6):141–142, 2012.
15. Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin based approach. arXiv preprint arXiv:1802.09841, 2018.
16. Long Duong, Hadi Afshar, Dominique Estival, Glen Pink, Philip R Cohen, and Mark Johnson. Active learning for deep semantic parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 43–48, 2018.
17. Mehdi Elahi, Francesco Ricci, and Neil Rubens. A survey of active learning in collaborative filtering recommender systems. Computer Science Review, 20:29–50, 2016.
18. Linton C Freeman and Linton C Freeman. Elementary applied statistics: for students in behavioral science. New York: Wiley, 1965.
19. Yifan Fu, Xingquan Zhu, and Bin Li. A survey on instance selection for active learning. Knowledge and information systems, 35(2):249–283, 2013.
20. Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In International Conference on Machine Learning, pages 1183–1192. PMLR, 2017.
21. Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint arXiv:1711.00941, 2017.
22. Daniel Gissin and Shai Shalev-Shwartz. Discriminative active learning. arXiv preprint arXiv:1907.06347, 2019. 11
23. Elmar Haussmann, Michele Fenzi, Kashyap Chitta, Jan Ivanecky, Hanson Xu, Donna Roy, Akshita Mittel, Nicolas Koumchatzky, Clement Farabet, and Jose M Alvarez. Scalable active learning for object detection. In 2020 IEEE Intelligent Vehicles Symposium (IV), pages 1430– 1435. IEEE, 2020.
24. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
25. Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty. In International Conference on Machine Learning, pages 2712–2721. PMLR, 2019.
26. Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian active learning for classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.
27. Kuan-Hao Huang. Deepal: Deep active learning in python. arXiv preprint arXiv:2111.15258, 2021.
28. Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535–547, 2019.
29. Michael Kampffmeyer, Arnt-Borre Salberg, and Robert Jenssen. Semantic segmentation of small objects and modeling of uncertainty in urban remote sensing images using deep convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 1–9, 2016.
30. Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D Manning. Mind your outliers! investigating the negative impact of outliers on active learning for visual question answering. arXiv preprint arXiv:2107.02331, 2021.
31. Daniel S Kermany, Michael Goldbaum, Wenjia Cai, Carolina CS Valentim, Huiying Liang, Sally L Baxter, Alex McKeown, Ge Yang, Xiaokang Wu, Fangbing Yan, et al. Identifying medical diagnoses and treatable diseases by image-based deep learning. Cell, 172(5):1122–1131, 2018.
32. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
33. Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: Generalization based data subset selection for efficient and robust learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 8110–8118, 2021.
34. Andreas Kirsch, Joost Van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. Advances in neural information processing systems, 32, 2019.
35. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, pages 5637–5664. PMLR, 2021.
36. Christine Körner and Stefan Wrobel. Multi-class ensemble-based active learning. In European conference on machine learning, pages 687–694. Springer, 2006.
37. Suraj Kothawade, Nathan Beck, Krishnateja Killamsetty, and Rishabh Iyer. Similar: Submodular information measures based active learning in realistic scenarios. Advances in Neural Information Processing Systems, 34, 2021.
38. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 12
39. Punit Kumar and Atul Gupta. Active learning query strategies for classification, regression, and clustering: a survey. Journal of Computer Science and Technology, 35(4):913–945, 2020.
40. Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world, 2016.
41. Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015.
42. Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan. An empirical study on distribution shift robustness from the perspective of pre-training and data augmentation. arXiv preprint arXiv:2205.12753, 2022.
43. Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati, and Shadab Khan. Towards robust and reproducible active learning using neural networks. arXiv preprint arXiv:2002.09564, 2020.
44. Usman Naseem, Matloob Khushi, Shah Khalid Khan, Kamran Shaukat, and Mohammad Ali Moni. A comparative analysis of active learning for biomedical text mining. Applied System Innovation, 4(1):23, 2021.
45. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
46. Vu-Linh Nguyen, Sébastien Destercke, and Eyke Hüllermeier. Epistemic uncertainty sampling. In International Conference on Discovery Science, pages 72–86. Springer, 2019.
47. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.
48. Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.
49. Davi Pereira-Santos, Ricardo Bastos Cavalcante Prudêncio, and André CPLF de Carvalho. Empirical investigation of active learning strategies. Neurocomputing, 326:15–27, 2019.
50. Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and José Miguel Hernández-Lobato. Bayesian batch active learning as sparse subset approximation. Advances in neural information processing systems, 32:6359–6370, 2019.
51. Maria E Ramirez-Loaiza, Manali Sharma, Geet Kumar, and Mustafa Bilgic. Active learning: an empirical study of common baselines. Data mining and knowledge discovery, 31(2):287–313, 2017.
52. Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B Gupta, Xiaojiang Chen, and Xin Wang. A survey of deep active learning. ACM Computing Surveys (CSUR), 54 (9):1–40, 2021.
53. Soumya Roy, Asim Unmesh, and Vinay P Namboodiri. Deep active learning for object detection. In BMVC, volume 362, page 91, 2018.
54. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In International Conference on Learning Representations, 2019.
55. Isah Charles Saidu and Lehel Csató. Active learning with bayesian unet for efficient semantic image segmentation. Journal of Imaging, 7(2):37, 2021.
56. Andrew I Schein and Lyle H Ungar. Active learning for logistic regression: an evaluation. Machine Learning, 68(3):235–265, 2007.
57. Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. arXiv preprint arXiv:1708.00489, 2017. 13
58. Robin Senge, Stefan Bösner, Krzysztof Dembczy´nski, Jörg Haasenritter, Oliver Hirsch, Norbert Donner-Banzhoff, and Eyke Hüllermeier. Reliable classification: Learning classifiers that distinguish aleatoric and epistemic uncertainty. Information Sciences, 255:16–29, 2014.
59. Burr Settles. Active learning literature survey. 2009.
60. Burr Settles and Mark Craven. An analysis of active learning strategies for sequence labeling tasks. In proceedings of the 2008 conference on empirical methods in natural language processing, pages 1070–1079, 2008.
61. Claude Elwood Shannon. A mathematical theory of communication. ACM SIGMOBILE mobile computing and communications review, 5(1):3–55, 2001.
62. Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod, and Animashree Anandkumar. Deep active learning for named entity recognition. arXiv preprint arXiv:1707.05928, 2017.
63. Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang. Deep active learning: Unified and principled method for query and training. In International Conference on Artificial Intelligence and Statistics, pages 1308–1318. PMLR, 2020.
64. Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5972–5981, 2019.
65. Sayanan Sivaraman and Mohan M Trivedi. Active learning for on-road vehicle detection: A comparative study. Machine vision and applications, 25(3):599–611, 2014.
66. Fabio A Spanhol, Luiz S Oliveira, Caroline Petitjean, and Laurent Heutte. A dataset for breast cancer histopathological image classification. Ieee transactions on biomedical engineering, 63 (7):1455–1462, 2015.
67. Toan Tran, Thanh-Toan Do, Ian Reid, and Gustavo Carneiro. Bayesian generative active deep learning. In International Conference on Machine Learning, pages 6295–6304. PMLR, 2019.
68. Devis Tuia, Michele Volpi, Loris Copa, Mikhail Kanevski, and Jordi Munoz-Mari. A survey of active learning algorithms for supervised remote sensing image classification. IEEE Journal of Selected Topics in Signal Processing, 5(3):606–617, 2011.
69. Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International joint conference on neural networks (IJCNN), pages 112–119. IEEE, 2014.
70. Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-effective active learning for deep image classification. IEEE Transactions on Circuits and Systems for Video Technology, 27(12):2591–2600, 2016.
71. Meng Wang and Xian-Sheng Hua. Active learning in multimedia annotation and retrieval: A survey. ACM Transactions on Intelligent Systems and Technology (TIST), 2(2):1–21, 2011.
72. Tianyang Wang, Xingjian Li, Pengkun Yang, Guosheng Hu, Xiangrui Zeng, Siyu Huang, Cheng-Zhong Xu, and Min Xu. Boosting active learning via improving test performance. arXiv preprint arXiv:2112.05683, 2021.
73. Kai Wei, Rishabh Iyer, and Jeff Bilmes. Submodularity in data subset selection and active learning. In International conference on machine learning, pages 1954–1963. PMLR, 2015.
74. Jian Wu, Victor S Sheng, Jing Zhang, Hua Li, Tetiana Dadakova, Christine Leon Swisher, Zhiming Cui, and Pengpeng Zhao. Multi-label active learning algorithms for image classification: Overview and future promise. ACM Computing Surveys (CSUR), 53(2):1–35, 2020.
75. Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.
76. Yichen Xie, Masayoshi Tomizuka, and Wei Zhan. Towards general and efficient active learning. arXiv preprint arXiv:2112.07963, 2021. 14
77. Changchang Yin, Buyue Qian, Shilei Cao, Xiaoyu Li, Jishang Wei, Qinghua Zheng, and Ian Davidson. Deep similarity-based batch mode active learning with exploration-exploitation. In 2017 IEEE International Conference on Data Mining (ICDM), pages 575–584. IEEE, 2017.
78. Donggeun Yoo and In So Kweon. Learning loss for active learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93–102, 2019.
79. Xueying Zhan, Qing Li, and Antoni B Chan. Multiple-criteria based active learning with fixed-size determinantal point processes. arXiv preprint arXiv:2107.01622, 2021.
80. Xueying Zhan, Huan Liu, Qing Li, and Antoni B. Chan. A comparative survey: Benchmarking for pool-based active learning. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event / Montreal, Canada, 19-27 August 2021, pages 4679–4686. ijcai.org, 2021.
81. Zhen Zhao, Miaojing Shi, Xiaoxiao Zhao, and Li Li. Active crowd counting with limited supervision. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XX 16, pages 565–581. Springer, 2020.
82. Fedor Zhdanov. Diverse mini-batch active learning. arXiv preprint arXiv:1901.05954, 2019.
83. Jia-Jie Zhu and José Bento. Generative adversarial active learning. arXiv preprint arXiv:1702.07956, 2017. 15 

## A Related Work: comparison with other existing DAL toolkits
The existing major DAL toolkits/libraries include:
* Our previous work DeepAL2 [27].
* Pytorch Active Learning (PAL) Library 3 . Accompanied with the book Human-in-the-Loop Machine Learning.
* DISTIL library4.

Compared with our previous work DeepAL, we 1) add more built-in support datasets/tasks, where DeepAL only support CIFAR10, MNIST, FashionMNIST and SVHN, while in DeepAL+, we add EMNIST, TinyImageNet, PneumoniaMNIST, BreakHis and wilds-series tasks [35] like waterbirds. 2) We then optimize part of existing algorithms to make it better to be adopted on Deep Active Learning tasks like KMeans, we re-implemented it by using faiss-gpu libirary [4], it is much faster and perform better than scikit-learn library [48] based KMeans implementation. We conduct Principal Component Analysis (PCA) to reduce dimension on representativeness-based approach, i.e., KCenter since it costs too much memory for storing pair-wise similarity matrix on DAL tasks. Note that both DeepAL and DeepAL+ remove CoreSet approach [57] since Coreset uses the greedy 2-OPT solution for the k-Center problem as an initialization and checks the feasibility of a mixed integer program (MIP). They adopted Gourbi optimizer5 to solve MIP and it is not a free optimizer. The users can use KCenter, a greedy optimization of CoreSet. 3) We Add more independent DAL methods implementations like MeanSTD, VarRatio, BADGE, LPL, VAAL, WAAL and CEAL.

PAL library is a fundamental human-in-loop framework. Users need to interact with computers by inputting the ground truth labels of the instance asked by the computer. Besides typical uncertainty-/representativeness-/diversity-/adaptive-based AL approaches like Least Confidence, PAL also includes AL with transfer learning (ALTL). PAL is more likely to provide a template and tell people how to apply AL to different human-in-loop tasks. If someone is new to the AL research field and he could try to use this library to understand how AL works.

DISTIL library majorly serves for the submodular functions proposed by the authors group [37], besides the implementations that are already implemented in DeepAL and DeepAL+, they have own implementations like FASS [73], BatchBALD [34] (we have also implemented this method, it is based on BALD, but it is really memory consuming so we finally deleted it.), Glister [33] (for robust learning) and Submodular (Conditional) Mutual Information (S(C)MI) [37] for AL. It is a easy-to-use library especially if someone want to use their submodular functions. They have no implementations of AL with enhanced techniques like LPL and WAAL.

We made a brief comparison between our DeepAL+ and existing DAL libraries, see Table 2.

Table 2: Comparison between our DeepAL+ and existing DAL libraries. We excluded Random since strictly speaking, it does not belong to AL approaches. 

2 https://github.com/ej0cl6/deep-active-learning 
3 https://github.com/rmunro/pytorch_active_learning 
4 https://github.com/decile-team/distil 
5 https://www.gurobi.com/ 

## B More introduction of DeepAL+ toolkit
We listed some introductions of our DeepAL+ in previous Section A. DeepAL+ is user-friendly, using a single command can run experiments, we construct the framework/benchmark by easy-to-separate mode, we split the basic networks, querying strategies, dataset/task design, and parameters add-in (e.g., set numbers of training epochs, optimizer parameters). It is simple to add new AL sampling strategies, new basic backbones, and new datasets/tasks in these benchmarks. It makes the users propose new AL sampling strategies easier, test new methods on multiple basic tasks, and compare them with most SOTA DAL methods. We sincerely hope our DeepAL+ would help researchers in the DAL research field reduce unnecessary workload and focus on designing new DAL approaches more. This work is ongoing; we would continually add the latest and well-perform DAL approaches and incorporate more datasets/tasks. Moreover, if newly proposed DAL methods are designed based on DeepAL or DeepAL+, it would be easier to be further incorporated into our toolkit like BADGE and WAAL.

## C Licences
Datasets. We listed the licence of datasets we used in our experiments, all datasets employed in our comparative experiments are public datasets:
* CIFAR10 and CIFAR100 [38]: MIT Licence.
* MNIST [14], EMNIST [13]: Creative Commons Attribution-Share Alike 3.0 license.
* FashionMNIST [75]: MIT Licence.
* PneumoniaMNIST [31]: CC BY 4.0 License.
* BreakHis [66]: Creative Commons Attribution 4.0 International License.
* Waterbird [35, 54]: MIT License.

Methods. We listed all related license of the original implementations of DAL methods that we re-implemented and basic backbone models in our DeepAL+ toolkit:
* PyTorch [47]: Modified BSD.
* Scikit-Learn [48]: BSD License.
* BADGE [2]: Not listed.
* LPL [78]: Not listed.
* VAAL [64]: BSD 2-Clause “Simplified” License.
* WAAL [63]: Not listed.
* CEAL [70]: Not listed.
* Methods originated from DeepAL [27] library implementation: MIT Licence.
* KMeans (faiss library [28] implementation): MIT Licence.
* ResNet18 [24]: MIT License.

## D Experimental Settings
### D.1 Datasets
Considering some DAL approaches currently only support computer vision tasks like VAAL, for consistency and fairness of our experiments, we adopt 1) the image classification tasks, similar to most DAL papers. We use the following datasets (details in Table 3): MNIST [14], FashionMNIST [75], EMNIST [13], SVHN [45], CIFAR10 and CIFAR100 [38] and Tiny ImageNet [41]. Additionally, to explore DAL performance on imbalanced data, we construct an imbalanced dataset based on CIFAR10, called CIFAR10-imb, which sub-samples the training set with ratios of 1:2:· · · :10 for classes 0 through 9. 2) The medical image analysis tasks, including Breast Cancer Histopathological Image Classification (BreakHis) [66] and Pneumonia-MNIST (pediatric chest X-ray) (PneumoniaMNIST) 17 [31]. Additionally, we adopted an object recognition with correlated backgrounds dataset (Waterbird) [54], it considers two classes: waterbird and landbird. These objected were manually mixed to water and land background, and waterbirds (landbirds) more frequently appearing against a water (land) background. It is a challenging task since DNNs might spuriously rely on background instead of learning to recognize semantic/object.

Table 3: Datasets used in comparative experiments. #i is the size of initial labeled pool, #u is the size of unlabeled data pool, #t is the size of testing set, #k is number of categories and #e is number of epochs used to train the basic classifier in each AL round.

### D.2 Implementation details
We employed ResNet186 [24] as the basic learner. On MNIST, EMNIST, FashionMNIST, TinyImagenet, CIFAR10 and CIFAR100, we adopted Adam optimizer (learning rate: 1e − 3) . On PneumoniaMNIST, BreakHis and Waterbird, since Adam would cause overfitting, we use SGD optimizer with learning rate: 1e-2 on BreakHis and PneumoniaMNIST, learning rate: 0.0005, weight decay: 1e-5, momentum: 0.9 on Waterbird.

For a fair comparison, consistent experimental settings of the basic classifier are used across all DAL methods. The dataset-specific implementation details are discussed as follows.
* MNIST, FashionMNIST and EMNIST: number of training epochs is 20, the kernel size of the first convolutional layer in ResNet18 is 7 × 7 (consistent with the original PyTorch implementation), input pre-processing step include normalization.
* CIFAR10, CIFAR100: number of training epochs is 30, the kernel size of the first convolutional layer in ResNet18 is 3 × 3 (consistent with PyTorch-CIFAR implementation7 ), input pre-processing steps include random crop (pad=4), random horizontal flip (p = 0.5) and normalization.
* TinyImageNet: number of training epochs is 40, the same implementation of ResNet18 as CIFAR, input pre-processing steps include random rotation (degree=20), random horizontal flip (p = 0.5) and normalization.
* SVHN: number of training epochs is 20, the same implementation of ResNet18 as MNIST, input pre-processing steps include normalization.
* BreakHis: number of training epochs is 10, the same implementation of ResNet10 as CIFAR, input pre-proccessing steps include random rotation (degree=90), random horizontal flip (p = 0.8), random resize crop (scale=224), randomly change the brightness, contrast, saturation and hue of image – ColorJitter (brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1) and normalization.
* PneumoniaMNIST: number of training epochs is 10, the same implementation o ResNet18 as CIFAR, input pre-processing steps include resize (shape=255), center crop (shape = 224), random horizontal flip (p = 0.5), random rotation (degree=10), random gray scale, random affine (translate=(0.05, 0.05), degree=0). 
* Waterbirds: number of training epochs is 30, the same implementation of ResNet18 as MNIST, input pre-processing steps include random horizontal flip (p = 0.5).

6 https://pytorch.org/vision/stable/models.html#id10 
7 https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py 18

The model-specific implementation details are discussed as follows. For MC Dropout implementation, we employed 10 forward passes. For CEAL(Entropy), we set threshold of confidence/entropy score for assigning pseudo labels as 1e − 5. For KCenter, since using the full feature vector would take too much memory in pair-wise distance calculation, we employ Principal components analysis (PCA) to reduce feature dimension to 32 according to [30]. For VAE in VAAL, we followed the same architecture in [64] and train VAE with 30 epochs with Adam optimizer (learning rate: 1e − 3). For LPL, we train LossNet with Adam optimizer (learning rate: 1e − 2); since LossNet is co-trained with basic classifier, we firstly co-trained LossNet and basic classifier followed by normal training processes, then we detached feature updating (the same as stop training basic classifier) and assign 20 extra epochs for training LossNet.

### D.3 Experimental environments in our comparative experiments
We conduct experiments on a single Tesla V100-SXM2 GPU with 16GB memory except for running experiments on PneumoniaMNIST and BreakHis, since running them need > 16GB and < 32GB memories. We run experiments of PneumoniaMNIST and BreakHis on another single Tesla V100-SXM2 GPU with 32GB memory. We only use a single GPU for each experiment.

## E Completed Experimental Results
### E.1 Overall experiments
#### E.1.1 Performance of Standard Image Classification tasks.
Tables 4, 5, 6 record the overall performances of standard image classification tasks group, including MNIST, FashionMNIST, EMNIST, SVHN, CIFAR10, CIFAR1O-imb, CIFAR100 and TinyImageNet datasets. Including AUBC (acc) performance with mean and standard deviation over 3 trials, the average running time that takes the running time of Random as unit and the F-acc score.

Note that KMeans(GPU) performs better than KMeans on major tasks, i.e., Table 5. However, from the average running time, KMeans(GPU) seems to have more time than KMeans, it does not mean KMeans(GPU) run slower than KMeans, since the running time calculation does not count the waiting time, e.g., wait for memory allocation, the time for data load from GPU to CPU or from CPU to GPU. In KMeans, in every AL iteration, we need to load data (feature embedding) from GPU to CPU and use scikit-learn library to calculate. At this step, the program must waste time waiting for the operating system to allocate memory for calculation. Nevertheless, these waiting times could be saved in KMeans(GPU). So actually KMeans(GPU) run faster than KMeans on DAL tasks that use GPU for calculation.

#### E.1.2 Performance of Medical Image Analysis tasks.
Table 7 records the overall performances of medical image analysis group, including PneumoniaMNIST and BreakHis datasets. Both LPL, WAAL and BADGE perform well on these medical image analysis tasks. Another thing that worth to pay attention is: all MC dropout based versions (i.e., LeastConfD, MarginD. EntropyD, as well as BALD), perform worse than original versions (i.e., LeastConf, Margin and Entropy), especially on PneumonialMNIST. For example, on PneumonialMNIST, the AUBC value of LeastConf is 0.852, while LeastConfD only have 0.8243 AUBC value. A potential reason is in PneumoniaMNIST, to justify whether an image – a chest X-ray report pneumonia, one needs to check the local lesions and observe the lung’s overall condition. The basic learner needs both local and global features to make an accurate prediction. While MC dropout reduces the model capacity and might ignore some feature information, making less convincing predictions [3] and hurt DAL performance. Another phenomenon is, considering F-acc, we noticed that many DAL approaches’ F-acc are higher than the accuracy trained on full dataset (0.9039), e.g.,9149 on MarginD, 0.9204 on BALD, 0.9179 on CEAL, 0.9197 on AdvBIM. These results can be summarized as one phenomenon: the subset selected from the full subset would contribute to better performance. This is because PheumoniaMNIST contains distribution/dataset shift between training and testing set. Also, this dataset might contain redundant data samples and confusing 19 information. That is, these medical images are not one-to-one. They are many-to-one. One patient would correspond to several chest X-ray images, which causes redundancy. Additionally, some patients may have more than one disease e.g., we can see on some X-ray images that there is a posterior spinal fixator that the patient used to fix his spine. These features also might influence the predictions.

Compared with standard tasks (i.e., standard image classification tasks in our comparative survey), real-life applications would encounter more unexpected problems like we discussed aforementioned PneumoniaMNIST dataset. That is why we are working on adding more different kinds of tasks for testing DAL approaches. We also encourage DAL researchers to try DAL approaches on various data scenarios and tasks.

Table 4: Results of DAL comparative experiments with MNIST w/ and w/o pre-train and Waterbird w/ and w/o pre-train. We report the AUBC for overall accuracy, final accuracy (F-acc) after quota Q is exhausted, and the average running time of the whole AL processes (including training and querying processes) relative to Random. We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively. “#” indicates that the experiment has not been completed yet.

Table 5: Results of DAL comparative experiments, including CIFAR10, CIFAR10-imb, CIFAR100 and SVHN. We report the AUBC for overall accuracy, final accuracy (F-acc) after quota Q is exhausted, and the average running time of the whole AL processes (including training and querying processes) relative to Random. We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively. “#” indicates that the experiment has not been completed yet.

### E.2 DAL method ranking and summarizing
We next consider the overall performance of DAL methods on the eight standard image classification datasets and two medical image analysis datasets using win-tie-loss counts, respectively, as shown in Table 9. We use a margin of 0.5%, e.g., a “win” is counted for method A if it outperforms method B by 0.5% in pairwise comparison. Table 9 shows the advantage of uncertainty-based DAL methods likeLeastConfD (3rd), and pseudo labeling for enhancing uncertainty-based DAL methods, i.e., CEAL (2nd). WAAL perform the best. Additionally, Dropout method also can improve DAL methods, e.g., LeastConfD ranks 3nd while LeastConf only ranks 9th. LPL only 13th. Although they achieve the best performances on some datasets (e.g., SVHN, CIFAR10) and have high win counts, they also perform extremely poorly on other datasets (e.g., Tiny ImageNet), which contributes to their low ranking. VAAL and KMeans perform even worse than Random. AdvBIM ranks far behind due to many incomplete tasks. This is a drawback of AdvBIM (also of AdvDeepFool), that is, these methods spend too much for re-calculating adversarial distance r for each unlabeled data sample per AL round. 


Table 6: Results of DAL comparative experiments, including EMNIST, FashionMNIST and TinyImageNet. We report the AUBC for accuracy, final accuracy (F-acc) after quota Q is exhausted, and the average running time of the whole AL processes (including training and querying processes) relative to Random. We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively. “∗” indicates that the experiment needed too much memory, e.g., KCenter on EMNIST, while “#” indicates that the experiment has not been completed yet.

Table 7: Results of DAL comparative experiments, including PneumoniaMNIST and BreakHis. We report the AUBC for overall accuracy, final accuracy (F-acc) after quota Q is exhausted, and the average running time of the whole AL processes (including training and querying processes) relative to Random.We rank F-acc and AUBC of each task with top 1st, 2nd and 3rd with red, teal and blue respectively. 

Table 8: Results of Waterbird. We report the AUBC for mismatch and worst group accuracy, final accuracy (F-acc) after quota Q is exhausted, and the average running time of the whole AL processes (including training and querying processes) relative to Random. We bold F-acc values that are higher than full performance. We did not rank the top three methods since labeling them is not of great reference value. “#” indicates that the experiment has not been completed yet.

On medical image analysis tasks, both WAAL and LPL outperform other DAL approaches, which constantly shows the advantage of DAL with enhancing techniques. Combined strategy, BADGE also obtained a good ranking (4th). More noticeably, BADGE always obtains comparable performances on various tasks. Therefore, for new/unseen tasks/data, we recommend first trying combined DAL approaches. In medical image analysis tasks, VAAL perform better than standard image classification tasks.

Table 9: Comparison of DAL methods using win-tie-loss across 8 datasets on standard image classification tasks and 2 medical image analysis tasks with AUBC (acc). Methods are ranked by 2 × win + tie.

To observe the performance differences on various DAL methods in varying AL stages, we provide overall accuracy-budget curves on multiple datasets, as shown in Figure 5. From this figure, it could be observed that LPL is weak in the early stage of AL processes due to the inaccurate loss prediction trained on insufficient labeled data. In later stages, by co-training LossNet and the basic classifier on more labeled data, LossNet has demonstrated its ability to enhance the basic classifier. In contrast, WAAL performs better in the early stage of AL processes due to the design of the loss function that is more suitable for AL. It helps distinguish labeled and unlabeled samples and select more representative data samples in the early stage. Therefore, WAAL brings more benefits when limiting the budget for labeling costs. 


Figure 5: Overall accuracy-budget curve of MNIST, FashionMNIST, CIFAR10, CIFAR10 (imb) and CIFAR100 datasets. The mean and standard deviation of the AUBC (acc) performance over 3 trials is shown in parentheses in the legend.

### E.3 Ablation study: numbers of training epochs and batch size
We present the accuracy-budget curves using different batch sizes and training epochs, as shown in Figure 6. A detailed analysis of this ablation study is in the main paper.

### E.4 Ablation study: w/ and w/o pre-training techniques
The detailed results of MNIST, Waterbird w/ and w/o pre-training techniques are shown in Tables 4 and 8, including the overall accuracy, mismatch group accuracy and worst group accuracy. We record AUBC (acc) with mean and standard deviation over 3 trials, F-acc and average running time. We have detailed analysed this experiment in main paper. From Tables 4 and 8, we could observe that on Waterbird w/o pre-train, most typical DAL sampling methods like LeastConf, Margin, Entropy, KCenter, etc., perform even worse than Random.

Figure 6: The accuracy-budget curve of different DAL methods on CIFAR10 dataset with different batch sizes and numbers of training epochs in each active learning round. From left to right, the value of b equals to 1, 000, 2, 000, 4, 000 and 10, 000. From top to bottom, the value of training epoch equals to 5, 10, 15, 20, 25 and 30.
