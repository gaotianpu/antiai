# Revisiting Pre-Trained Models for Chinese Natural Language Processing
重新审视汉语自然语言处理的预训练模型 https://arxiv.org/abs/2004.13922

Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. Resources available: https://github.com/ymcui/MacBERT

来自变形金刚的双向编码器表示(BERT)在各种NLP任务中表现出了惊人的改进，并且已经提出了连续的变体，以进一步提高预训练语言模型的性能。在本文中，我们的目标是重新审视汉语预训练语言模型，以检验它们在非英语语言中的有效性，并向社区发布汉语预训练的语言模型系列。我们还提出了一个简单但有效的模型，称为MacBERT，它以多种方式改进了RoBERTa，特别是采用MLM作为校正(Mac)的掩码策略。我们对八个中文NLP任务进行了广泛的实验，以重新审视现有的预训练语言模型以及提出的MacBERT。实验结果表明，MacBERT可以在许多NLP任务中实现最先进的性能，我们还用一些可能有助于未来研究的发现来消解细节。可用资源：https://github.com/ymcui/MacBERT