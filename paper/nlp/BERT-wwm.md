# Pre-Training with Whole Word Masking for Chinese BERT
https://arxiv.org/abs/1906.08101

Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and its consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we aim to first introduce the whole word masking (wwm) strategy for Chinese BERT, along with a series of Chinese pre-trained language models. Then we also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways. Especially, we propose a new masking strategy called MLM as correction (Mac). To demonstrate the effectiveness of these models, we create a series of Chinese pre-trained language models as our baselines, including BERT, RoBERTa, ELECTRA, RBT, etc. We carried out extensive experiments on ten Chinese NLP tasks to evaluate the created Chinese pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. We open-source our pre-trained language models for further facilitating our research community. Resources are available: https://github.com/ymcui/Chinese-BERT-wwm

来自变形金刚的双向编码器表示(BERT)在各种NLP任务中表现出了惊人的改进，并且已经提出了其连续变体，以进一步提高预训练语言模型的性能。在本文中，我们旨在首先介绍汉语BERT的全词掩码(wwm)策略，以及一系列汉语预训练语言模型。然后，我们还提出了一个简单但有效的模型，称为MacBERT，它以多种方式改进了RoBERTa。特别是，我们提出了一种新的掩码策略，称为MLM作为校正(Mac)。为了证明这些模型的有效性，我们创建了一系列汉语预训练语言模型作为基线，包括BERT、RoBERTa、ELECTRA、RBT等。实验结果表明，MacBERT可以在许多NLP任务中实现最先进的性能，我们还用一些可能有助于未来研究的发现来消解细节。我们开源了预先训练的语言模型，以进一步促进我们的研究社区。可用资源：https://github.com/ymcui/Chinese-BERT-wwm