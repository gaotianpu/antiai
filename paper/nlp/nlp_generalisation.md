# State-of-the-art generalisation research in NLP: a taxonomy and review
NLP中最新的泛化研究：分类和综述 2022-10-6 原文：https://arxiv.org/abs/2210.03050

## Abstract
The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what ‘good generalisation’ entails and how it should be evaluated is not well understood, nor are there any common standards to evaluate it. In this paper, we aim to lay the groundwork to improve both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP, we use that taxonomy to present a comprehensive map of published generalisation studies, and we make recommendations for which areas might deserve attention in the future. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they aim to solve, the type of data shift they consider, the source by which this data shift is obtained, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 previous papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis of the current state of generalisation research in NLP, and make recommendations for the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to make steps towards making state-of-the-art generalisation testing the new status quo in NLP. 

良好的泛化能力是自然语言处理(NLP)的首要需求之一。然而，“好的泛化”意味着什么，以及应该如何评价，目前还没有很好的理解，也没有任何共同的标准来评价它。在本文中，我们旨在为改善这两个问题奠定基础。我们提出了一种分类法，用于描述和理解NLP中的泛化研究，我们使用该分类法来呈现已发表泛化研究的全面地图，并就未来可能值得关注的领域提出建议。我们的分类法基于对泛化研究的广泛文献综述，并包含五个研究可以不同的维度：
1. 它们的主要动机
2. 它们旨在解决的泛化类型
3. 它们考虑的数据偏移类型
4. 获得数据偏移来源
5. 建模管道内偏移轨迹

我们使用我们的分类法对400多篇以前测试泛化的论文进行了分类，总共进行了600多个单独的实验。考虑到本综述的结果，我们对NLP中的泛化研究现状进行了深入分析，并对未来提出了建议。与本文一起，我们发布了一个网页，可以动态探索我们的审查结果，我们打算在新的NLP泛化研究发表后更新该网页。通过这项工作，我们的目标是采取步骤，使最先进的泛化测试NLP的新现状。

## 1 Introduction
Good generalisation, roughly defined as the ability to successfully transfer representations, knowledge, and strategies from past experience to new experiences, is one of the primary desiderata for models of natural language processing (NLP), as well as for models in the wider field of machine learning (Kirk et al., 2021; Lake et al., 2017; Elangovan et al., 2021; Linzen, 2020; Marcus, 2018, 1998; Schmidhuber, 1990; Shen et al., 2021; Wong and Wang, 2007; Yogatama et al., 2019, i.a.). For some, generalisation is crucial to ensure that models behave robustly, reliably, and fairly when making predictions about data different from the data that they were trained on, which is especially valuable when models are 1 arXiv:2210.03050v2 [cs.CL] 10 Oct 2022 employed in the real world. Others see generalisation as directly equivalent to good performance and believe that without it a model does not truly conduct the task we intended it to. Yet others strive for good generalisation in models because they believe models should behave in a human-like way – and humans are known to generalise well. While the importance of generalisation is almost undisputed, and there are countless papers on the matter, systematic generalisation testing is not the status quo in the field of NLP. 

良好的泛化，大致定义为成功地将表示、知识和策略从过去的经验迁移到新经验的能力，是自然语言处理(NLP)模型的主要需求之一，以及更广泛的机器学习领域的模型(Kirket al., 2021; Lakeet al., 2017; Elangovanet al., 2021; Linzen，2020; Marcus，2018，1998; Schmidhuber，1990; Shenet al., 2021; Wong和Wang，2007; Yogatamaet al., 2019，i.a.)。对于一些人来说，当对不同于他们所训练的数据的数据进行预测时，归纳对于确保模型表现稳健、可靠和公平至关重要，当模型在2022年10月10日的真实世界中使用时，这一点尤为重要。另一些人则认为泛化直接等同于良好的表现，并认为没有泛化，模型就无法真正执行我们预期的任务。然而，其他人则努力在模型中实现良好的泛化，因为他们认为模型应该以类似人类的方式表现，而人类也很擅长泛化。尽管泛化的重要性几乎是无可争议的，并且关于这一问题的论文数不胜数，但系统的泛化测试在NLP领域并不是现状。

At the root of this problem lies the fact that there is little understanding and agreement about what good generalisation actually entails, and what types of generalisation should be prioritised in which scenarios. While generalisation is widely discussed in NLP – in the past five years, in the ACL anthology alone over 1200 papers mentioned it in their title or abstract – there exists no systematic framework to characterise and discuss generalisation. Different studies differ amply in the assumptions they make about when and how models should generalise, and they use a wide range of different experimental and evaluation setups. As a result, it is difficult to understand what the current state of the field is when it comes to generalisation. It is difficult to understand how results in this area relate to each other, what sorts of generalisation are being addressed and which are neglected, which forms of generalisation testing we should prioritise, and how we can adequately assess generalisation in the first place. Missing answers to all of those questions are standing in the way of better model development: what we cannot measure, we cannot improve.

这个问题的根源在于，对于好的泛化实际上意味着什么，以及在哪些场景中应该优先考虑什么类型的泛化，人们几乎没有理解和共识。尽管在过去五年中，仅在ACL选集中，就有1200多篇论文在其标题或摘要中提到了泛化，但没有系统的框架来描述和讨论泛化。不同的研究在关于模型何时以及如何泛化的假设上存在很大差异，它们使用了广泛的不同实验和评估设置。因此，当涉及到泛化时，很难理解该领域的当前状态。很难理解这一领域的结果如何相互关联，什么样的泛化被处理，哪些被忽略，我们应该优先考虑哪些形式的泛化测试，以及我们如何首先充分评估泛化。所有这些问题的答案缺失阻碍了更好的模型开发：我们无法衡量的，我们无法改进。

In this paper, we introduce a new framework to systematise and understand generalisation research, and we address questions like the ones above. More precisely, 
1. We design a taxonomy to characterise generalisation research, grounded in hundreds of existing generalisation studies; 
2. We present an in-depth analysis based on over 400 papers with generalisation experiments that have come out in the last decades; 
3. We make recommendations for which areas we believe deserve attention in the near future and; 
4. We release a set of online tools that can help readers to better understand the current landscape of generalisation-testing, exploring the data by themselves.

在本文中，我们引入了一个新的框架来系统化和理解泛化研究，并解决了上述问题。更准确地说，
1. 我们以数百项现有的泛化研究为基础，设计了一种分类法来表征泛化研究; 
2. 我们基于过去几十年中发表的400多篇论文和泛化实验进行了深入分析; 
3. 我们提出了我们认为在不久的将来值得关注的领域建议; 
4. 我们发布了一套在线工具，可以帮助读者更好地理解当前的通用测试环境，自己探索数据。

With our taxonomy, analysis and online tools, we aim to lay the groundwork for making state-of-theart generalisation testing the status quo in NLP.

通过我们的分类法、分析和在线工具，我们的目标是为测试NLP的现状进行最新的泛化奠定基础。

### 1.1 What is generalisation?
Broadly speaking, generalisation is evaluated by assessing how well a model performs on a test dataset, given the relationship of this dataset with the data the model was trained on. For decades, it was common to put only one simple constraint on this relationship: that the train and test data are different. Typically, this was achieved by randomly splitting available data into a training and a test partition. Generalisation was, thus, evaluated by training and testing models on different but similarly sampled data, assumed to be independent and identically distributed (i.i.d.). In the past 20 years, we have seen great strides on such random train–test splits in a range of different applications. Since the first release of the Penn Treebank (Marcus et al., 1993), F1 scores for labelled constituency parsing went from values in the high 80’s at the end of the previous century (Collins, 1996; Magerman, 1995) and the first ten years of the current one (e.g. Petrov and Klein, 2007; Sangati and Zuidema, 2011) to scores up to 96 in the recent past (Mrini et al., 2020; Yang and Deng, 2020). On the same corpus, performance for language modelling went from per-word perplexity scores well above 100 (Kneser and Ney, 1995; Rosenfeld, 1996) to a score of 20.5 in 2020 (Brown et al., 2020). In many areas of NLP, the rate of progress has become even faster in the last few years. Scores for the popular evaluation set GLUE went from values between 60 and 70 at its release (Wang et al., 2018), to scores exceeding 90 less than a year after (most famously, Devlin et al., 2019), with performances on a wide range of tasks reaching and surpassing human-level scores 2 (e.g. Devlin et al., 2019; Wang et al., 2018; Liu et al., 2019c; Wang et al., 2019a). Yet more recently, strongly scaled-up models (e.g. Chowdhery et al., 2022) showed astounding performances on almost all existing i.i.d. natural language understanding benchmarks.

广义地说，考虑到一个模型在测试数据集上的数据与该数据集之间的关系，通过评估模型在该数据集上表现的好坏来评估泛化。几十年来，人们通常只对这种关系施加一个简单的约束：即训练数据和测试数据是不同的。通常，这是通过将可用数据随机拆分为训练集和测试集来实现的。因此，通过对不同但相似的采样数据的训练和测试模型来评估泛化，假设这些数据独立且分布相同(i.i.d.)。在过去的20年里，我们在这种随机序列上取得了巨大的进步 —— 在一系列不同的应用中进行了测试拆分。自Penn Treebank(Marcus et al.，1993)首次发布以来，F1得分标记选区解析的从上个世纪末80年代的高分(Collins，1996; Magerman，1995)和当前十年的前十年(例如Petrov和Klein，2007; Sangati和Zuidema，2011)上升到最近的96分(Mrini et al.、2020; Yang和Deng，2020)。在同一语料库中，语言建模的表现从远高于100的单词困惑度得分(Kneser和Ney，1995; Rosenfeld，1996)到2020年的20.5分(Brownet al., 2020)。在NLP的许多领域，进展速度在过去几年中变得更快。流行的评估集GLUE得分从发布时的60分到70分(Wanget al., 2018年)，在不到一年的时间内超过90分(最著名的是Devlinet al., 2019年)，其在广泛任务中的表现达到并超过了人类水平的2分(例如Devlin et al.，2019; Wanget al 2018; Liuet al., 2019c; Wang等，2019a)。然而，最近，大规模的模型(例如Chowdheryet al., 2022)在几乎所有现有的i.i.d.自然语言理解基准上显示出惊人的性能。

With this progress, however, came the realisation that, for an NLP model, reaching very high or human-level scores on an i.i.d. test set does not imply that the model robustly generalises to a wide range of different scenarios. In the recent past, we witnessed a surge of different studies pointing out generalisation failures in neural models that have state-of-the-art scores on random train–test splits (Blodgett et al., 2016; Marcus, 2018; Sinha et al., 2021; Khishigsuren et al., 2022; Kim and Linzen, 2020; Lake and Baroni, 2018; McCoy et al., 2019; Plank, 2016; Razeghi et al., 2022, to give just a few examples). Some show that when models perform well on i.i.d. test splits, they might rely on simple heuristics that do not robustly generalise in a wide range of non-i.i.d. scenarios (Gardner et al., 2020; Kaushik et al., 2019; McCoy et al., 2019), that models over-rely on stereotypes (Parrish et al., 2022; Srivastava et al., 2022), or bank on memorisation rather than generalisation (Lewis et al., 2021; Razeghi et al., 2022). Others, instead, discuss cases in which performances drop when the evaluation data differs from the training data in terms of genre, domain or topic (e.g. Michel and Neubig, 2018; Malinin et al., 2021; Plank, 2016), or when it is produced by different subpopulations (e.g. Blodgett et al., 2016; Dixon et al., 2018). Yet others focus on models’ inability to generalise compositionally (Dankers et al., 2022; Kim and Linzen, 2020; Lake and Baroni, 2018; Li et al., 2021c), structurally (Sinha et al., 2021; Weber et al., 2021a; Wei et al., 2021), to longer sequences (Dubois et al., 2020; Raunak et al., 2019), or to slightly different task formulations of the same problem (Srivastava et al., 2022).

然而，随着这一进展，人们认识到，对于NLP模型，在i.i.d.测试集上达到非常高的分数或人类水平的分数并不意味着该模型能够广泛地泛化到各种不同的场景。在最近的过去，我们见证了大量不同的研究，指出了在随机训练-测试分割方面具有最先进分数的神经模型中的泛化失败(Blodgettet al., 2016; Marcus，2018; Sinhaet al., 2021; Khishigsurenet al., 2022; Kim和Linzen，2020; Lake和Baroni，2018;McCoyet al., 2019;Plank，2016;Razeghi等，2022年，仅举几个例子)。一些研究表明，当模型在i.i.d.测试拆分中表现良好时，它们可能依赖于简单的启发式算法，而这些启发式算法不能在广泛的非i.i.d.场景中稳健地泛化(Gardneret al., 2020; Kaushiket al., 2019; McCoy et al.，2019)，模型过度依赖于刻板印象(Parrishet al., 2022; Srivastavaet al., 2021)，或者依靠记忆而不是泛化(Lewiset al., 2021; Razeghiet al., 2022)。相反，其他人讨论了当评估数据在类型、领域或主题方面与训练数据不同时(例如，Michel和Neubig，2018; Malininet al., 2021; Plank，2016)，或由不同的亚群体产生时(例如Blodgettet al., 2016; Dixonet al., 2018)，表现下降的情况。然而，其他人关注的是模型无法在结构上(Sinhaet al., 2021; Weberet al., 2021a; Weiet al., 2021)、更长序列(Duboiset al., 2020; Raunaket al., 2019)或同一问题的稍有不同的任务公式(Srivastavaet al., 2022)上进行组合泛化(Dankers等，2022; Kim和Linzen，2020; Lake和Baroni，2018; Li等，2021c)。

The examples above are just a few in a long list of studies that aim to investigate the generalisation abilities of NLP models, focusing in particular on models and training regimes that score well on traditional train–test splits. At the same time, these works differ amply in the assumptions they make about when and how models should generalise, and the evaluation settings they use to evaluate that.They encompass a wide range of generalisation-related research questions, and they use a wide range of different methodologies and experimental setups. Taken together, this body of work thus illustrates that there is no real agreement on what kind of generalisation is important for NLP models, and it also brings into question what kind of generalisation capabilities recent breakthroughs actually reflect. How should generalisation be tested for, if not with i.i.d. splits? How do we discover which types of generalisation should be prioritised, how the results of different studies relate to each other, what types of generalisation are already well addressed and which are neglected? Ultimately, on a more meta-level, how can we make progress on these important questions without a systematic way to discuss generalisation in NLP?

上面的例子只是一长串旨在研究NLP模型泛化能力研究中的一小部分，尤其关注那些在传统训练测试中得分很高的模型和训练机制。同时，这些工作在关于模型何时以及如何泛化的假设以及用于评估的评估设置方面存在很大差异。它们包含了广泛的泛化相关研究问题，并且使用了广泛的不同方法和实验设置。综上所述，这一系列工作表明，对于NLP模型来说，什么样的泛化是重要的，并没有达成真正的一致意见，这也让人产生了疑问，最近的突破实际上反映了什么样的泛化能力。如果没有i.i.d.分割，应该如何测试泛化？我们如何发现哪些类型的泛化应该优先考虑，不同研究的结果如何相互关联，哪些类型的归纳已经得到很好的解决，哪些被忽略？最终，在更根本的层面上，如果没有系统的方法来讨论NLP中的泛化，我们如何才能在这些重要问题上取得进展？

### 1.2 The generalisation taxonomy: a bird’s eye view  泛化分类法：鸟瞰图
It is exactly this meta-question that we aim to address with this paper, by proposing a framework that can be used to systematically characterise and understand generalisation research. More specifically, we present a generalisation taxonomy, an analysis of existing work on generalisation, and a set of online tools that can be used by researchers to explore and better understand generalisation studies in NLP. The generalisation taxonomy we propose is based on a detailed analysis of a large number of existing studies on generalisation in NLP, and it includes the main five axes along which those studies differ.1 The five axes capture different aspects of generalisation studies, that together form a comprehensive picture of the motivation and goal of the study and provide information on important choices in the experimental setup.

正是这一元问题，我们打算在本文中通过提出一个框架来解决，该框架可用于系统地描述和理解泛化研究。更具体地说，我们提出了一种泛化分类法，一种对现有泛化工作的分析，以及一套研究人员可以用来探索和更好地理解NLP中的泛化研究的在线工具。我们提出的泛化分类法是基于对NLP中大量现有泛化研究的详细分析，它包括这些研究不同的主要五个轴。1五个轴捕捉了泛化研究的不同方面，它们共同形成了研究动机和目标的全面图景，并提供了实验设置中重要选择的信息。

The first axis of our generalisation taxonomy (§2) is the high-level motivation for the study. The motivation of a study impacts or even determines what type of generalisation is desirable, as well as what kind of conclusions can be drawn from a model’s display or lack of generalisation. Furthermore, the motivation of a study shapes its experimental design. It is therefore important for researchers to be explicitly aware of it, to ensure that the experimental setup aligns with the questions they seek to answer. We consider four different types of motivations: the practical motivation, the cognitive motivation, the intrinsic motivation, and the fairness and inclusivity motivation.

我们泛化分类法的第一个轴(§2)是研究的高层次动机。研究的动机影响甚至决定了什么样的泛化是可取的，以及从模型的显示或缺乏泛化中可以得出什么样的结论。此外，研究的动机决定了其实验设计。因此，研究人员必须明确意识到这一点，以确保实验设置与他们寻求回答的问题一致。我们考虑四种不同类型的动机：实践动机、认知动机、内在动机和公平与包容动机。

Figure 1: A graphical representation of the NLP generalisation taxonomy we present in this paper. The taxonomy consists of five different (nominal) axes, that describe the high-level motivation of the work (§2); the type of generalisation the test is addressing (§3); what kind of data shift occurs between training and testing (§4), and what the source and locus of this shift are (§5 and §6, respectively). 
图1：我们在本文中提出的NLP泛化分类法的图形表示。分类法由五个不同的(名义)轴组成，描述了工作的高级动机(§2); 测试所涉及的泛化类型(§3); 训练和测试之间发生了什么样的数据转换(§4)，以及这种转换的来源和轨迹是什么(分别为§5和§6)。

The second axis in our taxonomy (§3) indicates the type of generalisation the test is addressing. This axis describes on a high level what exactly it is that a generalisation test is intended to capture, rather than considering why or how, making it one of the most important axes of our taxonomy. In the literature, we have found six main types of generalisation: compositional generalisation, structural generalisation, cross-task generalisation, cross-lingual generalisation, cross-domain generalisation, and robustness generalisation.

我们分类法中的第二个轴(§3)表明了测试所解决的泛化类型。这个轴在较高的层次上描述了泛化测试究竟要捕获什么，而不是考虑为什么或如何，使其成为我们分类法中最重要的轴之一。在文献中，我们发现了六种主要的泛化类型：成分泛化、结构泛化、跨任务泛化、跨语言泛化、跨领域泛化和稳健性泛化。

The third axis in our taxonomy (§4) describes what kind of data shift is considered in the generalisation test. This axis adds a statistical interpretation to our taxonomy and derives its importance from the fact that data shift plays an essential formal role in defining and understanding generalisation from a statistical perspective, as well as from the fact that different types of shifts are best addressed with different kinds of experimental setups. On the data shift axis, we consider three shifts which are well-attested in the literature: covariate shift, label shift and full shift. We further include two additional types of shift – assumed shift and multiple shifts – to account for studies that cannot be labelled with any of the three main shift types.

我们分类法中的第三个轴(§4)描述了一般化测试中考虑的数据偏移(data shift)类型。这一轴为我们的分类法增加了一种统计解释，其重要性来源于数据偏移在从统计角度定义和理解泛化方面发挥了重要的正式作用，以及不同类型的偏移最好用不同类型的实验设置来解决。在数据偏移轴上，我们考虑了文献中证明的三种移位：协变量偏移、标签偏移和完全偏移。我们还包括两种额外的偏移类型——假设偏移和多次偏移——以解释不能用三种主要偏移类型中的任何一种来标记的研究。

In the fourth axis of our taxonomy (§5), we consider what is the source of the data shift used in the experiment. The source of the data shift determines how much control the experimenter has over the training and testing data and, consequently, what kind of conclusions can be drawn from an experiment. We distinguish four different sources of shifts: naturally occurring shifts, artificially partitioned natural corpora, generated shifts and fully generated datasets.

在我们分类法的第四轴(§5)中，我们考虑了实验中使用的数据偏移源。数据偏移源决定了实验者对训练和测试数据的控制程度，从而决定了实验可以得出什么样的结论。我们区分了四种不同的偏移源：自然发生的偏移、人工分割的自然语料库、生成偏移和完全生成的数据集。

In the last axis of our taxonomy (§6), we consider what is the locus of the data shift, or, in other words, for what part of the modelling pipeline generalisation is investigated. The locus of the shift, together with the shift type, forms the last piece of the puzzle, as it determines what part of the modelling pipeline is investigated and thus the kind of generalisation question that can be asked. On this axis, we consider shifts between all stages in the contemporary modelling pipeline – pretraining, training and testing – as well as studies that consider shifts between multiple stages simultaneously.

在我们分类法的最后一个轴(§6)中，我们考虑了数据偏移轨迹是什么，或者换言之，研究了建模管道泛化的哪一部分。偏移轨迹，以及偏移类型，构成了最后一块谜题，因为它决定了建模管道的哪一部分被研究，从而决定了可以提出的泛化问题。在此轴上，我们考虑了当代建模流程中所有阶段之间的偏移 —— 预训练、训练和测试 —— 以及同时考虑多个阶段之间的偏移研究。

Figure 2: An overview figure of our literature review, including interactions. An interactive version of this plot can be found on our website https://genbench.github.io/visualisations. For more detailed explanations and analyses, we refer to §7. 
图2：我们的文献综述概览图，包括相互作用。我们的网站上可以找到该图的互动版本 https://genbench.github.io/visualisations . 关于更详细的解释和分析，我们参考§7。

### 1.3 Our review and analysis: a sneak-preview 我们的回顾和分析：预览
Using our taxonomy, we conduct an extensive literature review, in which we survey over 400 papers in the ACL anthology that contain the (sub)words generali(s|z)ation or generali(s|z)e in their title or abstract and that consider some form of data shift in their experiment. Using different visualisations, we analyse the most relevant trends and find several noteworthy patterns (§7.2.2).

使用我们的分类法，我们进行了广泛的文献综述，其中我们调查了ACL文集中的400多篇论文，这些论文的标题或摘要中包含了(子)词general(s|z)ation或generali(s|z)e，并在实验中考虑了某种形式的数据迁移。使用不同的可视化，我们分析了最相关的趋势，并发现了几个值得注意的模式(§7.2.2)。

First, we observe that the experimental design of a study is not always lined up with its motivation. To give an example, several studies considering compositional generalisation from a practical perspective use generated data not reflective of the scenarios that models might in practice be employed in, making it difficult to draw conclusions that match the proposed motivation of the study. As such, this demonstrates the importance of the motivation axis in designing generalisation studies. 
Then, we find that an increasing number of papers investigating generalisation does not explicitly consider the relationship between train and test data. This trend is likely due to the computational and engineering advances that allow model training on extremely large corpora: the ever-growing sizes of the training corpora, which are furthermore often not in the public domain, make it increasingly difficult to determine the relationship between train and test data, and consequently how generalisation should be evaluated in these scenarios. A similar issue arises in the setup where pretrained models are tested without further finetuning, such as in prompting or in-context learning setups. In such setups, there is a shift between pretraining and testing, which is – for the same reasons as laid out above – difficult to analyse. Our taxonomy provides the means to understand these problems, and it illustrates that they require further thought in the future to allow for generalisation testing in such increasingly popular setups. 
A third important observation is that many papers that contain a multi-stage modelling pipeline investigate generalisation in one part of that pipeline, but not in the other (as can be seen in Figure 2, by comparing the number of pretrain-train and finetune train–test loci with the number of multiple loci). For instance, a researcher might extensively evaluate whether a pretrained model can be finetuned on a large number of tasks, but use random splits to assess each individual task, or, conversely, they might test generalisation in the finetuning stage for a single task and draw conclusions about the pretrained model, without considering whether those results hold also when the model is finetuned on other tasks. Both these scenarios lead to models that generalise suboptimally when considered as a whole. Therefore, we argue that in the future it is important to prioritise models that generalise well at all levels of the modelling pipeline, and not just in one phase. Another takeaway is that our results suggest that more meta-studies might be needed that compare results across different values of the same axis, for example, to understand what is the relationship between results obtained with fully generated data and generated shifts. Such studies can improve our understanding of how different experimental design choices impact the conclusions that can be drawn from an experiment. Lastly, we find that both studies on cross-lingual generalisation and studies with a fairness motivation are under-represented in our review. In part, this may indicate that such studies refer less explicitly to generalisation in their title and abstract. However, we hypothesise that also prioritisation in the field plays a role. In particular, the fact that NLP is very English-centric (e.g. Bender, 2011; Cotterell et al., 2018) is likely to impact the number of cross-lingual studies. For fairness, on the other hand, under-representation could stem from the fact only relatively recently awareness of the potential harmfulness of models trained on large, uncontrolled corpora has started to grow. Either way, we believe that both cross-lingual generalisation and fairness are important matters to prioritise in the future. We also call to the reader to propose existing papers with these axis values via our website, so that we can increase our coverage.

首先，我们观察到研究的实验设计并不总是与其动机一致。举个例子，从实际角度考虑成分泛化的几项研究使用的数据并不反映模型在实践中可能使用的场景，因此很难得出与研究动机相匹配的结论。因此，这证明了动机轴在设计泛化研究中的重要性。
然后，我们发现越来越多的泛化研究论文没有明确考虑训练和测试数据之间的关系。这一趋势很可能是由于计算和工程的进步，使得可以在非常大的语料库上进行模型训练：训练语料库不断增长，而且通常不在公共领域，这使得确定训练和测试数据之间的关系变得越来越困难，因此在这些场景中应该如何评估泛化。类似的问题出现在测试预训练模型而不需要进一步微调的设置中，例如在提示或上下文学习设置中。在这样的设置中，预训练和测试之间存在转换，这是很难分析的，原因与上文所述相同。我们的分类法提供了理解这些问题的方法，它说明了未来需要进一步考虑这些问题，以便在越来越流行的设置中进行通用测试。
第三个重要的观察结果是，许多包含多阶段建模管道的论文研究了管道一部分的泛化，而不是另一部分(如图2所示，通过比较预训练序列和微调序列测试基因座的数量与多个基因座的数量)。例如，研究人员可能会广泛评估预训练模型是否可以对大量任务进行微调，但使用随机拆分来评估每个单独的任务，或者相反，他们可能会在微调阶段测试单个任务的泛化，并得出关于预训练模型的结论，而不考虑当模型在其他任务上进行微调时，这些结果是否也成立。这两种情况都会导致模型在整体考虑时出现次优化。
因此，我们认为，在未来，重要的是对模型进行优先排序，这些模型在建模管道的所有级别都能很好地推广，而不仅仅是在一个阶段。另一个要点是，我们的结果表明，可能需要更多的元研究来比较同一轴的不同值的结果，例如，以了解使用完全生成的数据获得的结果与生成的偏移之间的关系。这些研究可以提高我们对不同实验设计选择如何影响实验得出的结论的理解。最后，我们发现跨语言泛化研究和具有公平动机的研究在我们的综述中都没有得到充分的体现。部分地，这可能表明，这类研究在标题和摘要中较少明确提及泛化。然而，我们假设，该领域的优先顺序也起到了作用。特别是，NLP非常以英语为中心的事实(例如Bender，2011; Cotterellet al., 2018)可能会影响跨语言研究的数量。另一方面，为了公平起见，代表性不足可能是因为最近才开始意识到在大规模、不受控制的语料库上训练的模型的潜在危害。无论如何，我们认为跨语言的泛化和公平都是未来优先考虑的重要事项。我们还呼吁读者通过我们的网站提出具有这些轴值的现有论文，以便我们能够增加报道范围。

### 1.4 Outline and contributions 概述和贡献
We believe that generalisation testing should be the new status quo in NLP, and with this work, we aim to lay the groundwork for making that a reality. In summary, the contributions of our work are the following: 
1. We present an axis-based generalisation taxonomy that can be used to characterise generalisation studies in NLP; 
2. We review 449 papers, containing a total of 619 generalisation experiments, using this taxonomy; 
3. With these survey results, we discuss the status of generalisation research in NLP, and we provide suggestions to steer the field towards more sound and exhaustive generalisation tests. 
4. We present a website where our review results can be (visualised and textually) explored and (new) generalisation studies can be incorporated.

我们认为，泛化测试应该是NLP的新现状，通过这项工作，我们旨在为实现这一目标奠定基础。总之，我们的工作贡献如下：
1. 我们提出了一种基于轴的泛化分类法，可用于表征NLP中的泛化研究; 
2. 我们使用该分类法审查了449篇论文，共包含619个泛化实验; 
3. 根据这些调查结果，我们讨论了NLP中泛化研究的现状，并提出了建议，以引导该领域进行更合理和详尽的泛化测试。
4. 我们提供了一个网站，可以(可视化和文本化)探索我们的审查结果，并可以纳入(新的)泛化研究。

In the remainder of this paper, we will first discuss the different axes of our taxonomy in more detail (§2-6). After that, in §7, we will present our review and analysis of the current state of generalisation research. In §8, we wrap up by summarising our main findings and making concrete recommendations for the future. 

在本文的其余部分中，我们将首先更详细地讨论分类法的不同轴(§2-6)。之后，在第7节中，我们将对泛化研究的现状进行回顾和分析。在第8节中，我们总结了我们的主要发现，并对未来提出了具体建议。

## 2 Motivation: what is the high-level motivation for a generalisation test? 动机：泛化测试的高级动机是什么？
Now that we have outlined our main objectives, we discuss the five axes in our proposed taxonomy. The first axis we consider is the high-level motivation of a generalisation study. We identified four closely intertwined goals of generalisation research in NLP, which we refer to as the practical, the cognitive, the intrinsic, and the fairness motivation. The motivation of a study impacts or even determines what type of generalisation is desirable, as well as what kind of conclusions can be drawn from a model’s display or lack of generalisation. Consider, for instance, cases in which humans fail to generalise. For a study with a cognitive motivation, model failures in such cases might not be problematic, or perhaps even desirable. This is unlikely to be the case for studies with a fairness or practical motivation, where propagation of human biases is usually problematic. Connected to this, the motivation of a study shapes the decisions that need to be made for its experimental design. It is therefore important for researchers to be explicitly aware of it, to ensure that the experimental setup aligns with the questions they seek to answer. For a study with a practical motivation, for example, it is typically important to consider a data setup that matches real-world scenarios a model might occur in; this is less relevant for studies considering generalisation with a cognitive or intrinsic motivation. Given its strong influence on the other axes of the taxonomy, a study’s high-level motivation is the first axis we discuss. We describe the four motivations we distinguish below.(As we will see in what follows, the same questions can often be asked with different underlying motivations. This makes it sometimes difficult to identify what exactly the motivation of a generalisation study is. Often, studies may inform conclusions along all four dimensions. However, given the importance of the motivation for the implications and design of the study, we nevertheless try to identify the main guiding motive of a study in our review in §7, and we encourage researchers to be explicit about the motivation of their future studies. )

既然我们已经概述了我们的主要目标，我们将讨论我们建议的分类法中的五个轴。我们考虑的第一个轴是泛化研究的高层次动机。我们确定了NLP中四个紧密交织的泛化研究目标，我们称之为实践动机、认知动机、内在动机和公平动机。研究的动机影响甚至决定了什么样的泛化是可取的，以及从模型的显示或缺乏泛化中可以得出什么样的结论。例如，考虑一下人类无法泛化的情况。对于一项具有认知动机的研究来说，这种情况下的模型失败可能没有问题，甚至可能是可取的。这不太可能是出于公平或实际动机的研究，因为人类偏见的传播通常是有问题的。与此相关，研究的动机决定了实验设计需要做出的决定。因此，研究人员必须明确意识到这一点，以确保实验设置与他们寻求回答的问题一致。例如，对于具有实际动机的研究，考虑与模型可能发生的真实世界场景相匹配的数据设置通常很重要; 这与考虑具有认知或内在动机的泛化的研究不太相关。鉴于它对分类学的其他轴的强大影响，研究的高层次动机是我们讨论的第一个轴。我们在下面描述了我们区分的四种动机。(正如我们将在下文中看到的，同样的问题通常可以用不同的潜在动机提出。这使得有时很难确定泛化研究的确切动机是什么。通常，研究可能会在所有四个方面得出结论。然而，鉴于动机对研究的含义和设计的重要性，我们仍试图在第7节的综述中确定研究的主要指导动机，并鼓励研究人员明确其未来研究的动机。)

Practical: in what settings can the model be used or improved? One frequently posed motivation to study generalisation is of a highly practical nature. Studies that consider generalisation from a practical perspective seek to assess in what kind of scenarios a model can be used, or focus on improving model generalisation. One question that is often addressed with a primarily practical motivation is how well models generalise to different domains or differently collected data. For instance, Michel and Neubig (2018) consider how well machine translation models trained on canonical text can generalise to noisy data from an internet platform, and Lazaridou et al. (2021) investigate language model generalisation to different time periods. Other questions that are frequently addressed from a practical perspective concern biases in the training data, and whether models robustly generalise to datasets that do not share these (spurious) biases (e.g. Behnke et al., 2022; Zhou et al., 2021c).

实用：模型可以在哪些设置中使用或改进？ 研究泛化的一个常见动机是高度实用的。从实际角度考虑泛化的研究试图评估在何种情况下可以使用模型，或专注于改进模型泛化。一个通常以实际动机来解决的问题是，模型如何很好地推广到不同的领域或不同收集的数据。例如，Michel和Neubig(2018)考虑了在规范文本上训练的机器翻译模型如何能够很好地泛化到来自互联网平台的嘈杂数据，Lazaridouet al (2021)研究了不同时间段的语言模型泛化。从实际角度经常解决的其他问题涉及训练数据中的偏差，以及模型是否可靠地推广到不共享这些(虚假)偏差的数据集(例如Behnkeet al., 2022; Zhouet al., 2021c)。

Cognitive: does the model generalise like a human? A second high-level motivation that drives generalisation research is cognitively oriented and can be separated into two underlying categories. The first category is related to model behaviour: human generalisation is a useful reference point for the evaluation of model generalisation in NLP, because human generalisation is known to be powerful (e.g.Lake et al., 2017; Marcus, 2003) and, perhaps more importantly, precisely the type of generalisation that is required to successfully model natural language. Humans learn quickly, from fewer data than models, and they easily (compositionally) recombine concepts they already know to understand concepts they have never before encountered (Fodor and Pylyshyn, 1988; Linzen, 2020; Marcus, 2018). These feats are arguably also important for models; they, therefore, provide a good point of reference for generalisation testing.(We do not always expect from a model the same type or level of generalisation a human exhibits. There are cases in which it is desirable for models to generalise better than humans, for example across languages – something humans above a certain age typically do not excel at. In other cases, models already generalise better than humans – consider, for instance, a language identification system – and would hardly be useful if they did not.) In some cases, it might be difficult to distinguish cognitive and practical motivations: assuming human generalisation is strong, a model that generalises like a human should score well also on practically motivated tests. In our axes-based taxonomy, the difference between cognitive and practical resides mostly in the types of scenarios that are considered in tests: are the scenarios artificially created to get a higher-level, isolated impression of how their behaviour compares to human-like generalisation, or are the scenarios realistic and practically relevant?

认知：模型是否像人类一样泛化？推动泛化研究的第二个高层次动机是认知导向的，可以分为两个基本类别。第一类与模型行为有关：人类泛化是NLP中模型泛化评估的有用参考点，因为人类泛化众所周知是强大的(例如Lakeet al., 2017; Marcus，2003)，也许更重要的是，成功建模自然语言所需的泛化类型。人类学习速度快，从比模型更少的数据中学习，他们很容易(从成分上)重新组合他们已经知道的概念，以理解他们从未遇到过的概念(Fodor和Pylyshyn，1988; Linzen，2020; Marcus，2018)。这些壮举可以说对模特来说也很重要; 因此，它们为泛化测试提供了很好的参考点。(我们并不总是期望从模型中获得与人类表现出的相同类型或水平的泛化。在某些情况下，模型比人类更好地泛化是可取的，例如跨语言的泛化——这是特定年龄以上的人类通常不擅长的。在其他情况下，已经比人类更好泛化的模型——例如，考虑一个语言识别系统em–如果不这样做，就很难发挥作用。)在某些情况下，可能很难区分认知动机和实际动机：假设人类的泛化能力很强，那么一个像人类一样泛化的模型在实际动机测试中也应该得分很高。在我们基于轴的分类法中，认知和实际之间的区别主要在于测试中考虑的场景类型：这些场景是人为创建的，以获得更高层次的、孤立的印象，了解他们的行为与人类的一般化相比如何，还是场景是现实的和实际相关的？

The second, more deeply cognitively inspired category contains work that evaluates generalisation in models to learn more about cognition and language (e.g. Baroni, 2021; Hupkes, 2020; Marcus, 1999; McClelland and Plaut, 1999). Studies in this category investigate whether a particular model generalises primarily to derive new hypotheses about how human generalisation might work. For instance, Lakretz et al. (2021b) perform a detailed study of how LSTM models generalise to specific kinds of nested syntactic constructions, which they then use to inform a human experiment on the same syntactic constructions. 

第二个更深入的认知启发类别包括评估模型中的泛化以了解更多认知和语言的工作(例如，巴罗尼，2021; 哈普克斯，2020; 马库斯，1999; 麦克莱兰和普劳特，1999)。这一类别的研究调查了一个特定的模型是否主要是为了得出关于人类泛化可能如何工作的新假设。例如，Lakretzet al (2021b)对LSTM模型如何泛化为特定类型的嵌套句法结构进行了详细研究，然后他们使用这些嵌套句法结构为人类对相同句法结构的实验提供信息。

Intrinsic: does the model capture the task correctly? A third motivation to evaluate generalisation in NLP models, which cuts through the two previous motivations, appertains to the question “did a model learn the task we intended it to learn, as we intended it to learn it?". The assumption underpinning this type of research as a whole is that if a model has truly learned the task it is trained to do, it should be able to execute this task also in settings that differ from the exact training scenarios. What changes across studies is the set of conditions under which a model is considered to have appropriately learned a task. For instance, researchers studying compositional generalisation (see §3.1) assume that a correct understanding of language implies that the assumed compositional structure of language is captured. Under that assumption, a model should not have trouble generalising to new inputs that are generated using the same compositional system. Others instead assume that true language understanding implies being able to use language across a wide variety of tasks (see §3.3). Yet others argue that if a model truly captures the relationship between two sentences in NLI tasks (e.g. Bowman et al., 2015a; Marelli et al., 2014; Williams et al., 2018), it should be able to do so across different data sets, even if those were sampled in a slightly different way (e.g. Talman and Chatzikyriakidis, 2019). In studies that consider generalisation from this perspective, generalisation failures are taken as proof that the model – in fact – did not learn the task as we intended it to learn it (but instead showed behaviour that made us think it did, for instance by relying on spurious patterns or non-generalisable heuristics). Furthermore, studies with an intrinsic motivation are usually guided by the purely scientific motive of increasing knowledge and understanding, rather than targeting a specific goal.

内在：模型是否正确地捕捉了任务？第三个动机是评估NLP模型中的泛化，关于“一个模型学习了我们想要学习的任务，就像我们想要它学习的一样吗？。作为一个整体，支持这类研究的假设是，如果一个模型真正学会了它被训练完成的任务，它应该能够在不同于精确训练场景的环境中执行这项任务。研究中的变化是模型被认为适当地学会了一项任务的一组条件位置泛化(见§3.1)假定正确理解语言意味着所假定的语言组成结构被捕获。在这种假设下，一个模型应该不会在推广到使用相同组成系统生成的新输入时遇到困难。另一些人则认为，真正的语言理解意味着能够在各种任务中使用语言(见§3.3)。还有人认为，如果一个模型真正捕捉到了NLI任务中两个句子之间的关系(例如Bowmanet al., 2015a; Marelliet al., 2014; Williamset al., 2018)，那么它应该能够跨不同的数据集这样做，即使这些数据集的采样方式略有不同(例如Talman和Chatzikyriakidis，2019)。在从这个角度考虑泛化的研究中，泛化失败被视为模型——事实上——并没有按照我们想要的那样学习任务(而是表现出了让我们认为是这样的行为，例如依靠虚假模式或不可泛化的试探法)的证据。此外，具有内在动机的研究通常以增加知识和理解的纯粹科学动机为指导，而不是以特定目标为目标。

Fairness and inclusivity: does the model generalise in a fair and responsible way? A last yet very important motivation for generalisation research is the desire to have models that are fair, responsible and unbiased. One category of studies driven by these concepts, often ethical in nature, asks questions about how well models generalise to diverse demographics, typically considering minority or marginalised groups (e.g. Bender et al., 2021; Blodgett et al., 2016; Koh et al., 2021), or investigates to what extent models perpetuate (undesirable) biases learned from their training data (e.g. Hutchinson et al., 2020; Dixon et al., 2018; Sheng et al., 2019). Another line of research related to both fairness and inclusivity focuses on efficiency, both in terms of the amount of data that is required for a model to converge to a solution as well as the necessary amount of compute. In such studies, efficiency is seen as a correlate of generalisation: models that generalise well should learn more quickly and require fewer data (see, e.g. Marcus, 2018). The relationship of efficiency with fairness, inclusivity and responsibility stems from the idea that models that generalise well from small amounts of data are more inclusively applicable – for instance for low-resource languages or demographic groups for which little data is available. Furthermore, models that require less compute are more accessible for groups with smaller computational resources and have a lower environmental impact (see, e.g. Strubell et al., 2019). While we have not mentioned them before in the respective categories, studies on learning efficiency can, naturally, also be motivated by practical concerns, as well as by cognitive interests (e.g. comparing human’s and model’s sample efficiency). 

公平和包容：该模型是否以公平和负责任的方式泛化？ 推广研究的最后一个非常重要的动机是希望有公平、负责和公正的模型。由这些概念驱动的一类研究，通常是伦理性质的，询问模型在多大程度上适用于不同的人口统计，通常考虑少数群体或边缘化群体(例如Benderet al., 2021; Blodgettet al., 2016; Kohet al., 2021)，或调查模型在何种程度上延续了从其训练数据中获得的(不希望的)偏见(例如。Hutchinsonet al., 2020; Dixonet al., 2018;Shenget al., 2019)。另一条与公平性和包容性相关的研究线侧重于效率，即模型收敛到解决方案所需的数据量，以及必要的计算量。在这类研究中，效率被视为泛化的相关性：泛化良好的模型应该学习得更快，所需数据更少(参见，例如Marcus，2018)。效率与公平、包容性和责任之间的关系源于这样一种观点，即从少量数据中很好地泛化的模型更适用于包容性，例如，对于低资源语言或数据很少的人口群体。此外，需要较少计算的模型对于计算资源较小且环境影响较小的群体更容易访问(参见，例如Strubellet al., 2019)。虽然我们之前没有在各自的类别中提到过它们，但关于学习效率的研究自然也会受到实际问题以及认知兴趣的激励(例如，比较人类和模型的样本效率)。

## 3 Generalisation type: what type of generalisation is a test addressing? 泛化类型：什么类型的泛化是测试寻址？
A second important dimension when it comes to characterising generalisation research is what type of generalisation a test aims to evaluate. The second axis in our taxonomy describes, on a high level, what it is that a generalisation test intends to capture – rather than considering why or how – making it one of the most important axes of our taxonomy. We identify and describe six types of generalisation that are frequently considered in the literature. Some types are rooted in knowledge about human generalisation, such as those that target compositional (§3.1) or structural generalisation (§3.2). Others, instead, are motivated by more practical concerns, such as generalisation across tasks (§3.3), languages (§3.4) and domains (§3.5), or by an interest in analysing how robustly models generalise (§3.6). An overview of the types we consider is presented in Figure 3.

在描述泛化研究时，第二个重要方面是测试旨在评估什么类型的泛化。我们分类学中的第二个轴在高层次上描述了泛化测试要捕获的内容，而不是考虑为什么或如何捕获，使其成为我们分类法中最重要的轴之一。我们确定并描述了文献中经常考虑的六种泛化类型。有些类型植根于关于人类泛化的知识，例如那些以成分(§3.1)或结构泛化(§3.2)为目标的类型。相反，其他人则是出于更实际的考虑，例如跨任务(§3.3)、语言(§3.4)和领域(§3.5)的泛化，或是出于对分析稳健模型如何泛化的兴趣(§3.6)。我们考虑的类型概述如图3所示。

Figure 3: An infographic that illustrates the six different types of generalisation that we consider in our taxonomy, which are explained in more detail in §3.1-§3.6. 
图3：一个信息图，说明了我们在分类法中考虑的六种不同类型的泛化，在§3.1-§3.6中有更详细的解释。

### 3.1 Compositional generalisation 成分泛化
The first prominent type of generalisation that can be found in the literature is compositional generalisation, which is often argued to underpin human’s ability to quickly generalise to new data, tasks and domains (Fodor and Pylyshyn, 1988; Lake et al., 2017; Marcus, 2018; Schmidhuber, 1990). Because of this strong connection with humans and human language, work on compositional generalisation often has a primarily cognitive motivation, although practical concerns such as sample efficiency, quick adaptation and good generalisation in low-resource scenarios are frequently mentioned as additional or alternative motivations (Chaabouni et al., 2021; Linzen, 2020, to give just a few examples). While it has a strong intuitive appeal and clear mathematical definition (Montague, 1970), compositional generalisation is not easy to pin down empirically. Here, we follow Schmidhuber (1990) in defining compositionality as the ability to systematically recombine previously learned elements to map new inputs made up from these elements to their correct output(For an elaborate account of the different arguments that come into play when defining and evaluating compositionality for a neural network, we refer to Hupkes et al. (2020)). In language, the inputs are ‘forms’ (e.g. phrases, sentences, larger pieces of discourse), and the output that they need to be mapped to is their meaning or interpretation. Because of the need for both an input and output space, compositional generalisation is usually evaluated in tasks such as sequence classification (e.g. Bowman et al., 2015; Hupkes et al., 2018; Veldhoen et al., 2016), machine translation (e.g. Dankers et al., 2022; Liu et al., 2021b; Raunak et al., 2019), semantic parsing (e.g. Finegan-Dollak et al., 2018; Keysers et al., 2019; Kim and Linzen, 2020; Shaw et al., 2021) or other kinds of generation tasks (e.g. Hupkes et al., 2020; Lake and Baroni, 2018). In such tasks, the in- and output spaces are clearly distinct, and outputs can straightforwardly be viewed as an interpretation or (proxy) of meaning of its corresponding input. As far as we know, there have not yet been many explicit systematic attempts to evaluate compositionality in (ungrounded) language models (There are, however, several studies that focus on structural generalisation in such models. Contrary to compositional generalisation, structural generalisation does not focus on the ability of models to correctly interpret new inputs, or assign meanings to them, but only on whether they can generalise to their correct form. We will discuss structural generalisation in the next subsection. ). If and how compositionality can be adequately evaluated in such models, where the input and output (form and meaning) are conflated in one space (the space defined by the language vocabulary), is a question that is yet to be answered (An interesting example to consider in this context is the qualitative study conducted by Brown et al. (2020) to test if GPT-3 can use novel words correctly in a sentence; as another example, a bit further away from traditional forms of compositionality, Talmor et al. (2020) finetune pretrained masked language models on multi-hop composition in question answering.).

文献中可以发现的第一种突出的泛化类型是成分泛化，这通常被认为是人类快速泛化新数据、任务和领域的能力基础(Fodor和Pylyshyn，1988; Lakeet al., 2017; Marcus，2018; Schmidhuber，1990)。由于这种与人类和人类语言的强烈联系，关于成分泛化的工作通常具有主要的认知动机，低资源场景中的快速适应和良好泛化经常被视为额外或替代动机(Chaabouniet al., 2021; Linzen，2020，仅举几个例子)。尽管它具有强烈的直觉吸引力和清晰的数学定义(Montague，1970)，但从经验上来说，成分泛化并不容易确定。在这里，我们遵循Schmidhuber(1990)的定义，将合成性定义为系统地重新组合先前学习的元素以将这些元素组成的新输入映射到其正确输出的能力(关于定义和评估神经网络的合成性时起作用的不同论点的详细说明，我们参考Hupkeset al (2020))。在语言中，输入是“形式”(例如短语、句子、较大的话语片段)，需要映射到的输出是它们的含义或解释。由于需要输入和输出空间，通常在序列分类(例如Bowmanet al., 2015; Hupkeset al., 2018; Veldhoenet al., 2016)、机器翻译(例如Dankerset al., 2022; Liuet al., 2021b; Raunaket al., 2019)、，语义分析(例如Finegan Dollaket al., 2018; Keyserset al., 2019; Kim和Linzen，2020; Shawet al., 2021)或其他类型的生成任务(例如Hupkeset al., 2020; Lake和Baroni，2018)。在这样的任务中，输入和输出空间是明显不同的，输出可以直接被视为其相应输入的意义的解释或(智能体)。据我们所知，尚未有许多明确的系统性尝试来评估(未接地)语言模型中的合成性(然而，有几项研究侧重于此类模型中的结构泛化。与成分泛化相反，结构泛化并不关注模型正确解释新输入或赋予其含义的能力，而只关注它们是否能够泛化为正确的形式。我们将在下一小节中讨论结构泛化。)。如果以及如何在输入和输出(形式和意义)在一个空间(由语言词汇定义的空间)中混合的这种模型中充分评估合成性，是一个有待回答的问题(在这种情况下需要考虑的一个有趣的例子是Brownet al (2020)进行的定性研究，以测试GPT-3是否能够在句子中正确使用新词; 作为另一个例子，Talmoret al (2020)在问答中对多跳合成的预训练掩码语言模型进行微调，这与传统的合成形式稍有不同。

## 3.2 Structural generalisation 结构泛化
Another category of usually cognitively inspired generalisation instead focuses on the extent to which models can produce or generate structurally (grammatically) correct forms, rather than on whether they can assign them correct interpretations. Unlike compositional generalisation, structural generalisation does not require an output space (the meaning or interpretation space; see §3.1). This makes it more straightforwardly evaluated in form-only models (i.e. language models) and completely natural setups (i.e. with no need for simplified synthetic input and output spaces). We distinguish two broad categories of structural generalisation: syntactic generalisation, and morphological generalisation.

另一类通常是受认知启发的泛化则侧重于模型在多大程度上能够产生或生成结构上(语法上)正确的形式，而不是它们是否能够赋予它们正确的解释。与成分泛化不同，结构泛化不需要输出空间(含义或解释空间; 见§3.1)。这使得它在仅形式模型(即语言模型)和完全自然的设置(即不需要简化的合成输入和输出空间)中更直接地进行评估。我们区分了两大类结构泛化：句法泛化和形态泛化。

Syntactic generalisation. Some structural generalisation studies focus specifically on syntactic generalisation. They consider whether models can generalise to novel syntactic structures or novel elements in known syntactic structures. For instance, Jumelet et al. (2021) and Weber et al. (2021a) filter out from the training data specific licensing environments for negative polarity items and then test whether models nevertheless learn to generalise to such environments. It is unfortunately difficult to conduct this type of study, which involves several different training corpora, using very large language models. On the one hand, their high training cost makes the necessary experiments computationally extremely expensive. On the other hand, generating specific test splits given knowledge of what is in the training data is often also not possible for such models, because their training data is not in the open domain. These limitations prevent researchers from controlling the relationship between the evaluation and training data, and they make it hard to assess to what extent the incidental examples reported for the large language models (most notably, in their respective release papers) are reflective of successful generalisation and, if so, what that entails. Interesting exceptions are a few studies that do explicitly consider shifts between training and testing in the context of syntactic generalisation, such as those presented by Wei et al. (2021), Razeghi et al. (2022), and Elazar et al. (2022). Wei et al. (2021), in particular, investigate how the performance of pretrained language models in tests that assess syntactic rule learning is affected by a term’s training data frequency, by varying those frequencies in the training corpus. Razeghi et al. (2022), instead, focus on a larger model trained on more data, and while they do not systematically vary the training corpus, they do an elaborate analysis of how test performance in their trained models (GPT-J and GPT-Neo) is affected by absolute and relative frequencies of specific terms in the model’s training data. Even more recently, Elazar et al. (2022) studies the causal effect of simple statistics from the training data, such as co-occurrences, on models’ prediction.

句法泛化. 一些结构泛化研究特别关注句法泛化。他们考虑模型是否可以推广到新的句法结构或已知句法结构中的新元素。例如，Jumelet et al.(2021)和Weber et al.(2021a)从训练数据中筛选出负极性项目的特定许可环境，然后测试模型是否仍能学习推广到此类环境。不幸的是，很难使用非常大的语言模型进行这种类型的研究，这涉及到几个不同的训练语料库。一方面，他们高昂的训练成本使得必要的实验在计算上极其昂贵。另一方面，在已知训练数据中的内容的情况下，生成特定的测试拆分对于此类模型通常也是不可能的，因为它们的训练数据不在开放域中。这些限制使研究人员无法控制评估和训练数据之间的关系，并且很难评估大型语言模型(最值得注意的是，在他们各自的发布论文中)报告的附带样本在多大程度上反映了成功的泛化，如果是的话，这意味着什么。有趣的例外是，一些研究确实明确考虑了句法泛化背景下训练和测试之间的转换，例如Weiet al (2021)、Razeghiet al (2022年)和Elazaret al (2023年)提出的研究。Weiet al (2021)特别研究了预训练语言模型在评估句法规则学习的测试中的表现如何受到术语训练数据频率的影响，通过改变训练语料库中的这些频率。相反，Razeghiet al (2022)专注于在更多数据上训练的更大模型，尽管他们没有系统地改变训练语料库，但他们对训练模型(GPT-J和GPT-Neo)中的测试性能如何受到模型训练数据中特定项的绝对和相对频率的影响进行了详细分析。甚至在最近，Elazaret al (2022)研究了来自训练数据的简单统计数据对模型预测的因果效应，如共现现象。

Note that the vast majority of other studies focusing on the syntactic abilities of language models (e.g. Giulianelli et al., 2018; Jumelet and Hupkes, 2018; Linzen et al., 2016; Warstadt et al., 2019, 2020) focus on whether and how models recognise, represent, and process syntactic information, or they try to discern the causal mechanisms by which models use such abilities (Amini et al., 2022; Elazar et al., 2021a; Feder et al., 2021). These works do not (explicitly) consider the relationship between the data they test on and the data that a model was trained on, and as such they do not specifically study the models’ generalisation abilities across syntactic structures. We will not further discuss these studies, but in our map of generalisation literature (§7), we will include a few papers in which there is an implicit yet clear assumption that the test data substantially differs from the training data, for instance because it includes sentences created with semantically nonsensical words (Gulordava et al., 2018), or unusually deep levels of recursion (Lakretz et al., 2021a,b) that are not likely to naturally occur in corpora.

请注意，绝大多数其他关注语言模型句法能力的研究(例如Giulianelliet al., 2018; Jumelet和Hupkes，2018; Linzenet al., 2016; Warstadtet al., 20192020)关注模型是否以及如何识别、表示和处理句法信息，或者他们试图识别模型使用这些能力的因果机制(Aminiet al., 2022; Elazaret al., 2021a; Federet al., 2021)。这些工作没有(明确地)考虑他们测试的数据和模型训练的数据之间的关系，因此他们没有专门研究模型跨句法结构的泛化能力。我们不会进一步讨论这些研究，但在我们的泛化文献地图(§7)中，我们将包括一些论文，其中有一个隐含但明确的假设，即测试数据与训练数据有很大不同，例如，因为它包括用语义无义词创建的句子(Gulordavaet al., 2018)，或不太可能在语料库中自然发生的异常深层循环(Lakretzet al., 2021a，b)。

Morphological generalisation. A second category of structural generalisation studies focuses on morphological inflexion, a popular testing ground for questions about human generalisation. Papers focusing on morphological inflexion (e.g. Corkery et al., 2019; Dankers et al., 2021; Kirov and Cotterell, 2018; Liu and Hulden, 2022; Malouf, 2017; McCurdy et al., 2020) are typically rooted in strong cognitive motivations. While most of this work considers i.i.d. train–test splits, recent studies have focused on how morphological transducer models generalise across languages (e.g. McCarthy et al., 2019; Pimentel et al., 2021; Vylomova et al., 2020) as well within each language (Calderone et al., 2021; Liu and Hulden, 2022; Pimentel et al., 2021b; Szolnok et al., 2021; Wilson and Li, 2021; Li and Wilson, 2021). In doing so, they often take inspiration from wug tests, which are used in psycholinguistics to probe morphological generalisation to novel words in humans (Berko, 1958; Marcus et al., 1995). In principle, such studies could also be conducted with large language models but the lack of access to their training data is, again, a complication for determining whether the supposedly novel words were truly never seen by the models.

形态学泛化。第二类结构泛化研究侧重于形态拐点，这是人类泛化问题的流行测试场。关注形态转折的论文(例如，Corkeryet al., 2019;Dankerset al., 2021; Kirov和Cotterell，2018;Liu和Hulden，2022;Malouf，2017;McCurdyet al., 2020年)通常植根于强烈的认知动机。虽然这项工作大多考虑了i.i.d.训练-测试分割，但最近的研究重点是形态感知模型如何跨语言泛化(例如McCarthyet al., 2019; Pimentelet al., 2021; Vylomovaet al., 2020)以及在每种语言中(Calderoneet al., 2021; Liu和Hulden，2022; Pimentellet al., 2021b; Szolnoket al., 2021; Wilson和Li，2021; Li和Wilson，2021)。在这样做的过程中，他们经常从wug测试中获得灵感，该测试在心理语言学中用于探索人类对新词的形态泛化(Berko，1958; Marcuset al., 1995)。原则上，这类研究也可以用大型语言模型进行，但由于缺乏对其训练数据的访问，再次导致了确定模型是否真的从未见过这些所谓的新词的复杂性。

### 3.3 Generalisation across tasks 跨任务泛化
A third and completely different direction of generalisation research considers the ability of a single model to adapt to multiple NLP problems. We refer to this ability as generalisation across tasks, or cross-task generalisation. Along with the great advancements in NLP models, in the past ten years, the nature of cross-task generalisation tests has quite substantially changed; we discuss this evolution in the present section.

泛化研究的第三个完全不同的方向是考虑单个模型适应多个NLP问题的能力。我们将这种能力称为跨任务泛化。随着NLP模型的巨大进步，在过去十年中，跨任务泛化测试的性质发生了很大的变化; 我们将在本节讨论这种演变。

Multitask learning. Cross-task generalisation in NLP has been traditionally strongly connected to transfer and multitask learning (Collobert and Weston, 2008). In multitask learning, a model is either trained on a set of tasks and evaluated on those same tasks, or pretrained on some tasks and then adapted to others. As this setup favours approaches that benefit from positive transfer across tasks, it implicitly studies forms of cross-task generalisation(Notably, as illustrated by the work of Weber et al. (2021a), the definition of task can be taken liberally in this context, ranging from traditional notions of NLP tasks to considering subproblems of a single classic NLP task . For instance, while language modelling constitutes its own task, learning how to handle negative polarity items such as any or ever in a grammatically correct way can be considered a subtask of it.). Examples of benchmarks that were originally meant to address this kind of cross-task transfer – although they are not used as such any longer – are multitask benchmarks such DecaNLP (McCann et al., 2018), GLUE (Wang et al., 2018) and its successor SuperGLUE (Wang et al., 2019a). In recent times, a common approach has been to formulate all tasks as sequence-to-sequence problems, a direction explored in the DecaNLP benchmark (McCann et al., 2018), as well as in modelling, by T5 (Raffel et al., 2020), exT5 (Aribandi et al., 2022) and UnifiedSKG (Xie et al., 2022), among others.

多任务学习。NLP中的跨任务泛化传统上与迁移和多任务学习密切相关(Collobert和Weston，2008)。在多任务学习中，一个模型要么在一组任务上进行训练，然后在这些相同的任务上进行评估，要么在一些任务上进行预训练，然后适应其他任务。由于这种设置倾向于从跨任务的正向迁移中受益的方法，它隐含地研究了跨任务泛化的形式(值得注意的是，如韦伯et al 的工作所示。(2021a)，任务的定义可以在这种背景下自由地进行，从传统的NLP任务概念到考虑单个经典NLP任务的子问题。例如，虽然语言建模是自己的任务，但学习如何以语法正确的方式处理负极性项目(如任何或任何时候)可以被视为它的一个子任务。最初旨在解决这种跨任务迁移的基准测试的例子 —— 尽管它们不再如此使用 —— 是多任务基准测试，如DecNLP(McCannet al., 2018)、GLUE(Wanget al., 2018年)及其继任者SuperGLUE。近年来，一种常见的方法是将所有任务制定为序列到序列问题，这是DecNLP基准(McCann et al.，2018)以及建模中探索的一个方向，由T5(Raffel et al，2020)、exT5(Aribandi et al，2022)和UnifiedSKG(Xie et al.)et al 提出。

The pretrain-finetune paradigm. In the context of multitask learning, cross-task generalisation was deemed an extremely challenging topic. This has changed with the relatively recent trend in which models are first pretrained with a general-purpose objective (language modelling, or masked language modelling) on large natural language corpora. The model is then further finetuned in a second stage, in which task-specific parameters are added that learn to execute different tasks using the representations learned in the pretraining stage. The popularisation of this pretrain-finetune paradigm has shifted thoughts on how to evaluate cross-task generalisation. Rather than evaluating how learning one task can benefit another, this paradigm instead gives a central role to the question of how well a model that has acquired some general knowledge about language during pretraining can be used to generalise to different kinds of tasks in a finetuning stage which involves task-specific parameters (e.g. Devlin et al., 2019; Howard and Ruder, 2018; Peters et al., 2018; Liu et al., 2019c). Interestingly, in the finetuning stage, performance on the tasks themselves is typically evaluated with random train–test splits, and thus generalisation within individual tasks is not necessarily considered.

预训练+微调范例。在多任务学习的背景下，跨任务泛化被认为是一个极具挑战性的话题。这一点随着最近的趋势发生了变化，即首先在大型自然语言语料库上以通用目标(语言建模或掩码语言建模)对模型进行预训练。然后在第二阶段进一步微调模型，其中添加任务特定参数，这些参数学习使用在预训练阶段学习的表示来执行不同的任务。这种预训练+微调范式的普及改变了人们对如何评估跨任务泛化的思考。这一范式不是评估学习一项任务如何使另一项任务受益，而是将一个核心问题放在了预训练过程中获得了一些关于语言的一般知识的模型在涉及任务特定参数(例如。Devlinet al., 2019;霍华德和鲁德，2018;Peterset al., 2018;Liuet al., 2019c)。有趣的是，在微调阶段，任务本身的性能通常是通过随机的训练-测试拆分来评估的，因此不必考虑单个任务中的泛化。

Zero-shot and few-shot learning. The focus of cross-task generalisation studies has more recently shifted even further, to scenarios which consider how well pretrained language models fare in different tasks without any task-specific parameters(If the pretraining corpus is seen as a large collection of different uncontrolled tasks, this scenario is more similar to the original multitask learning scenario than the pretrain-finetune paradigm. ). In the most extreme case, this implies evaluating a language model directly on a range of tasks without any further training. To do so, tasks are reformulated as textcompletion problems, such that language models can be prompted directly with a question representing a specific task (zero-shot learning), potentially preceded by a few examples (few-shot learning) (Radford et al., 2019). The latter case, in which the intention is that models – without any parameter updates – ‘learn’ from the examples given in the context, is often referred to with the term in-context learning. Datasets for conducting tasks via prompting are typically created by adapting conventional multitask datasets, where prompting templates are (often manually) designed for each task (e.g. Mishra et al., 2022; Wang et al., 2022; Weller et al., 2020). Unfortunately, studies that investigate the relationship between the training and test data are rare, which leaves many open questions in this area. Where Brown et al. (2020) report that data leakage from training had a small impact on their results, other recent work suggests that the impressive capabilities of large language models on zero- or few-shot learning tasks can largely be attributed to the presence of similar or identical examples in the training corpus (Han and Tsvetkov, 2022; Razeghi et al., 2022). Moreover, models have been reported to be sensitive to exact task formulation (Jiang et al., 2020; Schick and Schütze, 2021) and even to the order of the examples given in the few-shot setting (Lu et al., 2022), to some extent contradicting the intuitive idea of task understanding – and thus being considered as evidence against models’ generalisation ability.

零次和少量学习。最近，跨任务泛化研究的重点进一步迁移到了考虑预训练语言模型在没有任何任务特定参数的情况下在不同任务中的表现的场景(如果预训练语料库被视为不同的非受控任务的大量集合，则该场景与原始的多任务学习场景比预训练微调范式更相似)。在最极端的情况下，这意味着直接在一系列任务上评估语言模型，而不需要任何进一步的训练。为此，任务被重新表述为文本完成问题，这样语言模型可以直接提示一个表示特定任务的问题(零次学习)，可能前面会有几个例子(零次教学)(Radfordet al., 2019)。后一种情况，其意图是模型——无需任何参数更新——从上下文中给出的样本中“学习”，通常使用上下文学习一词。通过提示执行任务的数据集通常是通过调整传统的多任务数据集来创建的，其中提示模板(通常是手动)为每个任务设计的(例如Mishraet al., 2022; Wanget al., 2021; Welleret al., 2020)。不幸的是，调查训练和测试数据之间关系的研究很少，这在这方面留下了许多悬而未决的问题。Brownet al (2020)报告称，训练中的数据泄漏对他们的结果影响很小，但最近的其他研究表明，大型语言模型在零次或少量学习任务中令人印象深刻的能力很大程度上可以归因于训练语料库中存在类似或相同的样本(Han和Tsvetkov，2022; Razeghiet al., 2022)。此外，据报道，模型对精确的任务公式(姜et al., 2020;Schick和Schütze，2021)甚至对少量样本设置中给出的样本顺序(Luet al., 2022年)都很敏感，在某种程度上与任务理解的直观想法相矛盾，因此被视为对模型泛化能力的证据。

In-context finetuning. A different class of studies that considers task evaluation in the prompting setup are those that finetune a pretrained model with prompts from one set of tasks and then evaluate them on another set of tasks (e.g. Zhong et al., 2021; Sanh et al., 2022; Wei et al., 2022). Parallel to the term ‘in-context learning’, this scenario is often referred to with the term in-context finetuning. Here, the relationship between task performance and generalisation is clearer than in the zero- and few-shot learning setups. While also in this case the pretraining corpus is uncontrolled, at least the relationship between the finetuning training and test data can be monitored, and the performances on the test data with and without finetuning easily compared. Nevertheless, there are few studies that do so.

语境微调。在提示设置中考虑任务评估的另一类研究是，使用一组任务的提示微调预训练模型，然后在另一组任务上对其进行评估(例如，钟et al., 2021; 桑赫et al., 2022; 韦et al., 2021)。与术语“情境学习”类似，这种情况通常与术语“语境微调”一起使用。在这里，任务表现和泛化之间的关系比零次和少量学习设置中的关系更清晰。尽管在这种情况下，预训练语料库是不受控制的，但至少可以监控微调训练和测试数据之间的关系，并且可以容易地比较有微调和没有微调的测试数据的性能。然而，很少有研究这样做。

### 3.4 Generalisation across languages 跨语言泛化
A fourth type of generalisation is generalisation across languages, or cross-lingual generalisation. As described by Bender (2011), the availability of truly language-dependent NLP technologies would be very valuable from both a scientific and practical perspective. However, the field of NLP has been very biased towards models and technologies for English (To the point that, as pointed out in the same article from Bender (2011), studies that focus only on English do not even systematically report that this is the language that they are reporting results for. ) , and most of the recent breakthroughs rely on amounts of data that are simply not available for the vast majority of the world’s languages. Crosslingual generalisation is thus extremely important to promote the inclusivity and democratisation of the field, as well as from a practical perspective.

第四类泛化是跨语言的泛化，或跨语言泛化。正如Bender(2011)所述，从科学和实践的角度来看，真正依赖于语言的NLP技术的可用性将非常有价值。然而，NLP领域一直非常偏向于英语的模型和技术(正如Bender(2011)的同一篇文章所指出的，仅关注英语的研究甚至没有系统地报告这是他们报告结果的语言)，最近的大多数突破都依赖于世界上绝大多数语言无法获得的大量数据。因此，跨语言泛化对于促进该领域的包容性和民主化，以及从实践角度来看，都是极其重要的。

Cross-lingual finetuning. There are several ways in which cross-lingual generalisation can be evaluated. Most existing cross-lingual studies focus on the scenario where labelled data is available in a single language (typically English), and the model is evaluated in multiple languages. A common approach to address this problem is to finetune a multilingually pretrained language model on the English labelled data, and then transfer to the rest of the languages in a zero-shot fashion (e.g. Pires et al., 2019; Wu and Dredze, 2019)(10 Other approaches instead use machine translation to translate test sets into English and directly use an English model or to translate the training data into another language and finetune a multilingual model on the augmented data. As this setup does not focus on generalisation per se, but rather depends on the quality of the translation model, we will not further discuss it. ). For instance, Pires et al. (2019) show that Multilingual BERT (Devlin et al., 2019) finetuned on English generalises well even to languages with different scripts, but exhibits some systematic deficiencies that affect language pairs that have different word-order features, such as English and Japanese. Multilingual learning. A second way in which cross-lingual generalisation can be evaluated is by considering whether models trained on multiple languages at the same time (multilingual models) perform better than models trained on only one language. In multitask learning, approaches that are simultaneously trained on multiple tasks can be seen as an implicit evaluation of generalisation across tasks. Similarly, multilingual models trained on multiple languages can be seen as implicitly evaluating generalisation across languages. There is a large number of papers that investigates and proposes multilingual models, usually for language modelling or machine translation (e.g. Aharoni et al., 2019; Al-Shedivat and Parikh, 2019; Fan et al., 2021; Costa-jussà et al., 2022; Zhang et al., 2020). Most of these papers have as main aim to introduce improved models, and they are not motivated by generalisation questions. Some, however, do include explicit generalisation experiments in their setup. For instance, Zhou et al. (2018) investigate how generalisation depends on the amount of data added for different languages; whereas Aharoni et al. (2019) investigate how zero-shot generalisation changes depending on the number of different languages that a model is trained on.

跨语言微调。有几种方法可以评估跨语言概括。大多数现有的跨语言研究侧重于这样一种场景，即标签数据以单一语言(通常是英语)提供，模型以多种语言评估。解决这个问题的一种常见方法是在英语标记的数据上微调多语言预训练语言模型，然后以零样本的方式(例如。Pireset al., 2019;Wu和Dredze，2019)(10其他方法则使用机器翻译将测试集翻译成英语，直接使用英语模型，或将训练数据翻译成另一种语言，并在增强数据上微调多语言模型。由于此设置不关注泛化本身，而是取决于翻译模型的质量，因此我们将不再进一步讨论。)。例如，Pireset al (2019)表明，多语言BERT(Devlinet al., 2019)对英语泛化进行了微调，甚至适用于具有不同脚本的语言，但表现出一些系统性缺陷，影响了具有不同语序特征的语言对，如英语和日语。多语言学习。评估跨语言泛化的第二种方法是，考虑同时使用多种语言训练的模型(多语言模型)是否比仅使用一种语言训练的更好。在多任务学习中，同时针对多个任务进行训练的方法可以被视为对跨任务概括的隐式评估。类似地，在多种语言上训练的多语言模型可以被视为隐含地评估跨语言的泛化。有大量研究和提出多语言模型的论文，通常用于语言建模或机器翻译(例如，Aharoniet al., 2019;al Shedivat和Parikh，2019; Fanet al., 2021; Costa jussáet al., 2022;Zhanget al., 2020年)。大多数这些论文的主要目的是介绍改进的模型，而不是出于泛化问题。然而，有些人确实在他们的设置中包含了明确的概括实验。例如，Zhouet al (2018)研究了泛化如何取决于为不同语言添加的数据量; 而Aharoniet al (2019)研究了零样本泛化如何根据模型所训练的不同语言的数量而变化。

Multilingual benchmarks. As pointed out before, while the field of multilingual modelling is vast and associated with many interesting generalisation questions, papers in this area do not often focus explicitly on generalisation. We would, therefore, like to end this subsection by discussing the most important available multilingual benchmarks which can be used to evaluate cross-lingual generalisation.Multilingual benchmarks or datasets are created in a variety of ways. Several benchmarks are created by translating monolingual benchmarks into different languages, usually through a professional translation service (Artetxe et al., 2020; Conneau et al., 2018; Ebrahimi et al., 2022; Lewis et al., 2020; Li et al., 2021a; FitzGerald et al., 2022; Longpre et al., 2021; Mostafazadeh et al., 2016; Williams et al., 2018; Xu et al., 2020; Yang et al., 2019; Zhang et al., 2019; Lin et al., 2021b; Ponti et al., 2020). Other multilingual benchmarks, instead, have been built by separately annotating each language via its native speakers (e.g. Adelani et al., 2021; Asai et al., 2021; Clark et al., 2020; Muller et al., 2021). Yet another way to construct multilingual benchmarks is to leverage existing resources that cover multiple languages. For instance, Wikipedia has been used as a resource to derive multilingual benchmarks (Botha et al., 2020; Liu et al., 2019a; Pan et al., 2017; Rahimi et al., 2019), and several multilingual summarisation datasets have been created by extracting article-summary pairs from online newspapers or how-to guides (e.g. Hasan et al., 2021; Ladhak et al., 2020; Nguyen and Daumé III, 2019; Scialom et al., 2020; Varab and Schluter, 2021). Various linguistic resources have also been exploited: for instance, the Universal Dependencies treebank (Nivre et al., 2020) has been used to evaluate cross-lingual part-of-speech tagging, and multilingual WordNet and Wiktionary have been used to build XL-WiC (Raganato et al., 2020), an extension of WiC (Pilehvar and Camacho-Collados, 2019) that reformulates word sense disambiguation in 12 languages as a binary classification task. Finally, in the same spirit of GLUE and SuperGLUE for English, there are also several aggregated benchmarks that include selected sets of benchmarks previously proposed by others (e.g. Liang et al., 2020; Hu et al., 2020b; Ruder et al., 2021; Wang et al., 2022), which allow for evaluating cross-task and cross-language generalisation simultaneously.

多语言基准。如前所述，尽管多语言建模领域非常广泛，并与许多有趣的泛化问题相关，但该领域的论文通常不会明确关注泛化。因此，我们希望通过讨论可用于评估跨语言概括的最重要的可用多语言基准来结束本小节。多语言基准或数据集以多种方式创建。通过将单语基准翻译成不同语言来创建多个基准，通常通过专业翻译服务(Artetxeet al., 2020;Conneauet al., 2018;Ebrahimiet al., 2022;Lewiset al., 020;Liet al., 2021a; FitzGeraldet al., 202;Longpreet al., 2021; Mostafazadehet al., 2016;Williamset al., 2018; Xuet al., 2020; Yanget al., 2019;Zhanget al., 20192;Linet al., 2011b; Ponti等，2020年)。相反，其他多语言基准是通过通过母语者分别注释每种语言来建立的(例如Adelaniet al., 2021; Asaiet al., 2021; Clarket al., 2020; Mulleret al., 2021)。构建多语言基准的另一种方法是利用涵盖多种语言的现有资源。例如，维基百科已被用作导出多语言基准的资源(Bothaet al., 2020; Liuet al., 2019a; Panet al., 2017; Rahimiet al., 2019)，通过从在线报纸或操作指南中提取文章摘要对(例如。Hasanet al., 2021; Ladhaket al., 2020;Nguyen和DauméIII，2019;Scialomet al., 2020; Varab和Schluter，2021)。还开发了各种语言资源：例如，通用依赖树库(Nivreet al., 2020)已用于评估跨语言词性标记，多语言WordNet和Wiktionary已用于构建XL WiC(Raganatoet al., 2020年)，WiC的扩展(Pilehvar和Camacho Collados，2019)，将12种语言的词义消歧重新定义为二进制分类任务。最后，本着英语的GLUE和SuperGLUE的相同精神，还有几个聚合基准，包括其他人先前提出的选定基准集(例如梁et al., 2020; 胡et al., 2020b; 鲁德et al., 2021; 王et al., 2022)，这些基准允许同时评估跨任务和跨语言概括。

### 3.5 Generalisation across domains
The next category we include considers a type of generalisation that is often required in naturally occurring scenarios (more so than the types discussed so far) and is thus very important in practice: generalisation across different domains. As examples of the practical relevance of cross-domain generalisation, consider, for instance, a sentiment analysis model trained to classify the sentiment of reviews for certain products which then needs to generalise to newly commercialised products, necessarily not represented in its training data (Ryu et al., 2018; Tan et al., 2019); a model trained on data collected from one demographic which is then asked to generalise to the entire population (Blodgett et al., 2016); or a machine translation model trained on canonical text and then expected to generalise noisy data from an internet platform (Blodgett et al., 2017; Michel and Neubig, 2018) or to data from a different real-world domain (Malinin et al., 2021). While there is not a precise definition of what constitutes a domain, different domains broadly refer to collections of texts exhibiting different topical and/or stylistic properties, such as different genres or formality levels. Again, examples help us clarify this definition. MultiNLI (Williams et al., 2018), for instance, collects training corpora from five different genres (e.g. fiction and telephone conversations) and includes both an in-domain evaluation set with corpora from those five genres, as well as an out-of-domain evaluation set with corpora from five more sources (e.g. face-to-face conversations and the 9/11 public report). Blodgett et al. (2016) consider how language identification tools trained on Standard English generalise poorly to African-American English. Fried et al. (2019) compare how neural and non-neural constituency parsers generalise on out-of-domain treebanks (e.g. on a treebank of biomedical texts), whereas Artetxe et al. (2021) compare how sparse and dense language models generalise within and out of domain (on texts from ArXiv, Github, OpenSubtitles, among many other sources). Kamath et al. (2020) study the problem of selective question answering under domain shift, where the test distribution includes both in-domain and out-of-domain questions and the model must abstain from answering when not confident. Connected to this last type of study, there is a substantial body of work in out-of-domain detection (Lane et al., 2007; Hendrycks et al., 2020; Ryu et al., 2017, 2018; Tan et al., 2019).

Domain generalisation has often been studied in connection with domain adaptation, the problem of adapting an existing general model to a new domain (Daumé III, 2007). This has been a very active research area in machine translation (Axelrod et al., 2011; Bertoldi and Federico, 2009; Chu et al., 2017; Chu and Wang, 2018; Hu et al., 2019; Joty et al., 2015; Koehn and Schroeder, 2007; Freitag and Al-Onaizan, 2016; Luong and Manning, 2015; Wang et al., 2017a,b), with several standard datasets (Michel and Neubig, 2018; Malinin et al., 2021) and dedicated tracks in popular shared tasks like WMT (Bojar et al., 2019; Specia et al., 2020). In addition to machine translation, domain adaptation has also been studied in part-of-speech tagging (Blitzer et al., 2006), sentiment analysis (Blitzer et al., 2007) and language model pre-training (Gururangan et al., 2020), among others.

Finally, domain generalisation is closely related to temporal generalisation, where the training data is produced in a specific time period and the model is tested on data from a different time period, either in the future or in the past. This problem has been studied in an as yet limited range of tasks, including language modelling (Lazaridou et al., 2021), named entity recognition in social media (Derczynski et al., 2016; Fromreide et al., 2014; Rijhwani and Preotiuc-Pietro, 2020), named entity disambiguation (Agarwal et al., 2018), document classification (Huang and Paul, 2018, 2019; He et al., 2018b) and sentiment analysis (Lukes and Søgaard, 2018).

### 3.6 Generalisation in the context of robustness
The last category of generalisation research we consider on the type axis considers how robust models are with respect to changes in their exact training data. We refer to such studies, that typically assess to what extent model performance is independent from the exact training data, with the term robustness generalisation. Studies of this kind usually focus on train–test shifts that stem from the data collection process. Different from most of the previous categories discussed in §3, such shifts are generally unintended and can be hard to spot. Existing research therefore focuses on characterising such scenarios and understanding their impact. This line of work is based on the idea that models should learn task solutions that abstract away over specific, often spurious correlations that may occur in the training data, i.e. models should learn the underlying generalising solution that humans associate with the task (e.g.

Gururangan et al., 2018; McCoy et al., 2019; Talman and Chatzikyriakidis, 2019). Oftentimes, studies in this category intend to show that models do not generalise in the way we would expect them to, because the training data was in some very subtle manner not representative of the true target distribution.

Robustness evaluation is very important from a practical perspective. If a model has a strong sensitivity to spurious patterns in the training data and is then tested on data collected with the same bias, this can result in overestimating its performance – either generally or on specific test cases – with potentially harmful consequences, for instance when a model does not generalise well to particular population demographics. Evaluating generalisation in the context of robustness can be driven by several different motivations. Some studies are motivated by more practical concerns, or are conducted to gain a better perspective on intrinsic task understanding, but robustness evaluation is also particularly important when the goal is to have fair and unbiased NLP models. Below, we discuss three common scenarios associated with robustness evaluation.

Annotation artefacts A scenario that often occurs in robustness studies is one where there are annotation artefacts in the training data, which may result in overestimation of a model’s performance on a particular task. Artefacts occur particularly frequently when datasets are collected through crowdsourcing. Crowdsourced datasets often depend strongly on how exactly the annotation procedure was set up, with subtle artefacts as a consequence. For instance, annotators may naturally tend to minimise their cognitive effort, resorting to patterns that models learn to exploit. Popular NLI datasets like SNLI (Bowman et al., 2015a) and MultiNLI (Williams et al., 2018) have been found particularly susceptible to such artefacts. For instance, Gururangan et al. (2018) and Poliak et al. (2018) showed that a hypothesisonly baseline performs better than chance, due to its exploitation of spurious patterns in word choice and grammatical features (e.g. negation being indicative of the contradiction class). Talman and Chatzikyriakidis (2019) showed that NLI models do not generalise well across different datasets. Besides NLI, other tasks like question answering have also been reported to suffer from annotation artefacts (Jia and

Liang, 2017; Kaushik and Lipton, 2018), even when such ertifacts were deliberately and consciously avoided during the annotation process (Elazar et al., 2021b). Finally, Lewis et al. (2021) showed that open-domain question answering datasets have a high overlap between train and test instances, and reveal that memorisation plays a bigger role in these benchmarks than previously assumed.

Standardised splits Another line of work questions the way we use data splits in general, and in particular the extent to which scores on standardised splits that stay static over time are reflective of a model’s generalisation abilities. For instance, Gorman and Bedrick (2019) show that models perform much worse on random train–test splits than the reported state-of-the-art performances on a standardised split. Søgaard et al. (2021) go even further, and advocate for the use of heuristic and adversarial splits, 15 where a model’s capability for generalisation is challenged directly – for instance by putting all longer sentences in the test set, or by splitting the data to maximise the difference between train and test set.

Subpopulation bias A third scenario in which robustness and performance overestimation play a role is the case where certain demographics are under- or over-represented in the training data. As this may result in models that generalise poorly to specific demographic groups, it is a particularly harmful case of overestimation. For instance, Dixon et al. (2018) show that toxicity classifiers suffer from unintended bias, caused by certain identity terms being disproportionately represented in the training data (e.g. “I am a gay man” being assigned high toxicity scores because the word “gay” is often used in toxic comments). Similarly, Park et al. (2018) show that abusive language detection models exhibit gender bias, caused by imbalances in the training data. As a way to detect such imbalances and thus systematically avoid such cases of overestimation, Koh et al. (2021) propose to evaluate models by their worst-group accuracy, rather than the average accuracy across all demographic groups, in their CivilComments-Wilds dataset (a variant of the CivilCommons toxicity classification dataset released by Borkan et al., 2019). 

## 4 Shift type: what kind of data shift is considered?
As we have seen in the previous two sections, tests to evaluate generalisation may differ in terms of their motivation and the type of generalisation that they target. What they share, instead, is that they all focus on cases in which there is a form of shift between the data a model is (pre)trained on and the data that is used for evaluation. In other words, for some datasets (X1, Y1) and (X2, Y2) considered in the experimental setup, it holds that p(x1, y1) 6 = p(x2, y2). In the third axis of our taxonomy, we discuss how to characterise shifts between the datasets used in a generalisation experiment. This axis adds a more statistical interpretation to our taxonomy and derives its importance from the fact that data shift plays an essential role in formally defining and understanding generalisation from a statistical perspective. On the data shift axis, graphically depicted in Figure 4, we consider three main types of shift which are well-attested in the literature: covariate shift, label shift and full shift. We further include two additional types of shift – assumed shift and multiple shifts – to account for studies that cannot be labelled with any of the three main shift types.

What are, precisely, data shifts? We formalise the differences between the test, training and potentially pretraining data involved in generalisation tests as shifts between the respective data distributions: p(xtst, ytst) test (1) p(xtr, ytr) training / finetuning / adaptation (2) p(xptr, yptr) pretraining (3)

By expressing these data distributions as the product of the probability of the input data p(x) and the conditional probability of the output labels given the input p(y|x) – p(xtr, ytr) = p(xtr) p(ytr|xtr) (4) p(xtst, ytst) = p(xtst) p(ytst|xtst) (5) we can define four main types of relations between any two data distributions.11 One of these four types constitutes the case in which there is no shift in data distributions – i.e. both p(xtr) = p(xtst) and 11For clarity, we leave pretraining distributions aside and focus on train–test shifts, as this is the most intuitive setting.

However, the shifts described in this section can be used to describe the relationship between any two data distributions involved in a modelling pipeline. 16 p(ytr|xtr) = p(ytst|xtst). This matches the i.i.d. evaluation setup traditionally used in machine learning. As discussed earlier, this type of evaluation, also referred to as within-distribution generalisation, has frequently been reported not to be indicative of good performance for the more complex forms of generalisation that we often desire from our models. We will not further discuss it here, but instead focus on the other three cases, commonly referred to as out-of-distribution (o.o.d.) evaluation.

Covariate shift The most commonly considered data distribution shift in o.o.d. generalisation research is one where p(xtst)6=p(xtr) but p(ytst|xtst)=p(ytr|xtr). In this scenario, often referred to as covariate shift (Moreno-Torres et al., 2012; Storkey, 2009), the distribution of the input data p(x) changes, but the conditional probability of the labels given the input – which describes the task – remains the same. Under this type of shift, one can evaluate if a model has learned the underlying task distribution while only being exposed to p(xtr, ytr). In NLP, covariate shift is a very common shift to evaluate in generalisation research. For example, challenge test sets such as HANS (McCoy et al., 2019), PAWS (Yang et al., 2019), or the COGS (Kim and Linzen, 2020) test set contain deliberately unusual, out-of-distribution examples, selected or generated to violate invalid heuristics in assigning labels to data samples. Less deliberate cases of covariate shift are evaluated in out-of-domain detection or robustness evaluation studies, such as those conducted by Ryu et al. (2018) and Tan et al. (2019) on real-world datasets. Tan et al. (2019), for instance, assume that the process by which the sentiment of a sentence is to be computed does not change, but the data that this process needs to be applied to does. Of the three o.o.d. shifts we discuss in this section, covariate shift is more easily addressed without performing additional training or pre- or post-processing than the other two shift types. As we will see in the next paragraphs, a common approach to address other, more complex shifts, is to turn them into covariate shifts.

Label shift The second type of shift corresponds to the case in which the focus is not on differences between the input distributions, p(xtst) = p(xtr), but instead in the conditional distributions of the labels/output: p(ytst|xtst)6=p(ytr|xtr). We refer to this case as label shift but it is also known as concept shift (Moreno-Torres et al., 2012). Label shift can happen within the same task when there is a change of domain – e.g. the phrase ‘it doesn’t run’ can lead to different sentiment labels depending on whether it appears in a review for software or one for mascara; when there are inter-annotator disagreements; or when there is a temporal shift in the data (see §3.5). Another common case of label shift is a change in task (as in §3.3), where the meaning of the labels themselves changes as well. For example, the same sentence may need to be binarily classified for sentiment in some cases and for toxicity in others. In even more extreme cases, the labels themselves might change, for example when shifting from language modelling (where the set of labels is the language vocabulary) to POS-tagging. In NLP studies, label shift is often seen as an obstacle that needs to be overcome rather than as a setting in which models are directly evaluated: if the same example has contradictory labels in training and test data, it is unclear what decision at test time should be considered good generalising behaviour.

In practice, there are two main ways in which label shift is typically addressed. The first is to add an additional adaptation or finetuning stage, in which a model is updated to represent the shift that occurred (e.g. Biesialska et al., 2020; Sun et al., 2020), or new parameters are added to represent newly introduced labels (Devlin et al., 2019; Howard and Ruder, 2018; Peters et al., 2018, i.a.). In that scenario, there is a label shift between the pretraining and finetuning training data, but not between the finetuning training and testing data. The level at which generalisation is (somewhat implicitly) evaluated in that case, is the pretraining level: does my pretrained model adapt well to different conditional label distributions when further trained? The second way to address label shift is to augment the input data with domain or task indicators (e.g. Brown et al., 2020; Raffel et al., 2020). We saw before that the phrase ‘it doesn’t run’ can be both positive and negative, depending on what it describes. Without further information, it is impossible for a model to infer the correct meaning. By adding indicators that specify the domain (review for mascara:..., review for software:...), the problem is converted into a 17 p(y|x) p(x) full shift label shift covariate shift assumed shift

Papers assume there is a shift, without explicitly enforcing or checking that (e.g. it is simply assumed that the sentences representing 'prompts' did not occur in the training data). multiple shifts multiple shifts happen in the same experiment, because there are shifts between both the pretraining and training, and training and testing data.

Figure 4: Types of data distribution shifts that can occur on the shift type axis of our taxonomy. covariate shift (or potentially even no shift, if both indicators are represented in the two distributions at hand), which then can be solved by correctly generalising. Something similar happens in the case where a task is transformed into a question in a prompting setup: by adding a prompt that describes what needs to be done with the input, label shifts caused by a change of task are turned into a different type of shift that can be solved without further finetuning (see, e.g. Bach et al., 2022; Brown et al., 2020; Schick and

Schütze, 2021).

Full shift The most extreme type of shift corresponds to the case in which both p(x) and p(y|x) change simultaneously: p(xtst) 6 = p(xtr) and p(ytst|xtst) 6 = p(ytr|xtr). We refer to this case with the term full shift. Full shifts may occur in language modelling tasks, where changes in the p(x) directly translate into changes in p(y|x) 12, or when adapting to new language pairs in multi-lingual experiments (e.g. Costa-jussà et al., 2022; Kodner et al., 2022). Another case of full shift is the one in which entirely different types of data are used either for pretraining (e.g. Papadimitriou and Jurafsky, 2020, who test if pretraining on music impacts learning language afterwards) or for evaluation (e.g. De Varda and Zamparelli, 2022, who evaluate generalisation to different languages). Oftentimes, covariate shifts might inadvertently also cause label shifts, for instance when the textual domain changes in a sequenceclassification task. In our characterisation, however, if the underlying task stays the same, we will assume that the (more controlled) covariate shift is the one that is investigated, unless specified otherwise. Contrary to label shifts, full shifts can, in some cases, be addressed without retraining, because they do not necessarily imply that the same input x is assigned a different label at test time. However, similar to label shifts, also full shifts are often turned into different types of shifts that can be more easily addressed.

Multiple shifts In this section, we have considered three different data distributions and the types of shifts that can occur between any pair of such data distributions. Some studies, however, consider shifts between multiple distributions at the same time. For instance, Li et al. (2022) investigate how different types of pretraining architectures generalise to o.o.d. splits in a finetuning stage; and Wang et al. (2021a) investigate which pretraining method performs better cross-domain generalisation in a second training stage. In our taxonomy, we label such cases multiple shifts, and – at least in the current version – we do not distinguish between different configurations of multiple shifts (e.g. label+covariate, or covariate+covariate). We will discuss multiple shifts further in §6.

### 4.1 On detecting shift type
We conclude this section by pointing out that while from a formal perspective the shifts that we describe are well-defined, they may be difficult to tell apart in practice because the base distributions by which 12An exception is the case in which a test consists of predicting only one word, such as, for instance, in a subject-verb agreement task. In that case, the predicted word is not (“autoregressively”) part of the input of another prediction, and thus it does not automatically constitute a change in p(y|x). 18 train test natural shift

All natural data, difference between train and test occurs "in the wild" e.g.: different domains, different annotators train test partitioned natural data

Training and test data are natural, but the split is purposefully created e.g. split by length or demographic train generated shift test

The training set is natural, but the test data is generated or selected (or vice versa, but that is rarer) .e.g. adversarial test data, generated test data train test fully generated data

All data is fully generated e.g. using a grammar or templates

Figure 5: Different sources of shifts, with indications of what data is fully natural, indicated with a small globe, and data that is generated, indicated with a robot icon. natural languages are ‘generated’ are rarely fully known. As a consequence, it is often not straightforward to determine what the relationship between two different datasets is. While in some cases there is nevertheless little discussion on the type of shift that occurs between two datasets, in other cases, it might be unclear if there is an actual shift, or what its nature is. When classifying shifts in our review, we will focus on cases where authors (i) explicitly consider the relationship between the data distributions they use in their experiments and (ii) the assumptions they make about this relationship are either well-grounded in the literature (e.g. it is commonly assumed that switching between domains constitutes a covariate shift) or empirically verified. Nevertheless, we identify numerous studies that claim to be about generalisation where such considerations are absent: it is assumed that there is a shift between train and test data, but this is not verified or grounded in previous research. Sometimes, the assumed shift is not explicitly checked because it is considered plausible given general (linguistic) knowledge about language. Consider, for instance, how Lakretz et al. (2021b), as discussed earlier in §3.2, study sentences with usually deep levels of recursion. Other times, the relationship between training and test data is not investigated because the researchers do not have access to the training data. The BigBench benchmark (Srivastava et al., 2022), for instance, contains several tasks that might measure generalisation, but the training datasets of the models investigated are not in the public domain. Yet in other cases, the training data is available to the authors of the paper, but simply no extensive analysis is presented (e.g. Brown et al., 2020; Chowdhery et al., 2022). In our survey, we also consider this entire body of work, which we mark assumed shift. 

## 5 Shift source: how are the train and test data produced?

In the previous section, we discussed what types of shifts may occur in generalisation tests. We now focus on a related relevant dimension, that expresses how those shifts originated: our fourth axis, graphically shown in Figure 5, indicates the source of the differences occurring between the pretraining, training and test data distributions. The source of the data shift determines how much control the experimenter has over the training and testing data and, consequently, what kind of conclusions can be drawn from an experiment. Using fully generated data, for example, provides full control and allows to test very specific aspects in isolation, but might not be suitable to draw conclusions about a model’s behaviour when it is exposed to a natural dataset. We distinguish four different sources of shifts: (i) naturally occurring shifts, shifts occurring naturally between different corpora; (ii) splits of natural corpora, in which the data distributions involved are all natural corpora, but they are artificially partitioned along a specific dimension; (iii) generated shifts, where the training data is natural, but the test data is designed 19 with a specific distribution shift in mind;13 and (iv) fully generated datasets, where all data involved is generated.

To formalise the description of these different sources of shift, we consider the unobserved base distribution which describes all data considered in an experiment: p(xbase, ybase, τ ) base (6)

The variable τ represents a data property of interest, with respect to which a specific generalisation ability is tested. This can be an observable property of the data (e.g. the length of an input sentence), an unobservable property (e.g. the timestamp that defines when a data point was produced), or even a property relative to the model (architecture) under investigation (e.g. τ could represent how quickly a data point was learned in relation to overall model convergence). The base distribution over x, y and τ can be used to define different partition schemes, which can be adopted in generalisation experiments.

Formally, such a partitioning scheme is a rule f :T → {true, false} that discriminates data points according to a property τ ∈ T . To investigate how a partitioning scheme impacts model behaviour, the pretraining, training and test distributions can be defined as: p(xptr, yptr) = p(xbase, ybase |fpretrain(τ ) = true) (7) p(xtr, ytr) = p(xbase, ybase |ftrain(τ ) = true) (8) p(xtst, ytst) = p(xbase, ybase |ftest(τ ) = true) (9)

Using these data descriptions, we can now discuss four different sources of shifts.

Naturally occurring shifts The first scenario we consider is the one in which shifts naturally occur between different corpora. In such cases, the variable τ refers to properties that naturally differ between collected datasets. What characterises this type of shift source, is that both the data partitions of interest are naturally occurring corpora, to which no systematic operations are applied: for the purposes of a generalisation test, experimenters have no direct control over the partitioning scheme f(τ ). Examples of naturally occurring shifts emerge from splits containing data from different annotators (Geva et al., 2019), sources or domains (e.g. Artetxe et al., 2021; Talman and Chatzikyriakidis, 2019), data sampled from different populations (e.g Dixon et al., 2018; Talat et al., 2018) data from different points in time (e.g. Lazaridou et al., 2021), or separately collected corpora targeting the same task, such as MNLI (Williams et al., 2018) and WNLI (Wang et al., 2018). In this category, we also include cross-task and cross-lingual generalisation studies in which all corpora involved are natural corpora (e.g. FitzGerald et al., 2022; Mishra et al., 2022).

Splits of natural corpora A slightly less natural setup is the one in which a natural corpus is considered, but it is artificially split along specific dimensions. The primary difference with the previous category is that the variable τ refers to data properties along which data would not naturally be split, such as the length or complexity of a sample. The experimenters have thus no control over the data itself, but they do control the partitioning scheme f(τ ). Raunak et al. (2020), for instance, split naturally occurring machine translation corpora such that longer sentences occur in the test data, and Weber et al. (2021a) split a language modelling corpus such that the training data does not contain specific types of negative polarity item licensers. Other examples of natural data splits could be splits that maximise compound divergence (Keysers et al., 2019) to investigate compositionality.14 13Or, more rarely, the other way around. 14Keysers et al. (2019) themselves do not apply this split to fully natural data, their corpus is fully generated using templates. 20

Generated shifts The third category on our source of shift axis concerns the case in which one data partition (usually the training set) is a fully natural corpus, but the other partition is designed with specific properties in mind, to address a generalisation aspect of interest. Data in the constructed partition may avoid or contain specific (syntactic) patterns (Bhargava et al., 2021; Cui et al., 2022), violate heuristics about gender (Dayanik and Padó, 2021; Libovický et al., 2022), or include unusually long or complex sequences (Lakretz et al., 2021a; Raunak et al., 2019). As an example of this shift source,

Dankers et al. (2022) investigate compositionality in MT models trained on fully natural corpora by constructing test data that addresses compositional generalisation given the specific properties of the training corpus. For NLI, McCoy et al. (2019) design a test set that cannot be solved with models that rely on specific heuristics. Fancellu et al. (2017) create a test set for which the select sentences with negation scopes that are not delimited by punctuation. Another category of studies that fit into this type are those with adversarial test sets, generated either by humans (Kiela et al., 2021) or automatically using a specific model (e.g. Sakaguchi et al., 2021; Zellers et al., 2018). In the examples above, all of the constructed data occurs in the test data; note that the opposite – where instead the training data is synthetic or generated and the test data natural – is also possible, yet less common (e.g. Papadimitriou and Jurafsky, 2020).

Fully generated The last category we consider are splits that use only generated data, which sometimes may even be fully synthetic. Generating data is often the most precise way of measuring specific aspects of generalisation, as experimenters have direct control over both the base distribution and the partitioning scheme. Sometimes the data involved is entirely synthetic (e.g. Hupkes et al., 2020; Lake and Baroni, 2018), other times it is templated natural language or a narrow selection of an actual natural language corpus (e.g Keysers et al., 2019; Kim and Linzen, 2020). Generated splits can vary in several different dimensions. Sometimes, τ is a simple observable data property. For instance, Hupkes et al. (2020) split their corpus based on the presence of particular function pairs P, implicitly setting τ = P ∈ x. In some cases, τ may also be defined relative to the τ of other examples, and can only be computed globally, such as in the case of maximum compound divergence splitting (Keysers et al., 2019). 

## 6 Locus of shift: between which data distributions does the shift occur?

In the previous sections, we discussed high-level motivations for studying generalisation in NLP models, types of generalisation that have been frequently evaluated in the literature, kinds of data distribution shifts used for generalisation tests, and the possible sources of those shifts. These four axes demonstrate the depth and breadth of generalisation evaluation research, and they also clearly illustrate that generalisation is evaluated in a wide range of different experimental setups. What we have not yet explicitly discussed is between which data distributions those shifts can occur: the locus of the shift. In our taxonomy, the shift locus forms the last piece of the puzzle, as it determines what part of the modelling pipeline is investigated and, with that, what kind of generalisation questions can be asked. For instance, shifts between pretraining and training distributions allow the experimenter to investigate if a particular pretraining procedure is successful, whereas train–test shifts can be used to evaluate a model instance or a training procedure. We consider shifts between all stages in the contemporary modelling pipeline – pretraining, training and testing, as well as studies that consider shifts between multiple stages at the same time, as expressed by the data distributions that we have considered in §4 (for a graphical representation, we refer to Figure 6).

Given these distributions, there exist five possible loci of shifts: shifts only between the (finetune) training and the test data, shifts only between the pretraining and the training data, shifts only between the pretraining and the test data, and shifts between all data distributions. Because they often reflect different types of experiments, we separate shifts between train and test data without pretraining from 21 pretrain data train data test data pre-training model model (pre-trained model) train/fine-tune - test pre-training - test pre-training procedure finetuning procedure pre-training - train multiple

Figure 6: Different loci of splits, and what parts of the modelling pipeline they may investigate generalisation for. shifts between finetuning train and test data. We describe the four loci of shift and how they interact with different components of the modelling pipeline with the aid of three modelling distributions. These modelling distributions correspond to the different stages in contemporary machine learning pipelines – testing a model, training it, and potentially pretraining it: p(Ytst | Xtst, θ∗) model (10) p(θ∗ | Xtr, Ytr, φtr, θˆ) training/finetuning/adaptation (11) p(θˆ| Xptr, Yptr, φpr, θ0) pretraining (12)

In these equations, φ broadly denotes training and pretraining hyperparameters, θ refers to model parameters, and X , Y indicate sets of inputs (x) and their corresponding output (y). In short, Equation 10 defines a model instance, which specifies the probability distribution over the target test labels Ytst, given the model’s parameters θ∗ and a set of test inputs Xtst. Equation 11, instead, defines a training procedure, specifying a probability distribution over model parameters θ∗ ∈ Rd given a training dataset

Xtr, Ytr, a set of training hyperparameters φtr, and a (potentially pretrained) model initialisation θˆ.

Lastly, Equation 12 defines a pretraining procedure, specifying a conditional probability over the set of parameters θˆ, given a pretraining dataset, a set of pretraining hyperparameters φpr, and a model initialisation.15 Between which of these stages a shift occurs impacts which of these modelling distributions can be evaluated. We discuss the different potential loci of shifts below.

The train–test locus Probably the most commonly occurring locus of shift in generalisation experiments is the one between train and test data. This locus occurs in the classic setup where a model is trained on some training data and then directly evaluated on a shifted (out-of-distribution) test partition.

Studies with the train–test locus can assess two different parts of the modelling pipeline. In some cases, researchers investigate the generalisation abilities of a model instance (i.e. a set of parameters θ∗ , as described in Equation 10). Studies of this type therefore report the evaluation of a single model instance – typically made available by others – without considering how exactly it was trained, and how that impacted the model’s generalisation behaviour. For example, a surge of studies considered the behaviour 15Note that this formalisation generalises to the training from scratch paradigm when Xptr, Yptr = ∅, ∅, and to the incontext-learning setup when Xtr, Ytr = ∅, ∅. 22 of the pretrained language model made available by Gulordava et al. (2018), to investigate how it generalised to, for instance, different syntactic constructions (e.g. Lakretz et al., 2019).16 Alternatively, researchers might evaluate one or more training procedures, by considering if the training distribution results in model instances that generalise well – for example, to study how generalisation compares between dense and sparse models or how that changes with the scale of the input data (e.g. Artetxe et al., 2021; Rae et al., 2021), or how different architectures behave on a compositional generalisation test (Mul and Zuidema, 2019; Saxton et al., 2019). While also this case requires evaluating model instances, the focus of the evaluation is not on one particular model instance, but rather on the procedure that generated multiple model instances.

The finetune train–test locus The second potential locus of shift bears similarities to the first one but instead considers data shifts between the train and test data during finetuning, considering a model that has already gone through an earlier stage of training. This locus occurs when a model is evaluated on a finetuning test set that contains a shift with respect to the finetuning training data. An example of this category would be a test that investigates how well one pretrained model generalises with respect to an o.o.d. finetuning train–test split (Damonte and Monti, 2021; Kavumba et al., 2022; Ludwig et al., 2022).

The parts of the modelling pipeline that studies with a finetune train–test locus can evaluate are the same as studies with a train–test locus, although studies that investigate the generalisation abilities of a single finetuned model instance are rare. More frequently, research with this locus focuses on the finetuning procedure, by considering if it results in finetuned model instances that generalise well on the finetune test set. Note that studies evaluating o.o.d. splits during finetuning, often also include a comparison between different pretraining procedures (e.g. they investigate whether BERT or RoBERTa generalises better to an o.o.d. finetuning test set, or compare how BERT models trained on different corpora behave during finetuning). Such studies (usually) investigate both a shift from the pretraining to the finetuning training data (typically a label shift), as well as a shift in the finetuning stage, and we will mark them as having multiple loci, as will be further discussed in the last paragraph of this section.

The pretrain-train locus A third potential locus of shift is between the pretraining and training corpus. Experiments with this locus evaluate whether a particular pretraining procedure, as described in

Equation 12, results in models (parameter sets θˆ) that are useful when further trained on different tasks or domains. For instance, Artetxe et al. (2021) investigate which pretraining procedure shows the best downstream generalisation in a number of different tasks, Tian et al. (2021) investigate how well pretrained models generalise to a newly proposed first-order-logic dataset, and Freitag and Al-Onaizan (2016) test how well a pretrained NMT model can adapt to different domains. Crucially, we classify studies as having a pretrain-train locus only when in their second training stage – which is required to have this locus – they use i.i.d. splits. If also the finetuning stage contains a shift, we say that the study has multiple loci.

The pretrain–test locus The fourth potential locus of shift is between pretraining and test data. This locus occurs when a pretrained model is not further updated but evaluated directly (i.e. Xtr, Ytr = ∅, ∅) – as frequently happens in in-context learning setups (e.g. Lin et al., 2021b; Zhang et al., 2022a) – or when a pretrained model is finetuned on examples that are i.i.d. with respect to the pretraining data and then tested on out-of-distribution instances. The former case (θ∗ = θˆ) is similar to studies with only one training stage in the train–test locus, but distinguishes itself by the nature of the (pre)training procedure, which typically has a general purpose objective, rather than being task-specific (e.g. a language modelling objective). Furthermore, while generalisation studies with a train–test locus almost always 16The investigation of model instances is, however, more common with the pretrain-test locus that we will discuss later in this section. 23 explicitly consider the relationship between training and test data, this is frequently not the case with pretrain–test studies in an in-context learning or finetuning setup: often, they do not explicitly consider the relationship between training and test data, but merely assume a shift occurs between those stages (e.g. Radford et al., 2019).

Multiple loci The last option on our locus axis is the multiple loci case, which we use for works that consider, in a single study, multiple shifts between different parts of the modelling pipeline. More explicitly, experiments of this type present shifts both between the pretraining and training data, as well as between the training and test data.17 Multiple-loci experiments evaluate all stages of the modelling pipeline at once: they consider both how generalisable the models produced by the pretraining procedure are, as well as whether generalisation happens in the finetuning stage itself. For instance, some studies compare how well models with different pretraining procedures (e.g. BERT vs RoBERTa) generalise to o.o.d. splits during finetuning (e.g. Tu et al., 2020), others how different multilingual pretraining procedures perform cross-lingual task generalisation in a finetuning stage (e.g. FitzGerald et al., 2022; Hu et al., 2020b; Yanaka et al., 2021). Because multiple-loci experiments necessarily also contain multiple shifts, we mark them as multiple shifts in the shift type axis. The nature of these shifts may not be the same: the shift from pretraining to training may be of any type, while the shift from training to test is often – but not necessarily – a less extreme covariate shift. In the current version of the taxonomy, we do not further distinguish these cases but collapse them into a single ‘multiple shifts’ category. 

## 7 A review of existing generalisation research

In this paper, we have presented a taxonomy containing five categorical axes that can be used to characterise generalisation research. We now use our taxonomy to analyse a large amount of existing generalisation research and create a comprehensive map indicating which areas are covered and which are still unexplored. On our website18, we present interactive ways to visualise our results and to retrieve relevant citations, which the reader can use to get a more in-depth view, to understand how their work fits in with the rest of the literature or which areas might be promising to address. We provide instructions for other researchers to contribute to the review, for instance by proposing to add new studies and studies we may have missed or by proposing corrections to studies that might have been misqualified on one of their axes values. In this section, we present our main findings.

### 7.1 Setup
We first briefly describe the procedures we used for the selection of the papers in our review and their annotation.

Paper selection An initial selection of manuscripts was made through a substantive preliminary literature review by the main authors of this paper. We then carried out a search through the ACL anthology.

We started by retrieving all papers that have the (sub)words generalisation, generalization, generalise or generalize in their title or abstract. In Figure 8, we see that the number of papers with those keywords grew substantially over time, both in absolute and relative terms. We manually checked the abstracts and titles of the resulting papers to remove those that were not, in fact, addressing a generalisation question (for instance, because they proposed a generalisation of a method, or because they used random train–test splits). Furthermore, we restricted ourselves to papers with one modality. We then annotated 17We do not distinguish cases where the test data is shifted with respect to the pretraining data from cases where it is not, as the latter are very uncommon. It is, however, possible to set up an experiment where the pretraining and test data are drawn from the same distribution, for example to test whether a finetuning procedure results in catastrophic forgetting. 18https://genbench.github.io/visualisations 24
 

Figure 7: A graphical representation of our annotation process and an indication of where in a paper you might find the information required to complete the annotation. One paper can potentially contain multiple generalisation questions – e.g. both cross-domain and cross-task generalisation, or both generated shifts and splits using natural data. In that case, the diagram has to be walked through twice. Of course, the diagram is an aid that helps characterise papers but also simplifies the full taxonomy. On our website, we keep track of common questions that arise when using the diagram to characterise papers in an FAQ. 25 0 50 100 150 200 250 300 350 year (a) 01234567 year (b) 0 1000 2000 3000 4000 5000 6000 7000 Papers per year

Generalisation papers

Other year (c)<br/>
Figure 8: We selected papers from the ACL anthology that contain the (sub)words generalisation, generalization, generalise or generalize in their title or abstract. This figure shows how many of such papers exist per year, both absolutely (a) and percentually (b). In (c), we also show the total number of papers and generalisation papers published each year. the resulting papers using the taxonomy presented in the previous sections. During the annotation process, we sometimes removed entries that upon further reading did not, in fact, contain generalisation experiments, and we duplicated entries that contained multiple experiments with different values on one of our axes. The findings presented in this section encompass in total 619 generalisation experiments, presented in 449 papers. The full list of papers can be found in the second bibliography at the end of this paper, as well as on our website19. While the conclusions in this – static – paper pertain only to this specific selection of papers, we intend to keep expanding the number of entries on our website with existing papers we missed or as new generalisation papers are published.

Annotation The annotation of all selected papers was done collectively by the authors of this article.

Each paper was given five labels by a first annotator, one for every axis of our taxonomy, and these labels were then checked by a second annotator. Disagreements were discussed among the two annotators, and for unresolved cases, a third annotator was used. As a guide, we used the diagram presented in Figure 7.

An FAQ with common questions that occurred while using this diagram, which intends to capture our taxonomy but is naturally a simplified version of it, can be found on our website. In addition to the taxonomy axes values, we also annotated which task(s) the studies considered. If a paper performed the same experiment with multiple different tasks, we label it multiple tasks, use the overarching category (e.g. NLU) when possible, or mark it as multitask if the purpose is to show that a paper can do those all at the same time. If a paper contained multiple studies with different values on the same axis – e.g. a paper considers both cross-domain and compositional generalisation or uses both natural shifts and synthetic data – we record those experiments separately.

### 7.2 Results
We now proceed to present the main conclusions drawn from our review, in particular focusing on overall trends for each axis (§7.2.1) and on how the different axes interact with each other (§7.2.2).

##### 7.2.1 Overall trends on different axes
First, we discuss the overall occurrences of values on all axes, without taking into account interactions between them. We plot the (relative) occurrences of all values in Figure 9 and their development over time in Figure 10. Because the number of generalisation papers before 2018 included is very low (see 19https://genbench.github.io/references 26 count percent count 1952 1962 1971 1977 1982 1987 1992 1997 2002 2007 2012 2017 2022 1952 1962 1971 1977 1982 1987 1992 1997 2002 2007 2012 2017 2022 1957 1965 1974 1979 1984 1989 1994 1999 2004 2009 2014 2019 motivation shift locus shift type shift source generalisation type

Figure 9: Summary plot displaying the relative occurrences of the categories available within the five different axes of our taxonomy (shown clockwise are the motivation, the generalisation type, the shift source, the shift type and the shift locus).

Figure 8a), we restricted the over-time plots to the last five years; all other statistics reported are computed over all papers.

Motivations As we can see in Figure 9 (top left), by far the most common motivation to test generalisation is the practical motivation. The intrinsic and cognitive motivations follow, whereas the studies in our review that consider generalisation from a fairness perspective make up only 3% of the total. We hypothesise that one of the reasons that this percentage is so low stems from the fact that our keywords search in the anthology was not optimal for detecting fairness studies, and we welcome researchers to suggest other generalisation studies with a fairness motivation for review. We will include them in an updated version of this paper. However, we also speculate that only relatively recently attention for the potential harmfulness of models trained on large, uncontrolled corpora is starting to grow and that fairness has simply not been studied as much in the context of generalisation yet. Due to the extremely low number of fairness studies in our review, it is not possible to observe a reliable growth of fairness papers in the last few years. In Figure 10a, we see that trends on the motivation axis have some small fluctuations over time but have been relatively stable over the past five years.

Generalisation type For generalisation types (Figure 9, left side), we find that cross-domain is the most frequent, making up more than 30% of all studies, followed by robustness, cross-task and compositional generalisation. Structural and cross-lingual generalisation are the least commonly investigated.

As already mentioned in the respective section, studies looking at the understanding of syntactic and morphological structure typically focus more on whether models can capture structures at all, rather than on whether they generalise to new structures, which could be a potential explanation for the fact 27 13% compositional 30% finetune-train/test 22% robustness 7% multiple loci 9% structural 8% pretrain-test 11% pretrain-train 8% assumed 43% train-test 64% covariate 16% full 13% cognitive 5% label 3% fairness 7% multiple shifts 18% intrinsic 67% practical 11% fully generated 22% generated shifts 32% across domain 48% natural shifts 9% across language 19% partitioned natural data 15% across task 2018 2019 2020 2021 2022 0 20 40 60 80 100 motivation practical fairness cognitive intrinsic year (a) Motivation 2018 2019 2020 2021 2022 0 20 40 60 80 100 shift type covariate label full assumed multiple shifts year (b) Shift type 2018 2019 2020 2021 2022 0 20 40 60 80 100 shift locus train-test pretrain-train finetune-train/test multiple loci pretrain-test year (c) Shift locus

Figure 10: Trends from the past five years for three of the taxonomy’s axes (motivation, shift type and shift locus), normalised by the total number of papers annotated per year. that such studies are underrepresented. The underrepresentation of cross-lingual studies could, similar to studies with a fairness motivation, be partly explained by the fact that they might less frequently use the word generalisation in their title or abstract. However, we hypothesise that, at least in part, the low numbers are also reflective of the English-centric approach that is usually taken in NLP. As with fairness studies, we encourage researchers to suggest cross-lingual generalisation studies that we may have missed via our website so that we can determine better to what extent cross-lingual studies are, in fact, underrepresented.

Shift type Data shift types (Figure 9, bottom) are very unevenly distributed over their potential values: the vast majority of generalisation research considers covariate shift. Given the fact that covariate shift can occur between any two stages in the modelling pipeline, and label and full shift typically only occur between pretraining and finetuning, this is – to some extent – to be expected. Furthermore, covariate shift is more easily addressed by most current modelling techniques. More unexpected, perhaps, is the relatively high amount of assumed shifts, which correspond to studies that claim to test generalisation but do not explicitly consider how the test data relates to data used at various stages of model training.

In Figure 10b, we see that the percentage of assumed shifts has increased over the past few years. We hypothesise that this trend, which is a step in the wrong direction in that it indicates less precision about what we evaluate rather than more, is predominantly caused by the use of increasingly large, generalpurpose training corpora. Such large corpora, which are often also not in the public domain, make it very challenging to analyse the relationship between the training and testing data and, consequently, make it hard to determine what kind of conclusions can be drawn based on test accuracies. More promising, instead, is the fact that several studies consider multiple shifts, meaning that they assess generalisation throughout the entire modelling pipeline rather than only in one stage.

Shift source On the shift source axis (Figure 9, bottom right), we see that almost half of the reviewed generalisation studies consider naturally occurring shifts: natural corpora that are not deliberately split along a particular dimension. As we will see later, this type of data source is most prevalent in cross-task and cross-domain generalisation studies, for which such naturally different corpora are widely available.

The next most frequent category is generated shifts, where one of the datasets involved is generated with a specific generalisation property in mind, and artificially partitioned natural data, describing settings in which all data is natural, but the way it is split between train and test is not. Fully generated datasets are less common, making up only 11% of the total number of studies.

Shift locus Lastly, for the locus axis (Figure 9, top right), we see that the majority of cases focuses on (finetune) train–test splits. Much fewer studies focus on shifts between pretraining and training or 28 percent pretraining and testing. Similar to the previous axis, we observe that a comparatively small percentage of studies considers shifts in multiple stages of the modelling pipeline. We hypothesise that, at least in part, this might be driven by the larger amount of compute that is typically required for those scenarios.

In Figure 10c, however, we also see an alternative explanation for the lower overall frequency of studies considering multiple loci and pretrain–test loci: the values populating Figure 9 are averaged over all years represented in our paper selection, but the multiple and pretrain–test loci became more popular only in the last few years.

#### 7.2.2 Interactions between axes
Next, we consider interactions between different axes. Are there any combinations of axes that occur together very often or combinations that are instead rare? We encourage the reader to view these interactions dynamically on our website. Here, we discuss a few trends.

What data shift source is used for different generalisation types? In Figure 11a, we plot the frequency of each data source per generalisation type, normalised by the total number of times that generalisation type occurs (to make patterns comparable between generalisation types). From this plot, we can see that the type of data used is vastly different across different types of generalisation tests. Compositional generalisation, for instance, is predominantly tested with fully generated data, a data type that hardly occurs in research considering robustness, cross-lingual or cross-task generalisation. Those three types of generalisation are most frequently tested with naturally occurring shifts or, in some cases, with artificial splits of natural corpora. Structural generalisation, on the other hand, is the only generalisation type that appears to be tested across all different data types. As far as we know, there are very few studies that directly compare results between different sources of shift – for instance, to investigate to what extent results on generated shifts or fully generated data are indicative of performances on natural corpora.20 Such studies could provide insight into how choices in the experimental design impact the conclusions that are drawn from the experiment, and we believe that they are an important direction for future work.

For which loci of shift are different generalisation types studied? Another interesting question to ask is for which locus different generalisation types are considered. In Figure 11b, we see that of all the generalisation types, only cross-task generalisation is frequently investigated in the pretrain-train and pretrain–test stages. For all other types of generalisation, the vast majority of tests are conducted in the train–test or finetune-train/test stage. In some cases, these differences are to be expected: as general-purpose pretrained models are usually trained on very large, relatively uncontrolled corpora, investigating how they generalise to a different domain without further finetuning is typically not possible, and neither is evaluating their robustness, which typically also requires more detailed knowledge of the training data. The statistics also confirm the absence of studies that consider compositional generalisation from pretraining to finetuning, or even from pretraining to training, which as we previously reported (§3.1) is philosophically and theoretically challenging in such setups. A final observation is the relative under-representation of studies with multiple loci across all generalisation types, especially given the large number of studies that consider generalisation in the finetuning stage or the pretrain-training stage.

Those studies have used both a pretraining and finetuning stage but considered generalisation in only one of those. We hope to see this trend changing in the future, with more studies considering generalisation in the entire modelling pipeline, rather than only in a specific part of it. 20An example of such a study would be the work of Chaabouni et al. (2021), who investigate whether performance improvements on SCAN transfer to machine translation models trained on natural data. 29 8 69 2 21 14 71 2 12 9 77 3 11 22 1 54 23 47 27 4 21 40 26 18 16 across domain across language across task compositional robustness structural 0 20 40 60 80 100 shift source (a) Data source per generalisation type 42 9 10 6 32 27 16 9 5 43 5 2 39 27 26 74 2 0 2 21 44 7 7 3 39 74 5 0 11 11 across domain across language across task compositional robustness structural 0 20 40 60 80 100 shift locus (b) Shift locus per generalisation type 6 0 85 3 6 0 100 0 0 0 4 0 37 19 40 40 0 31 4 25 3 0 67 6 24 train-test multiple loci pretrain-train pretrain-test finetune-train/test 0 20 40 60 80 100 shift type (c) Shift type per locus 5 83 10 3 5 86 4 5 11 77 10 2 33 44 22 1 4 60 31 5 44 25 32 0 across domain across language across task compositional robustness structural 0 20 40 60 80 100 motivation (d) Motivation per generalisation type 68 5 9 4 14 35 7 14 8 35 52 7 4 11 26 56 6 17 11 11 cognitive practical intrinsic fairness 0 20 40 60 80 100 shift locus (e) Locus per motivation 30 28 33 9 17 58 5 20 33 24 18 25 28 72 0 0 cognitive practical intrinsic fairness 0 20 40 60 80 100 shift source (f) Shift source per generalisation type

Figure 11: Heatmaps of interactions between axes. The maps are normalised by the total row value. This facilitates the comparison of patterns between rows but renders columns incomparable. We welcome readers who would like to see different normalisations or readers that are curious about interactions between other axes to have a look at our website, where they can generate other plots based on the same underlying data. 30 generalisation type generalisation type shift locus generalisation type motivation motivation generated shifts natural shifts fully generated partitioned natural data train-test multiple loci pretrain-train pretrain-test finetune-train/test assumed multiple shifts covariate label full cognitivepracticalintrinsicfairness train-test multiple loci pretrain-train pretrain-test finetune-train/test generated shifts natural shifts fully generated partitioned natural data

Which types of data shifts occur across different loci? Another interaction we would like to discuss is the one between the shift locus and the type of data shift. We plot this interaction in Figure 11c.

A notable observation is that assumed shifts mostly occur in the pretrain–test locus, which confirms our hypothesis put forward earlier when discussing frequencies on the shift type axis – that assumed shifts are likely caused by the use of increasingly large, general-purpose training corpora. When such pretrained models are further finetuned, they often consider either a shift between pretraining and finetuning where new labels are introduced, or a covariate shift in the finetuning stage and, as such, do not require an in-depth understanding of the pretraining corpus.21 When such models are directly evaluated, however, the only shift that can be considered is the one between the very large pretraining corpus and the test corpus. This trend points to a substantial challenge when it comes to evaluating generalisation for models with limited knowledge about their pretraining.

How does motivation drive generalisation research? The last pattern we would like to discuss is the relationship between the motivation behind a study and the other axes, focusing in particular on generalisation type, shift locus and shift source, as shown in Figure 11d-11f. Considering first the relationship between motivation and generalisation type (Figure 11d), we see that cross-domain, robustness, crosstask and cross-lingual generalisation are predominantly motivated by practical considerations. Robustness generalisation studies are also frequently motivated by the interest in understanding how models work (the intrinsic motivation). When looking at compositional and structural generalisation studies, we see that both are frequently driven by cognitive motivations – which is to be expected given the importance of these concepts in human cognition and intelligence. The motivation given most frequently for compositional generalisation, however, is a practical one. While in human learning, compositionality is indeed often associated with important practical properties – speed of learning, quick generalisation – as far as we know, there is little empirical evidence that compositional models actually perform better for natural language tasks. A similar apparent mismatch can be seen in Figure 11f when looking at the practical motivation. Practical generalisation tests are typically aimed at improving models or at being directly informative of a model’s applicability. Nonetheless, almost 25% of the practically motivated studies use either artificially partitioned natural data or even fully generated data. To what extent could their conclusions then actually be informative of models applied in practical scenarios? These apparent mismatches between the motivation and the experimental setup exemplify the importance of the motivation axis in our taxonomy – being aware and explicit about it should ensure that the conclusions of a study are indeed informative of the question it claims to answer.

Another interesting observation that can be made from the interactions between motivation and shift locus is that the vast majority of cognitively motivated studies are conducted in a train–test setup. While there are many good reasons for this, conclusions about human generalisation are drawn from a much more varied range of ‘experimental setups’. For instance, any experiments done with adults are more similar to finetune train–test or pretrain–test locus than to the train–test locus, as adults have a lifelong experience over which the experimenter has little control beyond participant selection. On the one hand, this suggests that generalisation with a cognitive motivation should perhaps be evaluated more with those loci. On the other hand, it begs the question: for the – previously reported challenging – evaluation of generalisation of LLMs trained on uncontrolled corpora in a pretrain–test setting, could we perhaps take inspiration from how generalisation is evaluated in ‘pretrained’ humans? While there are, of course, substantial differences between the assumptions that can reasonably be made about the history of a human and the pretraining of an LLM22, we still believe that input from domain experts that 21The observant reader might note that there are, in fact, also several covariate and full shifts with a pretrain-train locus, as well as covariate shifts with a pretrain–test locus. These typically do not represent experiments with LLMS but instead, for instance, consider a multi-stage process for domain adaptation, which also includes a zero-shot comparison. 22On the one hand, for a human, some assumptions can be safely made or even verified with a participant – for instance, unless a person has previously participated in a psycholinguistic experiment, we can almost be certain that they have never 31 have extensively considered human generalisation might be very beneficial to improve generalisation testing in these more challenging setups. 

## 8 Conclusion
While the ability to generalise well – i.e. to successfully transfer skills learned from past experience to new experiences – is considered to be one of the primary desiderata for NLP models, there is very little agreement on what kind of generalisation behaviour modern-age NLP models should exhibit, and under what conditions that should be evaluated. For decades, generalisation has been simply evaluated with random train–test splits. The recent past, however, has seen a number of studies illustrating that models that exhibit near-perfect performances on such i.i.d. splits can sometimes drastically fail in a wide range of scenarios that require different forms of generalisation. This body of work demonstrates the need for more comprehensive generalisation testing, but does not provide much guidance on what that should look like: different papers use different experimental setups, different types of data and entertain even different ideas about what it means for an NLP model to generalise well. As a consequence, even though its importance is almost undisputed, extensive, state-of-the-art generalisation testing is not currently the standard in NLP. With this paper, we aimed to set the first steps towards making it the new status quo.

### 8.1 Our generalisation taxonomy
We presented a new framework to systematise and understand generalisation research, with the ultimate goal to lay the groundwork for making generalisation testing the new status quo in NLP. The first part of this framework consists of a generalisation taxonomy that can be used to characterise generalisation studies along various dimensions. This taxonomy, which is designed based on an extensive review of generalisation papers in NLP, can be used to critically analyse existing generalisation research and to structure new studies. It contains five nominal axes, that describe why the study was executed (the main motivation of the study), what the study intends to evaluate (the type of generalisation they aim to solve), and how it does so (the type of data shift they are considering, the source by which this data shift was obtained, and the locus in which the shift is investigated). An overview of our taxonomy is provided in Figure 1; the axes are discussed in §2-6.

### 8.2 Our analysis
To illustrate the use and usefulness of our taxonomy, we analysed by means of it 449 papers that have the (sub)words generali(s/z)ation or generali(s/z)e in their title or abstract. We hope that researchers will use our taxonomy to design future generalisation studies and to critically and explicitly characterise their experiments. To this end, on our website, we provide an annotation diagram that can be used to design and conceptualise generalisation studies. Through our extensive analysis, we demonstrated that the taxonomy is applicable to a wide range of generalisation studies, and we were able to provide a comprehensive map of the field, observing overall patterns and making suggestions for areas that should be prioritised in the future. In §7, we described the results of this review: we discussed overall patterns on individual axes, as well as interactions between different axes and trends over time – all illustrated with compelling data visualisations. Our most important conclusions and recommendations are:
• The goal of a study is not always perfectly lined up with its experimental design. We advise that future work is explicit about their motivations – which strongly impact what sort of generalisation conjugated nonce words. For an LLM, this is less trivially true, as reports about such human experiments may have been present in their (pre)training data. On the other hand, for an LLM it is possible to inspect the data that they have seen during pretraining, which is evidently not the case for humans. 32 is even desirable – and should incorporate deliberate assessments to ensure that the experimental setup is aligned with the goal of the study.
• Cross-lingual studies and generalisation studies motivated by fairness goals are underrepresented.

We suggest that these areas be given more attention in future work.
• Papers that target similar generalisation questions vary widely in the type of evaluation setup they use. In our view, the field would benefit from more meta-studies that consider how the results of experiments with different experimental paradigms compare to each other.
• The vast majority of generalisation studies focuses on only one stage of the modelling pipeline.

More work is needed that considers generalisation in all stages of training, to prioritise models whose generalising behaviour persists throughout their training curriculum.
• Recent popular NLP models that can be tested directly for their generalisation from pretraining to testing (e.g. in prompting setups, without any further model training) have often been evaluated without considering the relationship between the (pre)training and the test data. We envisage that this is due to the fact that generalisation is particularly difficult to assess when large uncontrolled training data is involved, and we suggest that inspiration might be taken from how generalisation is evaluated in experiments with adult humans, where control and access to the “pretraining” data of a participant are unattainable.

Along with this paper, we also launch a website with a set of visualisation tools and the possibility to browse through our review to find studies with specific features, as well as relevant paper references.

While the review and conclusions presented in this paper are necessarily static, we commit to keeping the entries on the website up to date when new papers on generalisation are published and we encourage researchers to engage with our online dynamic review by submitting both new studies and existing studies we might have missed – through the contributions page of our website.

### 8.3 Future work
By providing a systematic framework and set of concrete (online) tools to allow for a structured understanding of generalisation, we believe we have set the necessary first step towards making state-ofthe-art generalisation testing the new status quo in NLP. Our work is thus by no means the end of the road. While our taxonomy can make future generalisation research in NLP more comparable, structured and carefully designed, and while our survey suggests promising research directions, this work does not provide standardised data or procedures for generalisation testing. We envision that important generalisation tests should be hosted on a shared platform, along with a leaderboard to make generalisation testing more accessible and transparent. A large community of NLP researchers and domain experts should determine which tests to prioritise. Lastly, in the same way that our thoughts on how generalisation should be evaluated have evolved with our models in the past, it will likely continue to do so in the future. What we consider important to evaluate now might change next year, and when models get better at setups considered difficult now, we might discover new types of generalisation that we had not thought of before. How we evaluate models should be reflective of that, and which tests are prioritised should thus evolve along with our models and knowledge. Ideally, all of those aspects should be incorporated in the next steps towards making state-of-the-art generalisation testing the new status quo for any new model that is proposed, and we look forward to working on it. 

## 9 Limitations
Designing a coherent, consistent, and at the same time, usable taxonomy of generalisation research in NLP is a non-trivial task, which required substantial discussion among the authors. In this section, 33 we report the main decisional trade-offs of our work, concerning the definition of the taxonomy, the annotation process and the selection of papers to review.

### 9.1 Taxonomy design: the axes and their values
We designed this taxonomy by ensuring that the selected set of axes and axis values would highlight theoretically important but also practically functional distinctions between generalisation studies – yet our selection comes with limitations. One such limitation is that the axis values are relatively coarse.

This avoids fragmentation in the analysis and allows to draw higher-level conclusions, but sometimes also groups together papers that could be regarded separately. An already discussed example are the studies with a pretrain-train locus, which by definition all share that they include more than one training stage and investigate generalisation in the first one. This category thus contains both papers that use a general-purpose pretraining objective and then finetune on different tasks and studies whose finetuning objective matches the pretraining objectives (e.g. studies that consider domain-adaptation in a continual learning setup). While those differences are – at least in part – reflected on other axes, in some cases it might be helpful to distinguish those two cases more explicitly.

Something similar occurs on the shift type axis. Firstly, when there are multiple shifts, we do not currently distinguish between all possible combinations of individual shift types. Given the relatively low number of studies that actually consider multiple shifts, we prioritised intelligibility over completeness, but if the number of multiple-shift studies increases in the future, it could become useful to indicate all individual shift types in the case of studies with multiple shifts. Secondly, while the three formal shift types that we consider are statistically well-grounded, shifts of the same type can still largely vary.

Whether the distance between two distributions is small or large might make a substantial difference for the difficulty of the generalisation problem, which is something that is currently not reflected in our taxonomy. Although quantifying differences between distributions is often problematic in practice, we believe that adjusting the taxonomy to capture the difficulty of generalising to a particular shift can be helpful in the future. More generally, we imagine that future experimental paradigms might call for the addition of values on some of the axes, or even the addition of new axes.

### 9.2 Annotation: axes values in practice
In the description of the axes and their different values, we aimed to be as comprehensive and precise as possible. In practice, however, there are always cases in which the actual category of a paper is debatable. Sometimes this occurs because the paper itself is not clear about what exactly it attempts to evaluate or about its motivation; we hope that our taxonomy will reduce the number of such cases in the future. In other instances, it is simply difficult to apply some concepts or distinctions, in spite of their theoretical sharpness, to concrete studies. A clear example of this challenge is the shift type. In theory, p(x), p(y|x) and p(y) are clearly defined concepts; in practice, it is usually impossible to estimate the actual difference between two (natural) distributions. Some researchers might even argue that, in practice, train and test sets are virtually always distributionally different. For the purpose of systematising generalisation testing and characterising experiments, however, this is not a useful observation. In our taxonomy design and annotations, we aimed to make distinctions that we deemed useful, rather than relying on “true” but unknown differences between distributions.

### 9.3 Paper selection
To ensure that our selection of papers was not biased toward works already known by the authors, we automatically selected a large number of papers from the ACL anthology by searching for generalisation keywords in the abstract and title. While this resulted in a relatively large amount of papers, there are likely papers about generalisation that we did not retrieve with this approach. As mentioned earlier 34 (§7.2.1), we suspect that papers about cross-lingual generalisation and papers with a fairness motivation may require a different set of keywords. We hope that researchers will take the effort to inform us about generalisation papers that we may have missed, to guarantee that the selection of surveyed papers is as complete as possible.

Aside from unintentionally missed papers, we also deliberately excluded a few types of papers.

We did not include any studies that considered more than one modality. While we believe they are interesting to consider from a generalisation perspective, they are also more difficult to characterise within a single taxonomy, as they involve more distributions (with sometimes very different support) and thus more distribution shifts. We consider including such papers a compelling step for future work.

Another set of papers that we excluded are those that do not conduct behavioural experiments but look at the generalisability of representations (e.g. probing papers). We do not see any a priori reason that they could not be characterised with our taxonomy, and we believe this would be a valuable enterprise. In particular, although marking the difference between behavioural and representational experiments might require updating the taxonomy, a comparison of behavioural and representational experiments with the same axis values might make for an interesting meta-study.

### 9.4 Is generalisation always necessary?
A last critical observation that we would like to make is that our work builds on the assumption that strong generalisation skills are considered crucial for models of NLP. While we generally believe this to be true, there might be cases where generalisation is not in fact needed. Provocatively, one could argue that for LLMs trained on extremely large English data sets, practically speaking the vast majority of scenarios that one might want to use the model for is actually close to i.i.d. and that more complex forms of generalisation are thus not needed. We abstain from judging whether and when this holds, but argue that if a researcher believes that their setup requires no generalisation, they should clearly state so and explain why they believe that to be the case.

## Acknowledgements
We thank Adina Williams, Armand Joulin, Elia Bruni, Lucas Weber, Robert Kirk and Sebastian Riedel for providing us feedback on various stages of this draft, and Gary Marcus for providing detailed feedback on the final draft of this paper. We thank Elte Hupkes for making the app that allows searching through references, and we thank Daniel Haziza and Ece Takmaz for other contributions to the website.

## References
