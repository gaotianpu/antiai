# A Survey of Transformers
2021.6.8 https://arxiv.org/abs/2106.04554

## é˜…è¯»ç¬”è®°
* å¤„ç†é•¿åºåˆ—çš„æ•ˆç‡ä½ï¼Œå°è§„æ¨¡æ•°æ®éš¾ä»¥è®­ç»ƒ(ä¸æ˜¯äº‹å„¿)


## Abstract 
Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.

Transformersåœ¨è®¸å¤šäººå·¥æ™ºèƒ½é¢†åŸŸå–å¾—äº†å·¨å¤§æˆåŠŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰å’ŒéŸ³é¢‘å¤„ç†ã€‚å› æ­¤ï¼Œè‡ªç„¶ä¼šå¸å¼•å­¦æœ¯ç•Œå’Œè¡Œä¸šç ”ç©¶äººå‘˜çš„å¤§é‡å…´è¶£ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå·²ç»æå‡ºäº†å¤šç§Transformerå˜ä½“(ä¹Ÿç§°ä¸ºX-formers)ï¼Œä½†æ˜¯ï¼Œä»ç„¶ç¼ºå°‘å…³äºè¿™äº›Transformerå˜ä½“çš„ç³»ç»Ÿå’Œå…¨é¢çš„æ–‡çŒ®ç»¼è¿°ã€‚åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬å¯¹å„ç§X-formersè¿›è¡Œäº†å…¨é¢çš„å›é¡¾ã€‚æˆ‘ä»¬é¦–å…ˆç®€è¦ä»‹ç»äº†åŸå§‹Transformerï¼Œç„¶åæå‡ºäº†ä¸€ç§æ–°çš„X-formersã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä»ä¸‰ä¸ªè§’åº¦ä»‹ç»å„ç§X-formersï¼šæ¶æ„ä¿®æ”¹ã€é¢„è®­ç»ƒå’Œåº”ç”¨ã€‚æœ€åï¼Œæˆ‘ä»¬æ¦‚è¿°äº†æœªæ¥ç ”ç©¶çš„ä¸€äº›æ½œåœ¨æ–¹å‘ã€‚

CCS Concepts: * General and reference â†’ Surveys and overviews; * Computing methodologies â†’ Artificial intelligence.

Additional Key Words and Phrases: Transformer, Self-Attention, Pre-trained Models, Deep Learning 

## 1 INTRODUCTION
Transformer [137] is a prominent deep learning model that has been widely adopted in various fields, such as natural language processing (NLP), computer vision (CV) and speech processing. Transformer was originally proposed as a sequence-to-sequence model [130] for machine translation. Later works show that Transformer-based pre-trained models (PTMs) [100] can achieve state-ofthe-art performances on various tasks. As a consequence, Transformer has become the go-to architecture in NLP, especially for PTMs. In addition to language related applications, Transformer has also been adopted in CV [13, 33, 94], audio processing [15, 31, 41] and even other disciplines, such as chemistry [114] and life sciences [109].

Transformer[137]æ˜¯ä¸€ç§çªå‡ºçš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå·²è¢«å¹¿æ³›åº”ç”¨äºå„ç§é¢†åŸŸï¼Œå¦‚è‡ªç„¶è¯­è¨€å¤„ç†(NLP)ã€è®¡ç®—æœºè§†è§‰(CV)å’Œè¯­éŸ³å¤„ç†ã€‚Transformeræœ€åˆè¢«æå‡ºä½œä¸ºæœºå™¨ç¿»è¯‘çš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹[130]ã€‚åæ¥çš„ç ”ç©¶è¡¨æ˜ï¼ŒåŸºäºTransformerçš„é¢„è®­ç»ƒæ¨¡å‹(PTM)[100]å¯ä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šå®ç°æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚å› æ­¤ï¼ŒTransformerå·²æˆä¸ºNLPçš„é¦–é€‰æ¶æ„ï¼Œå°¤å…¶æ˜¯PTMã€‚é™¤äº†è¯­è¨€ç›¸å…³çš„åº”ç”¨ï¼ŒTransformerè¿˜è¢«åº”ç”¨äºCV[13ã€33ã€94]ã€éŸ³é¢‘å¤„ç†[15ã€31ã€41]ç”šè‡³å…¶ä»–å­¦ç§‘ï¼Œå¦‚åŒ–å­¦[114]å’Œç”Ÿå‘½ç§‘å­¦[109]ã€‚

Due to the success, a variety of Transformer variants (a.k.a. X-formers) have been proposed over the past few years. These X-formers improve the vanilla Transformer from different perspectives. 
1. Model Efficiency. A key challenge of applying Transformer is its inefficiency at processing long sequences mainly due to the computation and memory complexity of the self-attention module. The improvement methods include lightweight attention (e.g. sparse attention variants) and Divide-and-conquer methods (e.g., recurrent and hierarchical mechanism). 
2. Model Generalization. Since the transformer is a flexible architecture and makes few assumptions on the structural bias of input data, it is hard to train on small-scale data. The improvement methods include introducing structural bias or regularization, pre-training on large-scale unlabeled data, etc. 
3. Model Adaptation. This line of work aims to adapt the Transformer to specific downstream tasks and applications.

ç”±äºæˆåŠŸï¼Œåœ¨è¿‡å»å‡ å¹´ä¸­æå‡ºäº†å„ç§Transformerå˜ä½“(ä¹Ÿç§°ä¸ºX-formers)ã€‚è¿™äº›X-formerä»ä¸åŒçš„è§’åº¦æ”¹è¿›äº†æœ€åˆçš„Transformerã€‚
1. æ¨¡å‹æ•ˆç‡ã€‚åº”ç”¨Transformerçš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å…¶å¤„ç†é•¿åºåˆ—çš„æ•ˆç‡ä½ä¸‹ï¼Œè¿™ä¸»è¦æ˜¯ç”±äºè‡ªå…³æ³¨æ¨¡å—çš„è®¡ç®—å’Œå†…å­˜å¤æ‚æ€§ã€‚æ”¹è¿›æ–¹æ³•åŒ…æ‹¬è½»é‡çº§æ³¨æ„åŠ›(ä¾‹å¦‚ç¨€ç–æ³¨æ„åŠ›å˜ä½“)å’Œåˆ†è€Œæ²»ä¹‹æ–¹æ³•(ä¾‹å¦‚å¾ªç¯å’Œåˆ†å±‚æœºåˆ¶)ã€‚
2. æ¨¡å‹æ³›åŒ–ã€‚ç”±äºå˜æ¢å™¨æ˜¯ä¸€ç§çµæ´»çš„æ¶æ„ï¼Œå¹¶ä¸”å¾ˆå°‘å¯¹è¾“å…¥æ•°æ®çš„ç»“æ„åå·®è¿›è¡Œå‡è®¾ï¼Œå› æ­¤å¾ˆéš¾å¯¹å°è§„æ¨¡æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æ”¹è¿›æ–¹æ³•åŒ…æ‹¬å¼•å…¥ç»“æ„åå·®æˆ–æ­£åˆ™åŒ–ã€å¯¹å¤§è§„æ¨¡æœªæ ‡æ³¨æ•°æ®è¿›è¡Œé¢„è®­ç»ƒç­‰ã€‚
3. æ¨¡å‹é€‚åº”ã€‚è¿™é¡¹å·¥ä½œæ—¨åœ¨ä½¿Transformeré€‚åº”ç‰¹å®šçš„ä¸‹æ¸¸ä»»åŠ¡å’Œåº”ç”¨ã€‚

In this survey, we aim to provide a comprehensive review of the Transformer and its variants.Although we can organize X-formers on the basis of the perspectives mentioned above, many existing X-formers may address one or several issues. For example, sparse attention variants not only reduce the computational complexity but also introduce structural prior on input data to alleviate the overfitting problem on small datasets. Therefore, it is more methodical to categorize the various existing X-formers and propose a new taxonomy mainly according to their ways to improve the vanilla Transformer: architecture modification, pre-training, and applications. Considering the audience of this survey may be from different domains, we mainly focus on the general architecture variants and just briefly discuss the specific variants on pre-training and applications.

åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨å¯¹TransformeråŠå…¶å˜ä½“è¿›è¡Œå…¨é¢å®¡æŸ¥ã€‚å°½ç®¡æˆ‘ä»¬å¯ä»¥æ ¹æ®ä¸Šè¿°è§‚ç‚¹æ¥ç»„ç»‡Xæˆå½¢è€…ï¼Œä½†è®¸å¤šç°æœ‰çš„Xæˆå½¢è€…å¯èƒ½ä¼šè§£å†³ä¸€ä¸ªæˆ–å¤šä¸ªé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œç¨€ç–æ³¨æ„å˜é‡ä¸ä»…é™ä½äº†è®¡ç®—å¤æ‚æ€§ï¼Œè¿˜å¼•å…¥äº†è¾“å…¥æ•°æ®çš„ç»“æ„å…ˆéªŒï¼Œä»¥ç¼“è§£å°æ•°æ®é›†ä¸Šçš„è¿‡åº¦æ‹Ÿåˆé—®é¢˜ã€‚å› æ­¤ï¼Œå¯¹å„ç§ç°æœ‰çš„Xå½¢å™¨è¿›è¡Œåˆ†ç±»å¹¶æå‡ºä¸€ç§æ–°çš„åˆ†ç±»æ³•æ›´å…·ç³»ç»Ÿæ€§ï¼Œä¸»è¦æ˜¯æ ¹æ®å®ƒä»¬æ”¹è¿›æ™®é€šTransformerçš„æ–¹å¼ï¼šæ¶æ„ä¿®æ”¹ã€é¢„è®­ç»ƒå’Œåº”ç”¨ç¨‹åºã€‚è€ƒè™‘åˆ°æœ¬æ¬¡è°ƒæŸ¥çš„å—ä¼—å¯èƒ½æ¥è‡ªä¸åŒçš„é¢†åŸŸï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨ä¸€èˆ¬æ¶æ„å˜ä½“ï¼Œä»…ç®€è¦è®¨è®ºé¢„è®­ç»ƒå’Œåº”ç”¨ç¨‹åºçš„å…·ä½“å˜ä½“ã€‚

The rest of the survey is organized as follows. Sec. 2 introduces the architecture and the key components of Transformer. Sec. 3 clarifies the categorization of Transformer variants. Sec. 4âˆ¼5 review the module-level modifications, including attention module, position encoding, layer normalization and feed-forward layer. Sec. 6 reviews the architecture-level variants. Sec. 7 introduces some of the representative Transformer-based PTMs. Sec. 8 introduces the application of Transformer to various different fields. Sec. 9 discusses some aspects of Transformer that researchers might find intriguing and summarizes the paper. 

è°ƒæŸ¥çš„å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ã€‚ç¬¬2èŠ‚ä»‹ç»äº†Transformerçš„æ¶æ„å’Œå…³é”®ç»„ä»¶ã€‚ç¬¬3èŠ‚é˜æ˜äº†Transformerå˜ä½“çš„åˆ†ç±»ã€‚ç¬¬4ï½5èŠ‚å›é¡¾äº†æ¨¡å—çº§ä¿®æ”¹ï¼ŒåŒ…æ‹¬æ³¨æ„æ¨¡å—ã€ä½ç½®ç¼–ç ã€å±‚å½’ä¸€åŒ–å’Œå‰é¦ˆå±‚ã€‚ç¬¬6èŠ‚å®¡æŸ¥äº†æ¶æ„çº§åˆ«çš„å˜ä½“ã€‚ç¬¬7èŠ‚ä»‹ç»äº†ä¸€äº›å…¸å‹çš„åŸºäºTransformerçš„PTMã€‚ç¬¬8èŠ‚ä»‹ç»äº†Transformeråœ¨ä¸åŒé¢†åŸŸçš„åº”ç”¨ã€‚ç¬¬9èŠ‚è®¨è®ºäº†ç ”ç©¶äººå‘˜å¯èƒ½ä¼šå‘ç°æœ‰è¶£çš„Transformerçš„ä¸€äº›æ–¹é¢ï¼Œå¹¶æ€»ç»“äº†è®ºæ–‡ã€‚

## 2 BACKGROUND
### 2.1 Vanilla Transformer
The vanilla Transformer [137] is a sequence-to-sequence model and consists of an encoder and a decoder, each of which is a stack of ğ¿ identical blocks. Each encoder block is mainly composed of a multi-head self-attention module and a position-wise feed-forward network (FFN). For building a deeper model, a residual connection [49] is employed around each module, followed by Layer Normalization [4] module. Compared to the encoder blocks, decoder blocks additionally insert cross-attention modules between the multi-head self-attention modules and the position-wise FFNs. Furthermore, the self-attention modules in the decoder are adapted to prevent each position from attending to subsequent positions. The overall architecture of the vanilla Transformer is shown in Fig. 1.

æœ€åˆçš„Transformer[137]æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—æ¨¡å‹ï¼Œç”±ç¼–ç å™¨å’Œè§£ç å™¨ç»„æˆï¼Œæ¯ä¸ªç¼–ç å™¨å’Œè§£ç å™¨éƒ½æ˜¯ä¸€ä¸ªå †æ ˆğ¿ ç›¸åŒçš„å—ã€‚æ¯ä¸ªç¼–ç å™¨å—ä¸»è¦ç”±å¤šå¤´è‡ªå…³æ³¨æ¨¡å—å’Œä½ç½®å‰é¦ˆç½‘ç»œ(FFN)ç»„æˆã€‚ä¸ºäº†æ„å»ºæ›´æ·±å…¥çš„æ¨¡å‹ï¼Œåœ¨æ¯ä¸ªæ¨¡å—å‘¨å›´ä½¿ç”¨æ®‹å·®è¿æ¥[49]ï¼Œç„¶åæ˜¯å±‚å½’ä¸€åŒ–[4]æ¨¡å—ã€‚ä¸ç¼–ç å™¨å—ç›¸æ¯”ï¼Œè§£ç å™¨å—é¢å¤–åœ°åœ¨å¤šå¤´è‡ªå…³æ³¨æ¨¡å—å’Œä½ç½®æ–¹å‘FFNä¹‹é—´æ’å…¥äº¤å‰å…³æ³¨æ¨¡å—ã€‚æ­¤å¤–ï¼Œè§£ç å™¨ä¸­çš„è‡ªæ³¨æ„æ¨¡å—é€‚äºé˜²æ­¢æ¯ä¸ªä½ç½®å…³æ³¨åç»­ä½ç½®ã€‚æ™®é€šTransformerçš„æ€»ä½“ç»“æ„å¦‚å›¾1æ‰€ç¤ºã€‚

In the following subsection, we shall introduce the key modules of the vanilla Transformer.
åœ¨ä¸‹é¢çš„å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»æœ€åˆçš„Transformerçš„å…³é”®æ¨¡å—ã€‚

#### 2.1.1 Attention Modules. 
Transformer adopts attention mechanism with Query-Key-Value (QKV) model. Given the packed matrix representations of queries Q âˆˆ Rğ‘ Ã—ğ·ğ‘˜ , keys K âˆˆ Rğ‘€Ã—ğ·ğ‘˜ , and values V âˆˆ Rğ‘€Ã—ğ·ğ‘£ , the scaled dot-product attention used by Transformer is given by(1 if not stated otherwise, we use row-major notations throughout this survey (e.g., the ğ‘–-th row in Q is the query qğ‘– ) and all the vectors are row vectors by default.)

Transformeré‡‡ç”¨å¸¦æœ‰æŸ¥è¯¢-å…³é”®å­—-å€¼(QKV)æ¨¡å‹çš„æ³¨æ„æœºåˆ¶ã€‚ç»™å®šæŸ¥è¯¢QâˆˆRçš„å‹ç¼©çŸ©é˜µè¡¨ç¤ºğ‘ Ã—ğ·ğ‘˜ , é”®KâˆˆRğ‘€Ã—ğ·ğ‘˜ , å€¼VâˆˆRğ‘€Ã—ğ·ğ‘£ , Transformerä½¿ç”¨çš„ç¼©æ”¾ç‚¹ç§¯å…³æ³¨åº¦ç”±(1)ç»™å‡ºï¼Œå¦‚æœæ²¡æœ‰å¦å¤–è¯´æ˜ï¼Œæˆ‘ä»¬åœ¨æ•´ä¸ªè°ƒæŸ¥ä¸­ä½¿ç”¨è¡Œä¸»è¦ç¬¦å·(ä¾‹å¦‚ğ‘–-Qä¸­çš„ç¬¬è¡Œæ˜¯æŸ¥è¯¢Qğ‘– ) å¹¶ä¸”é»˜è®¤æƒ…å†µä¸‹æ‰€æœ‰å‘é‡éƒ½æ˜¯è¡Œå‘é‡ã€‚)

Attention(Q, K, V) = softmax  QKâŠ¤ âˆšğ·ğ‘˜  V = AV, 1.  

where ğ‘ and ğ‘€ denote the lengths of queries and keys (or values); ğ·ğ‘˜ and ğ·ğ‘£ denote the dimensions of keys (or queries) and values; A = softmax  QKâŠ¤ âˆšğ·ğ‘˜  is often called attention matrix; softmax is applied in a row-wise manner. The dot-products of queries and keys are divided by âˆšğ·ğ‘˜ to alleviate gradient vanishing problem of the softmax function.

å…¶ä¸­ï¼Œğ‘ å’Œğ‘€ è¡¨ç¤ºæŸ¥è¯¢å’Œé”®(æˆ–å€¼)çš„é•¿åº¦; ğ·ğ‘˜ å’Œğ·ğ‘£ è¡¨ç¤ºé”®(æˆ–æŸ¥è¯¢)å’Œå€¼çš„ç»´åº¦; A=è½¯æœ€å¤§QKâŠ¤âˆšğ·ğ‘˜ é€šå¸¸ç§°ä¸ºæ³¨æ„åŠ›çŸ©é˜µ; ä»¥è¡Œæ–¹å¼åº”ç”¨softmaxã€‚æŸ¥è¯¢å’Œé”®çš„ç‚¹ç§¯é™¤ä»¥âˆšğ·ğ‘˜ ä»¥ç¼“è§£softmaxå‡½æ•°çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

Instead of simply applying a single attention function, Transformer uses multi-head attention, where the ğ·ğ‘š-dimensional original queries, keys and values are projected into ğ·ğ‘˜ , ğ·ğ‘˜ and ğ·ğ‘£ dimensions, respectively, with ğ» different sets of learned projections. For each of the projected queries, keys and values, and output is computed with attention according to Eq. 1. . The model then concatenates all the outputs and projects them back to a ğ·ğ‘š-dimensional representation.

Transformerä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ï¼Œè€Œä¸æ˜¯ç®€å•åœ°åº”ç”¨å•ä¸ªæ³¨æ„åŠ›åŠŸèƒ½ğ·ğ‘š-ç»´åº¦åŸå§‹æŸ¥è¯¢ã€é”®å’Œå€¼è¢«æŠ•å½±åˆ°ğ·ğ‘˜ , ğ·ğ‘˜ å’Œğ·ğ‘£ å°ºå¯¸ï¼Œåˆ†åˆ«ä¸ºğ» ä¸åŒçš„å­¦ä¹ é¢„æµ‹é›†ã€‚å¯¹äºæ¯ä¸ªé¢„æµ‹çš„æŸ¥è¯¢ã€é”®å’Œå€¼ä»¥åŠè¾“å‡ºï¼Œæ ¹æ®ç­‰å¼(1)è¿›è¡Œè®¡ç®—ã€‚ç„¶åï¼Œæ¨¡å‹è¿æ¥æ‰€æœ‰è¾“å‡ºï¼Œå¹¶å°†å®ƒä»¬æŠ•å°„å›ğ·ğ‘š-ç»´åº¦è¡¨ç¤ºã€‚

MultiHeadAttn(Q, K, V) = Concat(head1, Â· Â· Â· , headğ» )Wğ‘‚, 2.  

where headğ‘– = Attention(QWğ‘„ğ‘– , KWğ¾ğ‘– , VWğ‘‰ğ‘– ). 3.  

![Fig. 1. Overview of vanilla Transformer architecture](../images/Transformers_Survey/fig_1.png)
Fig. 1. Overview of vanilla Transformer architecture

In Transformer, there are three types of attention in terms of the source of queries and key-value pairs:
* Self-attention. In Transformer encoder, we set Q = K = V = X in Eq. 2. , where X is the outputs of the previous layer.
* Masked Self-attention. In the Transformer decoder, the self-attention is restricted such that queries at each position can only attend to all key-value pairs up to and including that position. To enable parallel training, this is typically done by applying a mask function to the unnormalized attention matrix Ë†A = exp(QKâŠ¤ âˆšğ·ğ‘˜ ), where the illegal positions are masked out by setting ğ´Ë† ğ‘–ğ‘— = âˆ’âˆ if ğ‘– < ğ‘—. This kind of self-attention is often referred to as autoregressive or causal attention(2This term seems to be borrowed from the causal system, where the output depends on past and current inputs but not future inputs.). 
* Cross-attention. The queries are projected from the outputs of the previous (decoder) layer, whereas the keys and values are projected using the outputs of the encoder.

åœ¨Transformerä¸­ï¼Œåœ¨æŸ¥è¯¢æºå’Œé”®å€¼å¯¹æ–¹é¢æœ‰ä¸‰ç§å…³æ³¨ï¼š
* è‡ªæ³¨æ„ã€‚åœ¨Transformerç¼–ç å™¨ä¸­ï¼Œæˆ‘ä»¬åœ¨ç­‰å¼(2)ä¸­è®¾ç½®Q=K=V=Xï¼Œå…¶ä¸­Xæ˜¯å‰ä¸€å±‚çš„è¾“å‡ºã€‚
* Masked è‡ªæ³¨æ„ã€‚åœ¨Transformerè§£ç å™¨ä¸­ï¼Œè‡ªæ³¨æ„å—åˆ°é™åˆ¶ï¼Œä½¿å¾—æ¯ä¸ªä½ç½®çš„æŸ¥è¯¢åªèƒ½å…³æ³¨è¯¥ä½ç½®ä¹‹å‰(åŒ…æ‹¬è¯¥ä½ç½®)çš„æ‰€æœ‰é”®å€¼å¯¹ã€‚ä¸ºäº†å®ç°å¹¶è¡Œè®­ç»ƒï¼Œè¿™é€šå¸¸é€šè¿‡å°†æ©ç å‡½æ•°åº”ç”¨äºæœªæ ‡å‡†åŒ–çš„æ³¨æ„åŠ›çŸ©é˜µæ¥å®Œæˆğ·ğ‘˜ ), éæ³•ä½ç½®é€šè¿‡è®¾ç½®ğ´Ë† ğ‘–ğ‘— = âˆ’âˆ å¦‚æœğ‘– < ğ‘—. è¿™ç§è‡ªæ³¨æ„é€šå¸¸è¢«ç§°ä¸ºè‡ªå›å½’æˆ–å› æœæ³¨æ„2
* äº¤å‰å…³æ³¨ã€‚æŸ¥è¯¢æ˜¯ä»å‰ä¸€(è§£ç å™¨)å±‚çš„è¾“å‡ºä¸­æŠ•å½±çš„ï¼Œè€Œé”®å’Œå€¼æ˜¯ä½¿ç”¨ç¼–ç å™¨çš„è¾“å‡ºè¿›è¡ŒæŠ•å½±çš„ã€‚

#### 2.1.2 Position-wise FFN. 
The position-wise FFN(3The parameters are shared across different positions, thus the position-wise FFN can also be understood as two convolution layers with kernel size of 1. ) is a fully connected feed-forward module that operates separately and identically on each position

ä½ç½®æ–¹å‘FFN(3å‚æ•°åœ¨ä¸åŒä½ç½®ä¸Šå…±äº«ï¼Œå› æ­¤ä½ç½®æ–¹å‘çš„FFNä¹Ÿå¯ä»¥ç†è§£ä¸ºå†…æ ¸å¤§å°ä¸º1çš„ä¸¤ä¸ªå·ç§¯å±‚ã€‚)æ˜¯ä¸€ä¸ªå®Œå…¨è¿æ¥çš„å‰é¦ˆæ¨¡å—ï¼Œåœ¨æ¯ä¸ªä½ç½®ä¸Šç‹¬ç«‹ä¸”ç›¸åŒåœ°å·¥ä½œ

FFN(Hâ€²) = ReLU(Hâ€²W1 + b1)W2 + b2, 4.  

where Hâ€² is the outputs of previous layer, and W1 âˆˆ Rğ·ğ‘šÃ—ğ·ğ‘“ , W2 âˆˆ Rğ·ğ‘“ Ã—ğ·ğ‘š, b1 âˆˆ Rğ·ğ‘“ , b2 âˆˆ Rğ·ğ‘š are trainable parameters. Typically the intermediate dimension ğ·ğ‘“ of the FFN is set to be larger than ğ·ğ‘š.

å…¶ä¸­Hâ€²æ˜¯å‰ä¸€å±‚çš„è¾“å‡ºï¼ŒW1âˆˆRğ·ğ‘šÃ—ğ·ğ‘“ , W2âˆˆRğ·ğ‘“ Ã—ğ·ğ‘š, b1âˆˆRğ·ğ‘“ , b2âˆˆRğ·ğ‘š æ˜¯å¯è®­ç»ƒçš„å‚æ•°ã€‚é€šå¸¸ä¸ºä¸­é—´å°ºå¯¸ğ·ğ‘“ FFNçš„å€¼è®¾ç½®ä¸ºå¤§äºğ·ğ‘š.
 
#### 2.1.3 Residual Connection and Normalization. 
In order to build a deep model, Transformer employs a residual connection [49] around each module, followed by Layer Normalization [4]. For instance, each Transformer encoder block may be written as

ä¸ºäº†å»ºç«‹ä¸€ä¸ªæ·±åº¦æ¨¡å‹ï¼ŒTransformeråœ¨æ¯ä¸ªæ¨¡å—å‘¨å›´ä½¿ç”¨äº†ä¸€ä¸ªæ®‹ä½™è¿æ¥[49]ï¼Œç„¶åæ˜¯å±‚å½’ä¸€åŒ–[4]ã€‚ä¾‹å¦‚ï¼Œæ¯ä¸ªTransformerç¼–ç å™¨å—å¯ä»¥å†™ä¸º

Hâ€² = LayerNorm(SelfAttention(X) + X) 5. 

H = LayerNorm(FFN(Hâ€² ) + Hâ€²), 6.  where SelfAttention(Â·) denotes self attention module and LayerNorm(Â·) denotes the layer normalization operation.

H=LayerNorm(FFN(Hâ€²)+Hâ€²)ï¼Œ(6)ï¼Œå…¶ä¸­SelfAttention(Â·)è¡¨ç¤ºè‡ªæ³¨æ„æ¨¡å—ï¼ŒLayerNor(Â·)æŒ‡ç¤ºå±‚å½’ä¸€åŒ–æ“ä½œã€‚

#### 2.1.4 Position Encodings. 
Since Transformer doesnâ€™t introduce recurrence or convolution, it is ignorant of positional information (especially for the encoder). Thus additional positional representation (Detailed discussion in Sec. 5.1) is needed to model the ordering of tokens.

ç”±äºTransformerä¸å¼•å…¥å¾ªç¯æˆ–å·ç§¯ï¼Œå› æ­¤å®ƒä¸äº†è§£ä½ç½®ä¿¡æ¯(å°¤å…¶æ˜¯ç¼–ç å™¨)ã€‚å› æ­¤ï¼Œéœ€è¦é¢å¤–çš„ä½ç½®è¡¨ç¤º(ç¬¬5.1èŠ‚ä¸­çš„è¯¦ç»†è®¨è®º)æ¥æ¨¡æ‹Ÿä»¤ç‰Œçš„æ’åºã€‚

### 2.2 Model Usage
Generally, the Transformer architecture can be used in three different ways:
* Encoder-Decoder. The full Transformer architecture as introduced in Sec. 2.1 is used. This is typically used in sequence-to-sequence modeling (e.g., neural machine translation).
* Encoder only. Only the encoder is used and the outputs of the encoder are utilized as a representation for the input sequence. This is usually used for classification or sequence labeling problems.
* Decoder only. Only the decoder is used, where the encoder-decoder cross-attention module is also removed. This is typically used for sequence generation, such as language modeling.

é€šå¸¸ï¼ŒTransformeræ¶æ„å¯ä»¥ä¸‰ç§ä¸åŒçš„æ–¹å¼ä½¿ç”¨ï¼š
* ç¼–ç å™¨è§£ç å™¨ã€‚ä½¿ç”¨ç¬¬2.1èŠ‚ä¸­ä»‹ç»çš„å®Œæ•´Transformeræ¶æ„ã€‚è¿™é€šå¸¸ç”¨äºåºåˆ—åˆ°åºåˆ—å»ºæ¨¡(ä¾‹å¦‚ï¼Œç¥ç»æœºå™¨ç¿»è¯‘)ã€‚
* ä»…ç¼–ç å™¨ã€‚ä»…ä½¿ç”¨ç¼–ç å™¨ï¼Œå¹¶ä¸”ç¼–ç å™¨çš„è¾“å‡ºè¢«ç”¨ä½œè¾“å…¥åºåˆ—çš„è¡¨ç¤ºã€‚è¿™é€šå¸¸ç”¨äºåˆ†ç±»æˆ–åºåˆ—æ ‡è®°é—®é¢˜ã€‚
* ä»…è§£ç å™¨ã€‚ä»…ä½¿ç”¨è§£ç å™¨ï¼Œå…¶ä¸­ç¼–ç å™¨-è§£ç å™¨äº¤å‰å…³æ³¨æ¨¡å—ä¹Ÿè¢«ç§»é™¤ã€‚è¿™é€šå¸¸ç”¨äºåºåˆ—ç”Ÿæˆï¼Œä¾‹å¦‚è¯­è¨€å»ºæ¨¡ã€‚

### 2.3 Model Analysis
To illustrate the computation time and parameter requirements of the Transformer, we analyze the two core components of the Transformer (i.e., the self-attention module and the position-wise FFN) in Table 1. We assume that the hidden dimension ğ·ğ‘š of the model is ğ·, and that the input sequence length is ğ‘‡ . The intermediate dimension of FFN is set to 4ğ· and the dimension of keys and values are set to ğ·/ğ» as in Vaswani et al. [137].

ä¸ºäº†è¯´æ˜Transformerçš„è®¡ç®—æ—¶é—´å’Œå‚æ•°è¦æ±‚ï¼Œæˆ‘ä»¬åˆ†æäº†è¡¨1ä¸­Transformerçš„ä¸¤ä¸ªæ ¸å¿ƒç»„ä»¶(å³è‡ªæ³¨æ„æ¨¡å—å’Œä½ç½®å¼FFN)ã€‚æˆ‘ä»¬å‡è®¾éšè—ç»´åº¦ğ·ğ‘š æ¨¡å‹çš„ğ·, å¹¶ä¸”è¾“å…¥åºåˆ—é•¿åº¦ä¸ºğ‘‡ . FFNçš„ä¸­é—´å°ºå¯¸è®¾ç½®ä¸º4ğ· é”®å’Œå€¼çš„ç»´åº¦è®¾ç½®ä¸ºğ·/ğ» å¦‚Vaswaniet al [137]æ‰€è¿°ã€‚

Table 1. Complexity and parameter counts of self-attention and position-wise FFN
è¡¨1ã€‚è‡ªæ³¨æ„å’Œä½ç½®å¼FFNçš„å¤æ‚æ€§å’Œå‚æ•°è®¡æ•°

When the input sequences are short, the hidden dimension ğ· dominates the complexity of self-attention and position-wise FFN. The bottleneck of Transformer thus lies in FFN. However, as the input sequences grow longer, the sequence length ğ‘‡ gradually dominates the complexity of these modules, in which case self-attention becomes the bottleneck of Transformer. Furthermore, the computation of self-attention requires that a ğ‘‡ Ã— ğ‘‡ attention distribution matrix is stored, which makes the computation of Transformer infeasible for long-sequence scenarios (e.g., long text documents and pixel-level modeling of high-resolution images). One shall see that the goal of increasing the efficiency of Transformer generally leads to the long-sequence compatibility of self-attention, as well as the computation and parameter efficiency of position-wise FFN for ordinary settings.

å½“è¾“å…¥åºåˆ—çŸ­æ—¶ï¼Œéšè—ç»´åº¦ğ· æ§åˆ¶äº†è‡ªæ³¨æ„å’Œä½ç½®å‹FFNçš„å¤æ‚æ€§ã€‚å› æ­¤ï¼ŒTransformerçš„ç“¶é¢ˆåœ¨äºFFNã€‚ç„¶è€Œï¼Œéšç€è¾“å…¥åºåˆ—çš„å¢é•¿ï¼Œåºåˆ—é•¿åº¦ğ‘‡ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè‡ªæ³¨æ„æˆä¸ºTransformerçš„ç“¶é¢ˆã€‚æ­¤å¤–ï¼Œè‡ªæ³¨æ„çš„è®¡ç®—éœ€è¦ğ‘‡ Ã— ğ‘‡ å­˜å‚¨äº†æ³¨æ„åŠ›åˆ†å¸ƒçŸ©é˜µï¼Œè¿™ä½¿å¾—Transformerçš„è®¡ç®—å¯¹äºé•¿åºåˆ—åœºæ™¯(ä¾‹å¦‚ï¼Œé•¿æ–‡æœ¬æ–‡æ¡£å’Œé«˜åˆ†è¾¨ç‡å›¾åƒçš„åƒç´ çº§å»ºæ¨¡)ä¸å¯è¡Œã€‚åº”è¯¥çœ‹åˆ°ï¼Œæé«˜Transformeræ•ˆç‡çš„ç›®æ ‡é€šå¸¸ä¼šå¯¼è‡´è‡ªæ³¨æ„çš„é•¿åºåˆ—å…¼å®¹æ€§ï¼Œä»¥åŠæ™®é€šè®¾ç½®çš„ä½ç½®å¼FFNçš„è®¡ç®—å’Œå‚æ•°æ•ˆç‡ã€‚

### 2.4 Comparing Transformer to Other Network Types Transformerä¸å…¶ä»–ç½‘ç»œç±»å‹çš„æ¯”è¾ƒ
#### 2.4.1 Analysis of Self-Attention.  è‡ªæ³¨æ„åˆ†æ
As a central piece of Transformer, self-attention comes with a flexible mechanism to deal with variable-length inputs. It can be understood as a fully connected layer where the weights are dynamically generated from pairwise relations from inputs. Table 2 compares the complexity, sequential operations, and maximum path length(4The maximum length of the paths forward and backward signals have to traverse to get from any input position to arbitrary output position. Shorter length implies a better potential for learning long-range dependencies. ) of self-attention with three commonly used layer types. We summarize the advantages of self-attention as follows: 
1. It has the same maximum path length as fully connected layers, making it suitable for long-range dependencies modeling. Compared to fully connected layers, it is more parameterefficient and more flexible in handling variable-length inputs. 
2. Due to the limited receptive field of convolutional layers, one typically needs to stack a deep network to have a global receptive field. On the other hand, the constant maximum path length enables self-attention to model long-range dependencies with a constant number of layers. 
3. The constant sequential operations and maximum path length make self-attention more parallelizable and better at long-range modeling than recurrent layers.

ä½œä¸ºTransformerçš„ä¸€ä¸ªæ ¸å¿ƒéƒ¨åˆ†ï¼Œè‡ªæ³¨æ„å…·æœ‰å¤„ç†å¯å˜é•¿åº¦è¾“å…¥çš„çµæ´»æœºåˆ¶ã€‚å®ƒå¯ä»¥è¢«ç†è§£ä¸ºä¸€ä¸ªå®Œå…¨è¿æ¥çš„å±‚ï¼Œå…¶ä¸­æƒé‡æ˜¯ä»è¾“å…¥çš„æˆå¯¹å…³ç³»ä¸­åŠ¨æ€ç”Ÿæˆçš„ã€‚è¡¨2æ¯”è¾ƒäº†ä¸‰ç§å¸¸ç”¨å±‚ç±»å‹çš„è‡ªæ³¨æ„çš„å¤æ‚æ€§ã€é¡ºåºæ“ä½œå’Œæœ€å¤§è·¯å¾„é•¿åº¦(4ä¸ºäº†ä»ä»»ä½•è¾“å…¥ä½ç½®åˆ°è¾¾ä»»æ„è¾“å‡ºä½ç½®ï¼Œå‰å‘å’Œåå‘ä¿¡å·å¿…é¡»ç»è¿‡çš„è·¯å¾„çš„æœ€å¤§é•¿åº¦ã€‚æ›´çŸ­çš„é•¿åº¦æ„å‘³ç€å­¦ä¹ é•¿æœŸä¾èµ–æ€§çš„å¯èƒ½æ€§æ›´å¤§)ã€‚æˆ‘ä»¬æ€»ç»“äº†è‡ªæ³¨æ„çš„ä¼˜ç‚¹å¦‚ä¸‹ï¼š
1. å®ƒå…·æœ‰ä¸å®Œå…¨è¿æ¥å±‚ç›¸åŒçš„æœ€å¤§è·¯å¾„é•¿åº¦ï¼Œä½¿å…¶é€‚åˆäºé•¿è·ç¦»ä¾èµ–å…³ç³»å»ºæ¨¡ã€‚ä¸å®Œå…¨è¿æ¥çš„å±‚ç›¸æ¯”ï¼Œå®ƒåœ¨å¤„ç†å¯å˜é•¿åº¦è¾“å…¥æ—¶æ›´å…·å‚æ•°æ•ˆç‡å’Œçµæ´»æ€§ã€‚
2. ç”±äºå·ç§¯å±‚çš„æ¥æ”¶åœºæœ‰é™ï¼Œé€šå¸¸éœ€è¦å †å æ·±åº¦ç½‘ç»œä»¥å…·æœ‰å…¨å±€æ¥æ”¶åœºã€‚å¦ä¸€æ–¹é¢ï¼Œæ’å®šçš„æœ€å¤§è·¯å¾„é•¿åº¦ä½¿è‡ªå·±èƒ½å¤Ÿæ³¨æ„ä½¿ç”¨æ’å®šæ•°é‡çš„å±‚æ¥å»ºæ¨¡é•¿æœŸä¾èµ–å…³ç³»ã€‚
3. æ’å®šçš„é¡ºåºæ“ä½œå’Œæœ€å¤§è·¯å¾„é•¿åº¦ä½¿è‡ªæ³¨æ„æ¯”å¾ªç¯å±‚æ›´å…·å¹¶è¡Œæ€§ï¼Œå¹¶ä¸”æ›´æ“…é•¿è¿œç¨‹å»ºæ¨¡ã€‚

Table 2. Per-layer complexity, minimum number of sequential operations and maximum path lengths for different layer types. ğ‘‡ is the sequence length, ğ· is the representation dimension and ğ¾ is the kernel size of convolutions [137].
è¡¨2ã€‚ä¸åŒå±‚ç±»å‹çš„æ¯å±‚å¤æ‚æ€§ã€æœ€å°é¡ºåºæ“ä½œæ•°å’Œæœ€å¤§è·¯å¾„é•¿åº¦ã€‚ğ‘‡ æ˜¯åºåˆ—é•¿åº¦ï¼Œğ· æ˜¯è¡¨ç¤ºå°ºå¯¸ğ¾ æ˜¯å·ç§¯çš„æ ¸å¤§å°[137]ã€‚

#### 2.4.2 In Terms of Inductive Bias.  
Transformer is often compared against convolutional and recurrent networks. Convolutional networks are known to impose the inductive biases of translation invariance and locality with shared local kernel functions. Similarly, recurrent networks carry the inductive biases of temporal invariance and locality via their Markovian structure [9]. On the other hand, the Transformer architecture makes few assumptions about structural information of data. This makes Transformer a universal and flexible architecture. As a side effect, the lack of structural bias makes Transformer prone to overfitting for small-scale data.

å°±å½’çº³åå·®è€Œè¨€ï¼ŒTransformerç»å¸¸ä¸å·ç§¯å’Œå¾ªç¯ç½‘ç»œè¿›è¡Œæ¯”è¾ƒã€‚å·²çŸ¥å·ç§¯ç½‘ç»œåˆ©ç”¨å…±äº«çš„å±€éƒ¨æ ¸å‡½æ•°æ–½åŠ å¹³ç§»ä¸å˜æ€§å’Œå±€éƒ¨æ€§çš„å½’çº³åå·®ã€‚ç±»ä¼¼åœ°ï¼Œå¾ªç¯ç½‘ç»œé€šè¿‡å…¶é©¬å°”å¯å¤«ç»“æ„æºå¸¦æ—¶é—´ä¸å˜æ€§å’Œå±€éƒ¨æ€§çš„å½’çº³åå·®[9]ã€‚å¦ä¸€æ–¹é¢ï¼ŒTransformeræ¶æ„å¾ˆå°‘å¯¹æ•°æ®çš„ç»“æ„ä¿¡æ¯è¿›è¡Œå‡è®¾ã€‚è¿™ä½¿Transformeræˆä¸ºä¸€ç§é€šç”¨ä¸”çµæ´»çš„æ¶æ„ã€‚ä½œä¸ºä¸€ä¸ªå‰¯ä½œç”¨ï¼Œç»“æ„åå·®çš„ç¼ºä¹ä½¿å¾—Transformerå®¹æ˜“å¯¹å°è§„æ¨¡æ•°æ®è¿›è¡Œè¿‡åº¦æ‹Ÿåˆã€‚

Another closely related network type is Graph Neural Networks (GNNs) with message passing [149]. Transformer can be viewed as a GNN defined over a complete directed graph (with self-loop) where each input is a node in the graph. The key difference between Transformer and GNNs is that Transformer introduces no prior knowledge over how input data are structured â€” the message passing process in Transformer solely depends on similarity measures over the content. 

å¦ä¸€ç§å¯†åˆ‡ç›¸å…³çš„ç½‘ç»œç±»å‹æ˜¯å…·æœ‰æ¶ˆæ¯ä¼ é€’çš„å›¾å½¢ç¥ç»ç½‘ç»œ(GNN)[149]ã€‚Transformerå¯ä»¥è¢«è§†ä¸ºåœ¨å®Œæ•´æœ‰å‘å›¾(å¸¦è‡ªç¯)ä¸Šå®šä¹‰çš„GNNï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥éƒ½æ˜¯å›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ã€‚Transformerå’ŒGNNä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºï¼ŒTransformeræ²¡æœ‰å¼•å…¥å…³äºè¾“å…¥æ•°æ®ç»“æ„çš„å…ˆéªŒçŸ¥è¯† â€”â€” Transformerä¸­çš„æ¶ˆæ¯ä¼ é€’è¿‡ç¨‹ä»…å–å†³äºå†…å®¹çš„ç›¸ä¼¼æ€§åº¦é‡ã€‚

## 3 TAXONOMY OF TRANSFORMERS Transformeråˆ†ç±»
A wide variety of models have been proposed so far based on the vanilla Transformer from three perspectives: types of architecture modification, pre-training methods, and applications. Fig. 2 gives an illustrations of our categorization of Transformer variants.

åˆ°ç›®å‰ä¸ºæ­¢ï¼ŒåŸºäºvanillaTransformerä»ä¸‰ä¸ªè§’åº¦æå‡ºäº†å„ç§å„æ ·çš„æ¨¡å‹ï¼šæ¶æ„ä¿®æ”¹ç±»å‹ã€é¢„è®­ç»ƒæ–¹æ³•å’Œåº”ç”¨ç¨‹åºã€‚å›¾2è¯´æ˜äº†æˆ‘ä»¬å¯¹Transformerå˜ä½“çš„åˆ†ç±»ã€‚

Fig. 2. Categorization of Transformer variants.
å›¾2ã€‚Transformerå˜ä½“åˆ†ç±»ã€‚

Fig. 3. Taxonomy of Transformers

Fig. 3 illustrates our taxonomy and some representative models.
å›¾3è¯´æ˜äº†æˆ‘ä»¬çš„åˆ†ç±»å’Œä¸€äº›ä»£è¡¨æ€§æ¨¡å‹ã€‚

In this survey, we focus on reviewing the works on architecture modifications. Since the attention module is the key component of Transformer, we solely describe the attention-related variants in Sec. 4 and introduce the other module-level variants in Sec. 5. Then Sec. 6 describes the other architecture-level variants. Finally, we briefly review the works on pre-training in Sec. 7 and applications in Sec. 8. There are some comprehensive surveys on the latter two categories of work, such as pre-trained models (PTMs) [100] and visual Transformers[47, 64]. 

åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹å›é¡¾äº†æ¶æ„ä¿®æ”¹æ–¹é¢çš„å·¥ä½œã€‚ç”±äºæ³¨æ„åŠ›æ¨¡å—æ˜¯Transformerçš„å…³é”®ç»„ä»¶ï¼Œæˆ‘ä»¬åœ¨ç¬¬4èŠ‚ä¸­ä»…æè¿°äº†ä¸æ³¨æ„åŠ›ç›¸å…³çš„å˜ä½“ï¼Œå¹¶åœ¨ç¬¬5èŠ‚ä¸­ä»‹ç»äº†å…¶ä»–æ¨¡å—çº§å˜ä½“ã€‚ç„¶åï¼Œç¬¬6èŠ‚æè¿°äº†å…¶ä»–æ¶æ„çº§å˜ä½“ã€‚æœ€åï¼Œæˆ‘ä»¬ç®€è¦å›é¡¾äº†ç¬¬7èŠ‚ä¸­çš„é¢„è®­ç»ƒå·¥ä½œå’Œç¬¬8èŠ‚ä¸­çš„åº”ç”¨ã€‚å¯¹åä¸¤ç±»å·¥ä½œè¿›è¡Œäº†ä¸€äº›å…¨é¢çš„è°ƒæŸ¥ï¼Œå¦‚é¢„è®­ç»ƒæ¨¡å‹(PTM)[100]å’Œè§†è§‰Transformer[47ï¼Œ64]ã€‚

## 4 ATTENTION
Self-attention plays an important role in Transformer, but there are two challenges in practical applications. 
1.  Complexity. As discussion in Sec. 2.3, the complexity of self-attention is O (ğ‘‡ 2 Â· ğ·). Therefore, the attention module becomes a bottleneck when dealing with long sequences. 
2.  Structural prior. Self-attention does no assume any structural bias over inputs. Even the order information is also needed to be learned from training data. Therefore, Transformer (w/o pre-training) is usually easy to overfit on small or moderate-size data.

è‡ªæ³¨æ„åœ¨Transformerä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œä½†åœ¨å®é™…åº”ç”¨ä¸­å­˜åœ¨ä¸¤ä¸ªæŒ‘æˆ˜ã€‚
1. å¤æ‚æ€§ã€‚å¦‚ç¬¬2.3èŠ‚æ‰€è¿°ï¼Œè‡ªæ³¨æ„çš„å¤æ‚æ€§ä¸ºO(ğ‘‡ 2 Â· ğ·). å› æ­¤ï¼Œæ³¨æ„åŠ›æ¨¡å—åœ¨å¤„ç†é•¿åºåˆ—æ—¶æˆä¸ºç“¶é¢ˆã€‚
2. ç»“æ„ä¼˜å…ˆã€‚è‡ªæ³¨æ„ä¸ä¼šå¯¹è¾“å…¥äº§ç”Ÿä»»ä½•ç»“æ„æ€§åè§ã€‚ç”šè‡³orderä¿¡æ¯ä¹Ÿéœ€è¦ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ ã€‚å› æ­¤ï¼ŒTransformer(ä¸å¸¦é¢„è®­ç»ƒ)é€šå¸¸å¾ˆå®¹æ˜“è¿‡åº¦å¡«å……å°æˆ–ä¸­ç­‰å¤§å°çš„æ•°æ®ã€‚

The improvements on attention mechanism can be divided into several directions: 
1.  Sparse Attention. This line of work introduces sparsity bias into the attention mechanism, leading to reduced complexity. 
2.  Linearized Attention. This line of work disentangles the attention matrix with kernel feature maps. The attention is then computed in reversed order to achieve linear complexity.
3.  Prototype and Memory Compression. This class of methods reduces the number of queries or key-value memory pairs to reduce the size of the attention matrix. 
4.  Low-rank Self-Attention. This line of work capture the low-rank property of self-attention. 
5.  Attention with Prior. The line of research explores supplementing or substituting standard attention with prior attention distributions. 
6.  Improved Multi-Head Mechanism. The line of studies explores different alternative multi-head mechanisms.

æ³¨æ„åŠ›æœºåˆ¶çš„æ”¹è¿›å¯åˆ†ä¸ºå‡ ä¸ªæ–¹å‘ï¼š
1. åˆ†æ•£æ³¨æ„åŠ›ã€‚è¿™é¡¹å·¥ä½œå°†ç¨€ç–æ€§åå·®å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼Œä»è€Œé™ä½äº†å¤æ‚æ€§ã€‚
2. çº¿æ€§æ³¨æ„ã€‚è¿™ä¸€è¡Œå·¥ä½œå°†æ³¨æ„åŠ›çŸ©é˜µä¸æ ¸å¿ƒç‰¹å¾å›¾è§£å¼€ã€‚ç„¶åä»¥ç›¸åçš„é¡ºåºè®¡ç®—æ³¨æ„åŠ›ï¼Œä»¥å®ç°çº¿æ€§å¤æ‚æ€§ã€‚
3. åŸå‹å’Œè®°å¿†å‹ç¼©ã€‚è¿™ç±»æ–¹æ³•å‡å°‘äº†æŸ¥è¯¢æˆ–é”®å€¼è®°å¿†å¯¹çš„æ•°é‡ï¼Œä»¥å‡å°‘æ³¨æ„åŠ›çŸ©é˜µçš„å¤§å°ã€‚
4. ä½çº§è‡ªæ³¨æ„ã€‚è¿™é¡¹å·¥ä½œæŠ“ä½äº†è‡ªæ³¨æ„çš„ä½çº§å±æ€§ã€‚
5. æ³¨æ„Priorã€‚è¯¥ç ”ç©¶çº¿æ¢ç´¢ç”¨å…ˆå‰çš„æ³¨æ„åŠ›åˆ†å¸ƒæ¥è¡¥å……æˆ–æ›¿ä»£æ ‡å‡†æ³¨æ„åŠ›ã€‚
6. æ”¹è¿›çš„å¤šå¤´æœºæ„ã€‚è¯¥ç³»åˆ—ç ”ç©¶æ¢ç´¢äº†ä¸åŒçš„æ›¿ä»£å¤šå¤´æœºåˆ¶ã€‚

We will describe these attention variants at length in the rest of this section.

æˆ‘ä»¬å°†åœ¨æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†è¯¦ç»†æè¿°è¿™äº›æ³¨æ„åŠ›å˜ä½“ã€‚

### 4.1 Sparse Attention åˆ†æ•£æ³¨æ„åŠ›
In the standard self-attention mechanism, every token needs to attend to all other tokens. However, it is observed that for the trained Transformers the learned attention matrix A is often very sparse across most data points [17]. Therefore, it is possible to reduce computation complexity by incorporating structural bias to limit the number of query-key pairs that each query attends to. Under this limitation, we just compute the similarity score of the query-key pairs according to pre-defined patterns 

åœ¨æ ‡å‡†çš„è‡ªæ³¨æ„æœºåˆ¶ä¸­ï¼Œæ¯ä¸ªä»¤ç‰Œéƒ½éœ€è¦å…³æ³¨æ‰€æœ‰å…¶ä»–ä»¤ç‰Œã€‚ç„¶è€Œï¼Œè§‚å¯Ÿåˆ°ï¼Œå¯¹äºè®­ç»ƒçš„Transformersï¼Œå­¦ä¹ çš„æ³¨æ„åŠ›çŸ©é˜µAåœ¨å¤§å¤šæ•°æ•°æ®ç‚¹ä¸Šé€šå¸¸éå¸¸ç¨€ç–[17]ã€‚å› æ­¤ï¼Œé€šè¿‡å¼•å…¥ç»“æ„åå·®æ¥é™åˆ¶æ¯ä¸ªæŸ¥è¯¢æ‰€å…³æ³¨çš„æŸ¥è¯¢å…³é”®å­—å¯¹çš„æ•°é‡ï¼Œå¯ä»¥é™ä½è®¡ç®—å¤æ‚æ€§

Ë†Ağ‘–ğ‘— = ( qğ‘–kâŠ¤ğ‘— if token ğ‘– attends to token ğ‘—, âˆ’âˆ if token ğ‘– does not attend to token ğ‘—, (7) 

where Ë†A is un-normalized attention matrix. In implementation the âˆ’âˆ item is usually not stored in memory so as to decrease memory footprint.

å…¶ä¸­ï¼ŒAæ˜¯æœªå½’ä¸€åŒ–çš„æ³¨æ„åŠ›çŸ©é˜µã€‚åœ¨å®ç°ä¸­ï¼Œ-âˆé¡¹é€šå¸¸ä¸å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œä»¥å‡å°‘å†…å­˜å ç”¨ã€‚

From another perspective, the standard attention can be regarded as a complete bipartite graph where each query receives information from all memory nodes and updates its representation. The sparse attention can be considered as a sparse graph where some of the connections between nodes are removed.

ä»å¦ä¸€ä¸ªè§’åº¦æ¥çœ‹ï¼Œæ ‡å‡†æ³¨æ„åŠ›å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ªå®Œæ•´çš„äºŒåˆ†å›¾ï¼Œå…¶ä¸­æ¯ä¸ªæŸ¥è¯¢ä»æ‰€æœ‰å†…å­˜èŠ‚ç‚¹æ¥æ”¶ä¿¡æ¯å¹¶æ›´æ–°å…¶è¡¨ç¤ºã€‚ç¨€ç–æ³¨æ„åŠ›å¯ä»¥è¢«è§†ä¸ºç¨€ç–å›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹ä¹‹é—´çš„ä¸€äº›è¿æ¥è¢«ç§»é™¤ã€‚

Based on the metrics of determining the sparse connection, we categorize these approaches into two classes: position-based and content-based sparse attention.

åŸºäºç¡®å®šç¨€ç–è¿æ¥çš„åº¦é‡ï¼Œæˆ‘ä»¬å°†è¿™äº›æ–¹æ³•åˆ†ä¸ºä¸¤ç±»ï¼šåŸºäºä½ç½®çš„ç¨€ç–å…³æ³¨å’ŒåŸºäºå†…å®¹çš„ç¨€ç–å…³æ³¨ã€‚

#### 4.1.1 Position-based Sparse Attention. åŸºäºä½ç½®çš„åˆ†æ•£æ³¨æ„åŠ›
In position-based sparse attention, the attention matrix is limited according to some pre-defined patterns. Although these sparse patterns vary in different forms, we find that some of them can be decomposed into some atomic sparse patterns.

åœ¨åŸºäºä½ç½®çš„ç¨€ç–æ³¨æ„åŠ›ä¸­ï¼Œæ³¨æ„åŠ›çŸ©é˜µæ ¹æ®ä¸€äº›é¢„å®šä¹‰çš„æ¨¡å¼å—åˆ°é™åˆ¶ã€‚å°½ç®¡è¿™äº›ç¨€ç–æ¨¡å¼çš„å½¢å¼ä¸åŒï¼Œä½†æˆ‘ä»¬å‘ç°å…¶ä¸­ä¸€äº›å¯ä»¥åˆ†è§£ä¸ºä¸€äº›åŸå­ç¨€ç–æ¨¡å¼ã€‚

We first identify some atomic sparse patterns and then describe how these patterns are composed in some existing work. Finally, we introduce some extended sparse patterns for specific data types.

æˆ‘ä»¬é¦–å…ˆè¯†åˆ«ä¸€äº›åŸå­ç¨€ç–æ¨¡å¼ï¼Œç„¶åæè¿°è¿™äº›æ¨¡å¼æ˜¯å¦‚ä½•åœ¨ç°æœ‰å·¥ä½œä¸­ç»„æˆçš„ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸ºç‰¹å®šçš„æ•°æ®ç±»å‹å¼•å…¥äº†ä¸€äº›æ‰©å±•çš„ç¨€ç–æ¨¡å¼ã€‚

#### 4.1.1.1 Atomic Sparse Attention.  åŸå­ç¨€ç–æ³¨æ„
There are mainly five types of atomic sparse attention patterns, as shown in Fig. 4. 
1.  Global Attention. To alleviate the degradation of the ability to model the long-range dependencies in sparse attention, one can add some global nodes(5In practice, these global nodes can be selected from the sequence (internal global nodes) or virtual nodes with trainable parameters (external global nodes).) as the hub for information propagation between nodes. These global nodes can attend all nodes in the sequence and the whole sequence attend to these global nodes, as illustrated in Fig. 4(a). 
2.  Band Attention(a.k.a sliding window attention or local attention). Since most data come with a strong property of locality, it is natural to restrict each query to attend to its neighbor nodes. A widely adopted class of such sparse pattern is band attention, in which the attention matrix is a band matrix as illustrated in Fig. 4(b). 
3.  Dilated Attention. Analogous to dilated CNNs [134], one can potentially increase the receptive field of the band attention without increasing computation complexity by using a dilated window with gaps of dilation ğ‘¤ğ‘‘ â‰¥ 1, as depicted in Fig. 4(c). This can be easily extended to strided attention, where the window size is not limited but the dilation ğ‘¤ğ‘‘ is set to a large value. 
4.  Random Attention. To increase the ability of non-local interactions, a few edges are randomly sampled for each query, as illustrated in Fig. 4(d). This is based on the observation that random graphs (e.g., ErdÅ‘sâ€“RÃ©nyi random graph) can have similar spectral properties with complete graphs that leads to a fast mixing time for random walking on graphs. 
5.  Block Local Attention. This class of attention segments input sequence into several nonoverlapping query blocks, each of which is associated with a local memory block. All the queries in a query block attend to only the keys in the corresponding memory block. Fig. 4(e) depicts a commonly used case where the memory blocks are identical to their corresponding query blocks.

ä¸»è¦æœ‰äº”ç§ç±»å‹çš„åŸå­ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ï¼Œå¦‚å›¾4æ‰€ç¤ºã€‚
1. å…¨å±€å…³æ³¨ã€‚ä¸ºäº†ç¼“è§£ç¨€ç–æ³¨æ„åŠ›ä¸­å¯¹é•¿è·ç¦»ä¾èµ–æ€§å»ºæ¨¡èƒ½åŠ›çš„é€€åŒ–ï¼Œå¯ä»¥æ·»åŠ ä¸€äº›å…¨å±€èŠ‚ç‚¹(5å®é™…ä¸Šï¼Œè¿™äº›å…¨å±€èŠ‚ç‚¹å¯ä»¥ä»åºåˆ—(å†…éƒ¨å…¨å±€èŠ‚ç‚¹)æˆ–å…·æœ‰å¯è®­ç»ƒå‚æ•°çš„è™šæ‹ŸèŠ‚ç‚¹(å¤–éƒ¨å…¨å±€èŠ‚ç‚¹)ä¸­é€‰æ‹©)ä½œä¸ºèŠ‚ç‚¹ä¹‹é—´ä¿¡æ¯ä¼ æ’­çš„ä¸­å¿ƒã€‚è¿™äº›å…¨å±€èŠ‚ç‚¹å¯ä»¥å‚ä¸åºåˆ—ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ•´ä¸ªåºåˆ—å‚ä¸è¿™äº›å…¨å±€èŠ‚ç‚¹ï¼Œå¦‚å›¾4(a)æ‰€ç¤ºã€‚
2. Bandæ³¨æ„(åˆç§°æ»‘åŠ¨çª—å£æ³¨æ„æˆ–å±€éƒ¨æ³¨æ„)ã€‚ç”±äºå¤§å¤šæ•°æ•°æ®éƒ½å…·æœ‰å¾ˆå¼ºçš„å±€éƒ¨æ€§ï¼Œå› æ­¤è‡ªç„¶ä¼šå°†æ¯ä¸ªæŸ¥è¯¢é™åˆ¶åœ¨å…¶ç›¸é‚»èŠ‚ç‚¹ä¸Šã€‚è¿™ç§ç¨€ç–æ¨¡å¼çš„ä¸€ä¸ªå¹¿æ³›é‡‡ç”¨çš„ç±»åˆ«æ˜¯é¢‘å¸¦æ³¨æ„ï¼Œå…¶ä¸­æ³¨æ„çŸ©é˜µæ˜¯å¦‚å›¾4(b)æ‰€ç¤ºçš„é¢‘å¸¦çŸ©é˜µã€‚
3. æ³¨æ„åŠ›åˆ†æ•£ã€‚ç±»ä¼¼äºæ‰©å¼ çš„ç¥ç»ç½‘ç»œ[134]ï¼Œé€šè¿‡ä½¿ç”¨å…·æœ‰æ‰©å¼ é—´éš™çš„æ‰©å¼ çª—å£ï¼Œå¯ä»¥æ½œåœ¨åœ°å¢åŠ é¢‘å¸¦æ³¨æ„åŠ›çš„æ„Ÿå—é‡ï¼Œè€Œä¸å¢åŠ è®¡ç®—å¤æ‚æ€§ğ‘¤ğ‘‘ â‰¥ å¦‚å›¾4(c)æ‰€ç¤ºã€‚è¿™å¯ä»¥å¾ˆå®¹æ˜“åœ°æ‰©å±•åˆ°è·¨è¶Šå¼çš„æ³¨æ„åŠ›ï¼Œå…¶ä¸­çª—å£å¤§å°ä¸å—é™åˆ¶ï¼Œè€Œæ˜¯æ‰©å¼ ğ‘¤ğ‘‘ è®¾ç½®ä¸ºå¤§å€¼ã€‚
4. éšæœºæ³¨æ„ã€‚ä¸ºäº†æé«˜éå±€éƒ¨äº¤äº’çš„èƒ½åŠ›ï¼Œä¸ºæ¯ä¸ªæŸ¥è¯¢éšæœºé‡‡æ ·ä¸€äº›è¾¹ç¼˜ï¼Œå¦‚å›¾4(d)æ‰€ç¤ºã€‚è¿™æ˜¯åŸºäºéšæœºå›¾(ä¾‹å¦‚ErdÃ¥sâ€“RÃ©nyiéšæœºå›¾)å¯ä»¥å…·æœ‰ä¸å®Œæ•´å›¾ç›¸ä¼¼çš„å…‰è°±ç‰¹æ€§çš„è§‚å¯Ÿï¼Œè¿™å¯¼è‡´äº†å›¾ä¸Šéšæœºè¡Œèµ°çš„å¿«é€Ÿæ··åˆæ—¶é—´ã€‚
5. é˜»æ­¢æœ¬åœ°æ³¨æ„ã€‚è¿™ç±»æ³¨æ„åŠ›å°†è¾“å…¥åºåˆ—åˆ†å‰²æˆå‡ ä¸ªä¸é‡å çš„æŸ¥è¯¢å—ï¼Œæ¯ä¸ªæŸ¥è¯¢å—éƒ½ä¸ä¸€ä¸ªæœ¬åœ°å†…å­˜å—ç›¸å…³è”ã€‚ä¸€ä¸ªæŸ¥è¯¢å—ä¸­çš„æ‰€æœ‰æŸ¥è¯¢éƒ½åªå…³æ³¨ç›¸åº”å†…å­˜å—ä¸­çš„é”®ã€‚å›¾4(e)æè¿°äº†ä¸€ç§å¸¸è§çš„æƒ…å†µï¼Œå…¶ä¸­å­˜å‚¨å—ä¸å…¶å¯¹åº”çš„æŸ¥è¯¢å—ç›¸åŒã€‚

Fig. 4. Some representative atomic sparse attention patterns. The colored squares means corresponding attention scores are calculated and a blank square means the attention score is discarded. 
å›¾4ã€‚ä¸€äº›ä»£è¡¨æ€§çš„åŸå­ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼ã€‚å½©è‰²æ–¹å—è¡¨ç¤ºè®¡ç®—ç›¸åº”çš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œç©ºç™½æ–¹å—è¡¨ç¤ºä¸¢å¼ƒæ³¨æ„åŠ›å¾—åˆ†ã€‚


#### 4.1.1.2 Compound Sparse Attention. å¤åˆåˆ†æ•£æ³¨æ„åŠ›
Existing sparse attentions are often composed of more than one of the above atomic patterns. Fig. 5 illustrates some representative compound sparse attention patterns. 

ç°æœ‰çš„ç¨€ç–å…³æ³¨é€šå¸¸ç”±ä»¥ä¸ŠåŸå­æ¨¡å¼ä¸­çš„ä¸€ç§ä»¥ä¸Šç»„æˆã€‚å›¾5è¯´æ˜äº†ä¸€äº›å…¸å‹çš„å¤åˆç¨€ç–æ³¨æ„æ¨¡å¼ã€‚

Fig. 5. Some representative compound sparse attention patterns. The red boxes indicate sequence boundaries.
å›¾5ã€‚ä¸€äº›æœ‰ä»£è¡¨æ€§çš„å¤åˆç¨€ç–æ³¨æ„æ¨¡å¼ã€‚çº¢è‰²æ¡†è¡¨ç¤ºåºåˆ—è¾¹ç•Œã€‚

Star-Transformer [43] uses a combination of band attention and global attention. Specifically, Star-Transformer just includes only a global node and a band attention with the width of 3, in which any pair of non-adjacent nodes are connected through a shared global node and adjacent nodes are connected directly with each other. This kind of sparse pattern forms a star-shaped graph among nodes. Longformer [10] uses a combination of band attention and internal global-node attention. The global nodes are chosen to be [CLS] token for classification and all question tokens 10  for Question Answering tasks. They also replace some of the band attention heads in upper layers with dilated window attention to increase the receptive field without increasing computation. As a concurrent work to Longformer [10], Extended Transformer Construction (ETC) [1] utilizes combination of band attention and external global-node attention. ETC also includes a masking mechanism to handle structured inputs and adapt Contrastive Predictive Coding (CPC) [135] for pre-training. In addition to the band and global attention, BigBird [163] uses additional random attention to approximate full attention. Their theoretical analysis also reveals that the usage of a sparse encoder and sparse decoder can simulate any Turing Machine, which explains the success of those sparse attention models.

Star-Transformer[43]ä½¿ç”¨äº†bandå…³æ³¨å’Œå…¨å±€å…³æ³¨çš„ç»„åˆã€‚å…·ä½“è€Œè¨€ï¼ŒStar-Transformerä»…åŒ…æ‹¬ä¸€ä¸ªå…¨å±€èŠ‚ç‚¹å’Œå®½åº¦ä¸º3çš„é¢‘å¸¦å…³æ³¨ï¼Œå…¶ä¸­ä»»ä½•ä¸€å¯¹éç›¸é‚»èŠ‚ç‚¹é€šè¿‡å…±äº«å…¨å±€èŠ‚ç‚¹è¿æ¥ï¼Œç›¸é‚»èŠ‚ç‚¹å½¼æ­¤ç›´æ¥è¿æ¥ã€‚è¿™ç§ç¨€ç–æ¨¡å¼åœ¨èŠ‚ç‚¹ä¹‹é—´å½¢æˆæ˜Ÿå½¢å›¾ã€‚Longformer[10]ä½¿ç”¨äº†é¢‘å¸¦æ³¨æ„åŠ›å’Œå†…éƒ¨å…¨å±€èŠ‚ç‚¹æ³¨æ„åŠ›çš„ç»„åˆã€‚å…¨å±€èŠ‚ç‚¹è¢«é€‰æ‹©ä¸º[CLS]æ ‡è®°ç”¨äºåˆ†ç±»ï¼Œæ‰€æœ‰é—®é¢˜æ ‡è®°10Linet al ç”¨äºé—®ç­”ä»»åŠ¡ã€‚ä»–ä»¬è¿˜ç”¨æ‰©å¼ çš„çª—å£æ³¨æ„åŠ›å–ä»£äº†ä¸Šå±‚çš„ä¸€äº›å¸¦æ³¨æ„åŠ›å¤´ï¼Œä»¥å¢åŠ æ„Ÿå—é‡è€Œä¸å¢åŠ è®¡ç®—ã€‚ä½œä¸ºLongformer[10]çš„å¹¶è¡Œå·¥ä½œï¼Œæ‰©å±•Transformeræ„é€ (ETC)[1]åˆ©ç”¨äº†é¢‘å¸¦å…³æ³¨å’Œå¤–éƒ¨å…¨å±€èŠ‚ç‚¹å…³æ³¨çš„ç»„åˆã€‚ETCè¿˜åŒ…æ‹¬ä¸€ç§æ©ç æœºåˆ¶ï¼Œç”¨äºå¤„ç†ç»“æ„åŒ–è¾“å…¥å¹¶è°ƒæ•´å¯¹æ¯”é¢„æµ‹ç¼–ç (CPC)[135]ä»¥è¿›è¡Œé¢„è®­ç»ƒã€‚é™¤äº†ä¹é˜Ÿå’Œå…¨å±€æ³¨æ„åŠ›ä¹‹å¤–ï¼ŒBigBird[163]è¿˜ä½¿ç”¨é¢å¤–çš„éšæœºæ³¨æ„åŠ›æ¥è¿‘ä¼¼å®Œå…¨æ³¨æ„åŠ›ã€‚ä»–ä»¬çš„ç†è®ºåˆ†æè¿˜æ­ç¤ºäº†ç¨€ç–ç¼–ç å™¨å’Œç¨€ç–è§£ç å™¨çš„ä½¿ç”¨å¯ä»¥æ¨¡æ‹Ÿä»»ä½•å›¾çµæœºï¼Œè¿™è§£é‡Šäº†è¿™äº›ç¨€ç–æ³¨æ„åŠ›æ¨¡å‹çš„æˆåŠŸã€‚

Sparse Transformer [17] uses a factorized attention where different sparse patterns are designed for different types of data. For data with a periodic structure (e.g., images), it uses a composition of band attention and strided attention. Whereas for data without a periodic structure (e.g., text), it uses a composition of block local attention combined with global attention, where global nodes are from fixed positions in the input sequence.

ç¨€ç–å˜æ¢å™¨[17]ä½¿ç”¨å› å­åŒ–æ³¨æ„åŠ›ï¼Œå…¶ä¸­ä¸ºä¸åŒç±»å‹çš„æ•°æ®è®¾è®¡äº†ä¸åŒçš„ç¨€ç–æ¨¡å¼ã€‚å¯¹äºå…·æœ‰å‘¨æœŸæ€§ç»“æ„çš„æ•°æ®(ä¾‹å¦‚ï¼Œå›¾åƒ)ï¼Œå®ƒä½¿ç”¨æ³¢æ®µæ³¨æ„åŠ›å’Œè·¨æ­¥æ³¨æ„åŠ›çš„ç»„åˆã€‚è€Œå¯¹äºæ²¡æœ‰å‘¨æœŸæ€§ç»“æ„çš„æ•°æ®(ä¾‹å¦‚æ–‡æœ¬)ï¼Œå®ƒä½¿ç”¨å—å±€éƒ¨å…³æ³¨ä¸å…¨å±€å…³æ³¨ç›¸ç»“åˆçš„ç»„åˆï¼Œå…¶ä¸­å…¨å±€èŠ‚ç‚¹æ¥è‡ªè¾“å…¥åºåˆ—ä¸­çš„å›ºå®šä½ç½®ã€‚

#### 4.1.1.3 Extended Sparse Attention. åˆ†æ•£æ³¨æ„åŠ›
Apart from the above patterns, some existing studies have explored extended sparse patterns for specific data types.

é™¤ä¸Šè¿°æ¨¡å¼å¤–ï¼Œä¸€äº›ç°æœ‰ç ”ç©¶è¿˜æ¢ç´¢äº†ç‰¹å®šæ•°æ®ç±»å‹çš„æ‰©å±•ç¨€ç–æ¨¡å¼ã€‚

For text data, BP-Transformer [158] constructs a binary tree where all tokens are leaf nodes and the internal nodes are span nodes containing many tokens. The edges in this graph are constructed so that each leaf node is connected to its neighbor leaf nodes and higher-level span nodes containing tokens from a longer distance. This approach can be seen as an extension of global attention, where global nodes are hierarchically organized and any pair of tokens are connected with paths in the binary tree. An abstract view of this method is illustrated in Fig. 6(a).

å¯¹äºæ–‡æœ¬æ•°æ®ï¼ŒBP-Transformer[158]æ„å»ºäº†ä¸€ä¸ªäºŒå‰æ ‘ï¼Œå…¶ä¸­æ‰€æœ‰æ ‡è®°éƒ½æ˜¯å¶èŠ‚ç‚¹ï¼Œå†…éƒ¨èŠ‚ç‚¹æ˜¯åŒ…å«è®¸å¤šæ ‡è®°çš„è·¨åº¦èŠ‚ç‚¹ã€‚æ­¤å›¾ä¸­çš„è¾¹è¢«æ„é€ ä¸ºä½¿å¾—æ¯ä¸ªå¶èŠ‚ç‚¹è¿æ¥åˆ°å…¶ç›¸é‚»å¶èŠ‚ç‚¹å’ŒåŒ…å«æ¥è‡ªæ›´è¿œè·ç¦»çš„ä»¤ç‰Œçš„æ›´é«˜çº§åˆ«è·¨åº¦èŠ‚ç‚¹ã€‚è¿™ç§æ–¹æ³•å¯ä»¥çœ‹ä½œæ˜¯å…¨å±€æ³¨æ„åŠ›çš„æ‰©å±•ï¼Œå…¶ä¸­å…¨å±€èŠ‚ç‚¹æ˜¯åˆ†å±‚ç»„ç»‡çš„ï¼Œä»»ä½•ä¸€å¯¹ä»¤ç‰Œéƒ½ä¸äºŒå‰æ ‘ä¸­çš„è·¯å¾„ç›¸è¿æ¥ã€‚è¯¥æ–¹æ³•çš„æŠ½è±¡è§†å›¾å¦‚å›¾6(a)æ‰€ç¤ºã€‚

There are also some extensions for vision data. Image Transformer [94] explores two types of attention: 1.  flattening image pixels in raster-scan order and then applying block local sparse attention. 2.  2D block local attention, where query blocks and memory blocks are arranged directly in 2D plate, as depicted in Fig. 6(b). As another example of sparse pattern on vision data, Axial Transformer [54] applies independent attention modules over each axis of the image. Each attention module mixes information along one axis while keeping information along the other axis independent, as illustrated in Fig. 6(c). This can be understood as horizontally and vertically flattening image pixels in raster-scan order and then applying strided attention with gaps of image width and height, respectively. 

è§†è§‰æ•°æ®ä¹Ÿæœ‰ä¸€äº›æ‰©å±•ã€‚Image Transformer[94]æ¢è®¨äº†ä¸¤ç§ç±»å‹çš„æ³¨æ„åŠ›ï¼š1ã€‚ä»¥å…‰æ …æ‰«æé¡ºåºå±•å¹³å›¾åƒåƒç´ ï¼Œç„¶ååº”ç”¨å—å±€éƒ¨ç¨€ç–å…³æ³¨ã€‚2.2Då—å±€éƒ¨å…³æ³¨ï¼Œå…¶ä¸­æŸ¥è¯¢å—å’Œå­˜å‚¨å—ç›´æ¥å¸ƒç½®åœ¨2Dæ¿ä¸­ï¼Œå¦‚å›¾6(b)æ‰€ç¤ºã€‚ä½œä¸ºè§†è§‰æ•°æ®ä¸Šç¨€ç–æ¨¡å¼çš„å¦ä¸€ä¸ªæ ·æœ¬ï¼Œè½´å‘å˜æ¢å™¨[54]åœ¨å›¾åƒçš„æ¯ä¸ªè½´ä¸Šåº”ç”¨ç‹¬ç«‹çš„æ³¨æ„åŠ›æ¨¡å—ã€‚å¦‚å›¾6(c)æ‰€ç¤ºï¼Œæ¯ä¸ªæ³¨æ„åŠ›æ¨¡å—æ²¿ç€ä¸€ä¸ªè½´æ··åˆä¿¡æ¯ï¼ŒåŒæ—¶ä¿æŒæ²¿ç€å¦ä¸€ä¸ªè½´çš„ä¿¡æ¯ç‹¬ç«‹ã€‚è¿™å¯ä»¥ç†è§£ä¸ºä»¥å…‰æ …æ‰«æé¡ºåºæ°´å¹³å’Œå‚ç›´åœ°å±•å¹³å›¾åƒåƒç´ ï¼Œç„¶ååˆ†åˆ«ä»¥å›¾åƒå®½åº¦å’Œé«˜åº¦çš„é—´éš™æ–½åŠ è·¨æ­¥æ³¨æ„åŠ›ã€‚

Fig. 6. Other types of sparse attentions. The red box indicates the query position, and the orange nodes/squares means corresponding tokens are attended to by the query.
å›¾6ã€‚å…¶ä»–ç±»å‹çš„ç¨€ç–å…³æ³¨ã€‚çº¢è‰²æ¡†è¡¨ç¤ºæŸ¥è¯¢ä½ç½®ï¼Œæ©™è‰²èŠ‚ç‚¹/æ­£æ–¹å½¢è¡¨ç¤ºæŸ¥è¯¢å…³æ³¨ç›¸åº”çš„ä»¤ç‰Œã€‚

#### 4.1.2 Content-based Sparse Attention.  åŸºäºå†…å®¹çš„æ³¨æ„åŠ›åˆ†æ•£
Another line of work creates a sparse graph based on input content, i.e., the sparse connections are conditioned on inputs.

å¦ä¸€é¡¹å·¥ä½œæ˜¯åŸºäºè¾“å…¥å†…å®¹åˆ›å»ºç¨€ç–å›¾ï¼Œå³ç¨€ç–è¿æ¥ä»¥è¾“å…¥ä¸ºæ¡ä»¶ã€‚

A straightforward way of constructing a content-based sparse graph is to select those keys that are likely to have large similarity scores with the given query. To efficiently construct the sparse graph, we can recur to Maximum Inner Product Search (MIPS) problem, where one tries to find the keys with maximum dot product with a query without computing all dot product terms. Routing Transformer [111] uses k-means clustering to cluster both queries {qğ‘– }ğ‘‡ğ‘–=1 and keys {kğ‘– }ğ‘‡ğ‘–= on the same set of centroid vectors {ğœ‡ğ‘– }ğ‘˜ğ‘–=1 . Each query only attends to the keys that belong to the same cluster. During training, the cluster centroid vectors are updated using the exponentially moving average of vectors assigned to it, divided by the exponentially moving average of cluster counts: 

æ„å»ºåŸºäºå†…å®¹çš„ç¨€ç–å›¾çš„ä¸€ç§ç®€å•æ–¹æ³•æ˜¯é€‰æ‹©é‚£äº›å¯èƒ½ä¸ç»™å®šæŸ¥è¯¢å…·æœ‰è¾ƒå¤§ç›¸ä¼¼æ€§åˆ†æ•°çš„å…³é”®å­—ã€‚ä¸ºäº†æœ‰æ•ˆåœ°æ„é€ ç¨€ç–å›¾ï¼Œæˆ‘ä»¬å¯ä»¥é‡å¤åˆ°æœ€å¤§å†…ç§¯æœç´¢(MIPS)é—®é¢˜ï¼Œå³åœ¨ä¸è®¡ç®—æ‰€æœ‰ç‚¹ç§¯é¡¹çš„æƒ…å†µä¸‹ï¼Œè¯•å›¾é€šè¿‡æŸ¥è¯¢æ‰¾åˆ°å…·æœ‰æœ€å¤§ç‚¹ç§¯çš„é”®ã€‚è·¯ç”±è½¬æ¢å™¨[111]ä½¿ç”¨k-meansèšç±»æ¥èšç±»ä¸¤ä¸ªæŸ¥è¯¢{qğ‘– }ğ‘‡ğ‘–=1å’Œé”®{kğ‘– }ğ‘‡ğ‘–= åœ¨åŒä¸€ç»„å½¢å¿ƒå‘é‡ä¸Š{ğœ‡ğ‘– }ğ‘˜ğ‘–=1.æ¯ä¸ªæŸ¥è¯¢åªå…³æ³¨å±äºåŒä¸€é›†ç¾¤çš„é”®ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œä½¿ç”¨åˆ†é…ç»™å®ƒçš„å‘é‡çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡å€¼é™¤ä»¥ç°‡è®¡æ•°çš„æŒ‡æ•°ç§»åŠ¨å‡å€¼æ¥æ›´æ–°ç°‡å½¢å¿ƒå‘é‡ï¼š

Ëœğœ‡ â† ğœ† Ëœğœ‡ + (1 âˆ’ ğœ†) Â© Â­ Â« âˆ‘ï¸ğ‘–:ğœ‡ (qğ‘–)=ğœ‡ qğ‘– + ğ‘—:ğœ‡âˆ‘ï¸(kğ‘—)=ğœ‡ kğ‘— Âª Â® Â¬ , (8) 

ğ‘ğœ‡ â† ğœ†ğ‘ğœ‡ + (1 âˆ’ ğœ†)|ğœ‡|, (9) 

ğœ‡ â† Ëœğœ‡ğ‘ğœ‡ , (10) 

where |ğœ‡| denotes the number of vectors currently in cluster ğœ‡ and ğœ† âˆˆ (0, 1) is a hyperparameter.

å…¶ä¸­|ğœ‡| è¡¨ç¤ºé›†ç¾¤ä¸­å½“å‰å‘é‡çš„æ•°é‡ğœ‡ å’Œğœ† âˆˆ (0ï¼Œ1)æ˜¯è¶…å‚æ•°ã€‚

Let Pğ‘– denote the set of indices of keys that the ğ‘–-th query attend to. Pğ‘– in Routing Transformer is defined as

è®¾Pğ‘– è¡¨ç¤ºé”®çš„ç´¢å¼•é›†ğ‘–-ç¬¬ä¸ªæŸ¥è¯¢æ¶‰åŠ.Pğ‘– åœ¨è·¯ç”±Transformerä¸­å®šä¹‰ä¸º

Pğ‘– = { ğ‘— : ğœ‡(qğ‘–) = ğœ‡(kğ‘—)}. (11)

Reformer [66] uses locality-sensitive hashing (LSH) to select key-value pairs for each query. The proposed LSH attention allows each token to attend only to the tokens within the same hashing bucket. The basic idea is to use an LSH function to hash queries and keys into several buckets, with similar items fall in the same bucket with high probability. Specifically, they use the random matrix method for the LSH function. Let ğ‘ be the number of buckets, given a random matrix ğ‘… of size [ğ·ğ‘˜, ğ‘/2], the LSH function is computed by : 

é‡æ•´å™¨[66]ä½¿ç”¨ä½ç½®æ•æ„Ÿæ•£åˆ—(LSH)ä¸ºæ¯ä¸ªæŸ¥è¯¢é€‰æ‹©é”®å€¼å¯¹ã€‚æå‡ºçš„LSHå…³æ³¨å…è®¸æ¯ä¸ªä»¤ç‰Œåªå…³æ³¨åŒä¸€å“ˆå¸Œæ¡¶å†…çš„ä»¤ç‰Œã€‚å…¶åŸºæœ¬æ€æƒ³æ˜¯ä½¿ç”¨LSHå‡½æ•°å°†æŸ¥è¯¢å’Œé”®æ•£åˆ—åˆ°å¤šä¸ªæ¡¶ä¸­ï¼Œç±»ä¼¼çš„é¡¹ç›®å¾ˆå¯èƒ½è½åœ¨åŒä¸€ä¸ªæ¡¶ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œä»–ä»¬å¯¹LSHå‡½æ•°ä½¿ç”¨éšæœºçŸ©é˜µæ–¹æ³•ã€‚å…è®¸ğ‘ æ˜¯ç»™å®šéšæœºçŸ©é˜µçš„æ¡¶æ•°ğ‘… çš„å¤§å°[ğ·ğ‘˜, ğ‘/2] ï¼ŒLSHå‡½æ•°é€šè¿‡ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼š

â„(ğ‘¥) = arg max( [ğ‘¥ğ‘…; âˆ’ğ‘¥ğ‘…]). (12)

The LSH attention allows the ğ‘–-th query to attend only to key-value pairs with indices

LSHå…³æ³¨å…è®¸ğ‘–-åªå¤„ç†å¸¦ç´¢å¼•çš„é”®å€¼å¯¹çš„æŸ¥è¯¢

Pğ‘– = { ğ‘— : â„(qğ‘–) = â„(kğ‘—)}. (13)

Sparse Adaptive Connection (SAC) [78] views the input sequence as a graph and learns to construct attention edges to improve task-specific performances using an adaptive sparse connection. SAC uses an LSTM edge predictor to construct edges between tokens. With no ground truth for edges, the edge predictor is trained with reinforcement learning.

ç¨€ç–è‡ªé€‚åº”è¿æ¥(SAC)[78]å°†è¾“å…¥åºåˆ—è§†ä¸ºä¸€ä¸ªå›¾ï¼Œå¹¶å­¦ä¹ ä½¿ç”¨è‡ªé€‚åº”ç¨€ç–è¿æ¥æ„å»ºæ³¨æ„åŠ›è¾¹ç¼˜ä»¥æé«˜ä»»åŠ¡ç‰¹å®šæ€§èƒ½ã€‚SACä½¿ç”¨LSTMè¾¹ç¼˜é¢„æµ‹å™¨æ¥æ„é€ ä»¤ç‰Œä¹‹é—´çš„è¾¹ç¼˜ã€‚åœ¨è¾¹ç¼˜æ²¡æœ‰åœ°é¢çœŸå®æ€§çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¼ºåŒ–å­¦ä¹ æ¥è®­ç»ƒè¾¹ç¼˜é¢„æµ‹å™¨ã€‚

Sparse Sinkhorn Attention [132] first splits queries and keys into several blocks and assigns a key block to each query block. Each query is only allowed to attend to the keys in the key block that is assigned to its corresponding query block. The assignment of key blocks is controlled by a sorting network, which uses Sinkhorn normalization to produce a doubly stochastic matrix as the permutation matrix representing the assignment. They use this content-based block sparse attention along with block local attention introduced in Sec. 4.1.1 to enhance the ability of the model to model locality.

Sparse Sinkhorn Attention[132]é¦–å…ˆå°†æŸ¥è¯¢å’Œé”®æ‹†åˆ†ä¸ºå‡ ä¸ªå—ï¼Œå¹¶ä¸ºæ¯ä¸ªæŸ¥è¯¢å—åˆ†é…ä¸€ä¸ªé”®å—ã€‚æ¯ä¸ªæŸ¥è¯¢åªå…è®¸å…³æ³¨åˆ†é…ç»™å…¶ç›¸åº”æŸ¥è¯¢å—çš„é”®å—ä¸­çš„é”®ã€‚å…³é”®å—çš„åˆ†é…ç”±æ’åºç½‘ç»œæ§åˆ¶ï¼Œè¯¥æ’åºç½‘ç»œä½¿ç”¨Sinkhornå½’ä¸€åŒ–äº§ç”ŸåŒéšæœºçŸ©é˜µä½œä¸ºè¡¨ç¤ºåˆ†é…çš„ç½®æ¢çŸ©é˜µã€‚ä»–ä»¬ä½¿ç”¨è¿™ç§åŸºäºå†…å®¹çš„å—ç¨€ç–æ³¨æ„åŠ›ä»¥åŠç¬¬4.1.1èŠ‚ä¸­ä»‹ç»çš„å—å±€éƒ¨æ³¨æ„åŠ›æ¥å¢å¼ºæ¨¡å‹å¯¹å±€éƒ¨æ€§çš„å»ºæ¨¡èƒ½åŠ›ã€‚

### 4.2 Linearized Attention çº¿æ€§åŒ–æ³¨æ„åŠ›
Assuming Q, K, V âˆˆ Rğ‘‡Ã—ğ· , the complexity of computing softmax(QKâŠ¤)V is quadratic w.r.t. sequence lengthğ‘‡ , as illustrated in Fig. 7(a). Ifsoftmax(QKâŠ¤) can be disentangled into Qâ€²K â€²âŠ¤, we can compute Qâ€²K â€²âŠ¤V in reversed order (i.e., Qâ€² (K â€²âŠ¤V)), leading to a complexity of O (ğ‘‡ ). 

å‡è®¾Qï¼ŒKï¼ŒVâˆˆRğ‘‡Ã—ğ· , è®¡ç®—softmax(QKâŠ¤)Vçš„å¤æ‚æ€§æ˜¯äºŒæ¬¡w.r.t.åºåˆ—é•¿åº¦ğ‘‡ , å¦‚å›¾7(a)æ‰€ç¤ºã€‚å¦‚æœsoftmax(QKâŠ¤)å¯ä»¥è§£ç¼ ä¸ºQâ€²Kâ€²\8868; ï¼Œæˆ‘ä»¬å¯ä»¥ä»¥ç›¸åçš„é¡ºåº(å³ï¼ŒQâ€²(Kâ€²éŸ”V))è®¡ç®—Qâ€²Kã€ƒ\8868 Vï¼Œå¯¼è‡´Oçš„å¤æ‚æ€§(ğ‘‡ ). 

Let Ë†A = exp(QKâŠ¤) denote un-normalized attention matrix, and exp(Â·) is applied element-wise, the regular attention can be rewritten as Z = Dâˆ’1 Ë†AV, where D = diag( Ë†A1âŠ¤ğ‘‡ ); 1ğ‘‡âŠ¤ is the all-ones column vector of length ğ‘‡ ; diag(Â·) is a diagonal matrix with the input vector as the diagonal.

è®¾â‘ªA=exp(QKâŠ¤)è¡¨ç¤ºæœªå½’ä¸€åŒ–çš„æ³¨æ„çŸ©é˜µï¼Œå¹¶ä¸”exp(Â·)æ˜¯æŒ‰å…ƒç´ åº”ç”¨çš„ï¼Œè§„åˆ™æ³¨æ„å¯ä»¥é‡å†™ä¸ºZ=Dâˆ’1â‘ªAVï¼Œå…¶ä¸­D=diagğ‘‡ ); 1.ğ‘‡âŠ¤ æ˜¯é•¿åº¦ä¸º1çš„åˆ—å‘é‡ğ‘‡ ; diag(Â·)æ˜¯ä¸€ä¸ªä»¥è¾“å…¥å‘é‡ä¸ºå¯¹è§’çº¿çš„å¯¹è§’çŸ©é˜µã€‚

Linearized attention is a class of methods that approximate or replace the unnormalized attention matrix exp(QKâŠ¤) with ğœ™ (Q)ğœ™ (K)âŠ¤, where ğœ™ is a feature map that is applied in row-wise manner. Hence the computation of unnormalized attention matrix can be linearized by computing ğœ™ (Q) (ğœ™ (K)âŠ¤V)(6Similarly, the partition term D can be computed with ğœ™ (Q)  ğœ™ (K)âŠ¤1ğ‘‡âŠ¤ in linear time.), as illustrated in Fig. 7(b).

çº¿æ€§åŒ–æ³¨æ„åŠ›æ˜¯ä¸€ç±»æ–¹æ³•ï¼Œå®ƒå°†éæ ‡å‡†åŒ–æ³¨æ„åŠ›çŸ©é˜µexp(QKâŠ¤)è¿‘ä¼¼æˆ–æ›¿æ¢ä¸ºğœ™ (Q)ğœ™ (K) âŠ¤ï¼Œå…¶ä¸­ğœ™ æ˜¯ä»¥è¡Œæ–¹å¼åº”ç”¨çš„ç‰¹å¾å›¾ã€‚å› æ­¤ï¼Œéæ ‡å‡†åŒ–æ³¨æ„åŠ›çŸ©é˜µçš„è®¡ç®—å¯ä»¥é€šè¿‡è®¡ç®—æ¥çº¿æ€§åŒ–ğœ™ (Q)(ğœ™ (K) âŠ¤V)6ï¼Œå¦‚å›¾7(b)æ‰€ç¤ºã€‚

Fig. 7. Illustration of complexity difference between standard self-attention and linearized self-attention.
å›¾7ã€‚è¯´æ˜æ ‡å‡†è‡ªæ³¨æ„å’Œçº¿æ€§åŒ–è‡ªæ³¨æ„ä¹‹é—´çš„å¤æ‚æ€§å·®å¼‚ã€‚

To gain further insights into linearized attention, we derive the formulation in vector form. We consider a general form of attention 

ä¸ºäº†è¿›ä¸€æ­¥äº†è§£çº¿æ€§åŒ–æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬æ¨å¯¼äº†å‘é‡å½¢å¼çš„å…¬å¼

zğ‘– = âˆ‘ï¸ğ‘— sim(qğ‘–, kğ‘—) Ã ğ‘—â€² sim(qğ‘–, kğ‘—â€²) vğ‘—, (14) 

where sim(Â·, Â·) is a scoring function measuring similarity between input vectors. In vanilla Transformer, the scoring function is the exponential of inner product exp(âŸ¨Â·, Â·âŸ©). A natural choice of sim(Â·, Â·) is a kernel function K (x, y) = ğœ™ (x)ğœ™ (y)âŠ¤, which leads to 

å…¶ä¸­sim(Â·ï¼ŒÂ·)æ˜¯æµ‹é‡è¾“å…¥å‘é‡ä¹‹é—´ç›¸ä¼¼æ€§çš„è¯„åˆ†å‡½æ•°ã€‚åœ¨vanilla Transformerä¸­ï¼Œè¯„åˆ†å‡½æ•°æ˜¯å†…ç§¯expçš„æŒ‡æ•°(âŸ¨Â·ï¼ŒÂ·âŸ©)ã€‚sim(Â·ï¼ŒÂ·)çš„è‡ªç„¶é€‰æ‹©æ˜¯æ ¸å‡½æ•°K(xï¼Œy)=ğœ™ (x)ğœ™ (y) âŠ¤ï¼Œè¿™å¯¼è‡´

zğ‘– = âˆ‘ï¸ğ‘— ğœ™ (qğ‘–)ğœ™ (kğ‘—)âŠ¤ Ã ğ‘—â€² ğœ™ (qğ‘–)ğœ™ (kğ‘—â€²)âŠ¤ vğ‘— (15) 
= ğœ™ (qğ‘–)Ã ğ‘— ğœ™ (kğ‘—) âŠ— vğ‘— ğœ™ (qğ‘–)Ã ğ‘—â€² ğœ™ (kğ‘—â€²)âŠ¤ , (16) 

where âŠ— denotes outer product of vectors. Based on this formulation, attention can be linearized by first computing the highlighted terms Ã ğ‘— ğœ™ (kğ‘—) âŠ— vğ‘— andÃ ğ‘—â€² ğœ™ (kğ‘—â€²)âŠ¤. This could be especially beneficial for autoregressive attention, as the cumulative sums Sğ‘– = Ã ğ‘–ğ‘—=1 ğœ™ (kğ‘—) âŠ—vğ‘— and uğ‘– = Ã ğ‘–ğ‘—=1 ğœ™ (kğ‘—) can be computed from Sğ‘–âˆ’1 and uğ‘–âˆ’1 in constant time. The effectively enables Transformer decoders to run like RNNs.

å…¶ä¸­âŠ—è¡¨ç¤ºå‘é‡çš„å¤–ç§¯ã€‚åŸºäºè¿™ä¸ªå…¬å¼ï¼Œæ³¨æ„åŠ›å¯ä»¥é€šè¿‡é¦–å…ˆè®¡ç®—çªå‡ºæ˜¾ç¤ºçš„é¡¹æ¥çº¿æ€§åŒ–ğ‘— ğœ™ (k)ğ‘—) âŠ— vğ‘— å’Œğ‘—â€² ğœ™ (k)ğ‘—â€²)âŠ¤. è¿™å¯¹äºè‡ªå›å½’å…³æ³¨å°¤å…¶æœ‰ç›Šï¼Œå› ä¸ºç´¯ç§¯æ€»å’ŒSğ‘– = Ã ğ‘–ğ‘—=1.ğœ™ (k)ğ‘—) âŠ—vğ‘— å’Œuğ‘– = Ã ğ‘–ğ‘—=1.ğœ™ (k)ğ‘—) å¯ä»¥ä»Sè®¡ç®—ğ‘–âˆ’1å’Œuğ‘–âˆ’1åœ¨æ’å®šæ—¶é—´å†…ã€‚æœ‰æ•ˆåœ°ä½¿Transformerè§£ç å™¨èƒ½å¤ŸåƒRNNä¸€æ ·è¿è¡Œã€‚

An interpretation of Eq. (16) is that the model maintains a memory matrix by aggregating associations represented by outer products of (feature mapped) keys and values, and then retrieve a value by multiplying the memory matrix with feature mapped query with proper normalization. There are two key components in this approach: 1.  feature map ğœ™ (Â·), and 2.  aggregation rule.

å¯¹ç­‰å¼(16)çš„è§£é‡Šæ˜¯ï¼Œè¯¥æ¨¡å‹é€šè¿‡èšåˆç”±(ç‰¹å¾æ˜ å°„)é”®å’Œå€¼çš„å¤–ç§¯è¡¨ç¤ºçš„å…³è”æ¥ç»´æŒå­˜å‚¨å™¨çŸ©é˜µï¼Œç„¶åé€šè¿‡å°†å­˜å‚¨å™¨çŸ©é˜µä¸å…·æœ‰é€‚å½“å½’ä¸€åŒ–çš„ç‰¹å¾æ˜ å°„æŸ¥è¯¢ç›¸ä¹˜æ¥æ£€ç´¢å€¼ã€‚è¿™ç§æ–¹æ³•æœ‰ä¸¤ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ï¼š1ã€‚åœ°å½¢å›¾ğœ™ (Â·)å’Œ2ã€‚èšåˆè§„åˆ™ã€‚

#### 4.2.1 Feature Maps. 
Linear Transformer [62] propose to use a simple feature mapğœ™ğ‘–(x) = elu(ğ‘¥ğ‘–)+1. This feature map does not aim to approximate dot product attention, but is empirically proved to perform on par with the standard Transformer. 

çº¿æ€§Transformer[62]å»ºè®®ä½¿ç”¨ç®€å•çš„ç‰¹å¾å›¾ğœ™ğ‘–(x) =æ´—è„±æ¶²(ğ‘¥ğ‘–)+1.æ­¤ç‰¹å¾å›¾çš„ç›®çš„ä¸æ˜¯è¿‘ä¼¼ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œä½†ç»éªŒè¯æ˜å…¶æ€§èƒ½ä¸æ ‡å‡†Transformerç›¸å½“ã€‚

Performer [18, 19] uses random feature maps that approximate the scoring function of Transformer. The random feature maps take functions ğ‘“1, Â· Â· Â· , ğ‘“ğ‘™ : R â†’ R and â„ : Rğ· â†’ R. 

æ‰§è¡Œè€…[18ï¼Œ19]ä½¿ç”¨éšæœºç‰¹å¾å›¾æ¥è¿‘ä¼¼Transformerçš„è¯„åˆ†å‡½æ•°ã€‚éšæœºç‰¹å¾å›¾å…·æœ‰åŠŸèƒ½ğ‘“1, Â· Â· Â· , ğ‘“ğ‘™ : Râ†’ Rå’Œâ„ : Rğ· â†’ R

ğœ™ (x) = â„(x) âˆšğ‘š [ğ‘“1 (ğœ”1âŠ¤x), Â· Â· Â· , ğ‘“ğ‘š (ğœ”âŠ¤ğ‘šx), Â· Â· Â· , ğ‘“ğ‘™ (ğœ”1âŠ¤x), Â· Â· Â· , ğ‘“ğ‘™ (ğœ”âŠ¤ğ‘šx)], (17) 

where ğœ”1, Â· Â· Â· , ğœ”ğ‘š iidâˆ¼ D are drawn from some distribution D âˆˆ P (Rğ· ).

å…¶ä¸­ï¼Œğœ”1, Â· Â· Â· , ğœ”ğ‘š iidï½Dç”±ä¸€äº›åˆ†å¸ƒDâˆˆP(Rğ· ).

The first version of Performer [18] is inspired from the random Fourier feature map [105] that was originally used to approximate Gaussian kernel. It uses trigonometric functions with â„(x) = exp( âˆ¥x2âˆ¥2 ),ğ‘™ = 2, ğ‘“1 = sin, ğ‘“2 = cos. This approach has also been used in Random Feature Attention (RFA) [95], with the difference thatâ„(x) is set to 1 as the queries and keys are â„“2-normalized before applying the feature map.

Performer[18]çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬æºè‡ªæœ€åˆç”¨äºè¿‘ä¼¼é«˜æ–¯æ ¸çš„éšæœºå‚…é‡Œå¶ç‰¹å¾å›¾[105]ã€‚å®ƒä½¿ç”¨ä¸‰è§’å‡½æ•°â„(x) =exp(â€–x2â€–2)ï¼Œğ‘™ = 2.ğ‘“1ï¼sinï¼Œğ‘“2=ä½™å¼¦ã€‚éšæœºç‰¹å¾æ³¨æ„(RFA)[95]ä¸­ä¹Ÿä½¿ç”¨äº†è¿™ç§æ–¹æ³•ï¼ŒåŒºåˆ«åœ¨äºâ„(x) è®¾ç½®ä¸º1ï¼Œå› ä¸ºæŸ¥è¯¢å’Œé”®æ˜¯â„“2åœ¨åº”ç”¨ç‰¹å¾å›¾ä¹‹å‰å½’ä¸€åŒ–ã€‚

Although the trigonometric random feature map leads to an unbiased approximation, it does not guarantee non-negative attention scores and thus could lead to unstable behaviors and abnormal behaviors. To mitigate this issue, the second version of Performer [19] proposes positive random feature maps, which uses â„(x) = exp(âˆ’ âˆ¥x2âˆ¥2 ),ğ‘™ = 1, ğ‘“1 = exp and thus guarantees unbiased and nonnegative approximation of dot-product attention. This approach is more stable than Choromanski et al. [18] and reports better approximation results.

å°½ç®¡ä¸‰è§’éšæœºç‰¹å¾å›¾å¯¼è‡´äº†æ— åè¿‘ä¼¼ï¼Œä½†å®ƒä¸èƒ½ä¿è¯éè´Ÿæ³¨æ„åŠ›å¾—åˆ†ï¼Œå› æ­¤å¯èƒ½å¯¼è‡´ä¸ç¨³å®šè¡Œä¸ºå’Œå¼‚å¸¸è¡Œä¸ºã€‚ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼ŒPerformer[19]çš„ç¬¬äºŒä¸ªç‰ˆæœ¬æå‡ºäº†æ­£éšæœºç‰¹å¾å›¾ï¼Œå®ƒä½¿ç”¨â„(x) =exp(âˆ’â€–x2â€–2)ï¼Œğ‘™ = 1.ğ‘“1ï¼expï¼Œä»è€Œä¿è¯ç‚¹ç§¯æ³¨æ„åŠ›çš„æ— åå’Œéè´Ÿè¿‘ä¼¼ã€‚è¯¥æ–¹æ³•æ¯”Choromanskiet al æ›´ç¨³å®š[18]ï¼Œå¹¶æŠ¥å‘Šäº†æ›´å¥½çš„è¿‘ä¼¼ç»“æœã€‚

In addition to using random feature maps to approximate standard dot product attention, Peng et al. [95] and Choromanski et al. [19] also explore approximating order-1 arc-cosine kernel with â„(x) = 1,ğ‘™ = 1, ğ‘“1 = ReLU. This feature map has been show to be effective in various tasks including machine translation and protein sequence modeling.

é™¤äº†ä½¿ç”¨éšæœºç‰¹å¾å›¾æ¥è¿‘ä¼¼æ ‡å‡†ç‚¹ç§¯æ³¨æ„åŠ›ä¹‹å¤–ï¼ŒPenget al [95]å’ŒChoromanskiet al [19]è¿˜æ¢ç´¢äº†ç”¨â„(x) ï¼1ï¼Œğ‘™ = 1.ğ‘“1=ReLUã€‚è¯¥ç‰¹å¾å›¾å·²è¢«è¯æ˜åœ¨åŒ…æ‹¬æœºå™¨ç¿»è¯‘å’Œè›‹ç™½è´¨åºåˆ—å»ºæ¨¡åœ¨å†…çš„å„ç§ä»»åŠ¡ä¸­æ˜¯æœ‰æ•ˆçš„ã€‚

Schlag et al. [113] design a feature map that aims at facilitating orthogonality in feature space. Specifically, given an input x âˆˆ Rğ· , the feature map ğœ™ : Rğ· â†’ R2ğœˆğ· is defined by the partial function 

Schlaget al [113]è®¾è®¡äº†ä¸€ä¸ªæ—¨åœ¨ä¿ƒè¿›ç‰¹å¾ç©ºé—´æ­£äº¤æ€§çš„ç‰¹å¾å›¾ã€‚å…·ä½“åœ°ï¼Œç»™å®šè¾“å…¥xâˆˆRğ· , ç‰¹å¾å›¾ğœ™ : Rğ· â†’ ç¬¬2æ¬¡ğœˆğ· ç”±éƒ¨åˆ†å‡½æ•°å®šä¹‰

ğœ™ğ‘–+2(ğ‘—âˆ’1)ğ· (x) = ReLU( [x, âˆ’x])ğ‘–ReLU( [x, âˆ’x])ğ‘–+ğ‘— for ğ‘– = 1, Â· Â· Â· , 2ğ·, ğ‘— = 1, Â· Â· Â· , ğœˆ. (18)

#### 4.2.2 Aggregation Rule. 
In Eq. (16) the associations {ğœ™ (k)ğ‘— âŠ— vğ‘— } are aggregated into the memory matrix by simple summation. This is adopted by several studies [18, 19, 62]. However, it could be more beneficial for the network to selectively drop associations as new associations are added to the memory matrix.

RFA [95] introduces a gating mechanism to the summation to model local dependency in sequence data. Specifically, when adding a new association to the memory matrix S, at a particular time step, they weigh S by a learnable, input-dependent scalar ğ‘”, and the new association by (1 âˆ’ ğ‘”) (and a similar mechanism to u). With this modification, history associations are exponentially decayed and recent context is favored in each timestep.

Schlag et al. [113] argue that simple summation limits the capacity of the memory matrix and thus propose to enlarge the capacity in a write-and-remove fashion. Specifically, given a new input key-value pair (kğ‘–, vğ‘–), the model first retrieve the value Â¯vğ‘– currently associated with kğ‘– using matrix multiplication. It then writes to the memory matrix a convex combination of Â¯vğ‘– and vğ‘– , using a input-dependent gating scalar ğ‘”, and removes the association Â¯vğ‘– . They also propose sum normalization (normalizing ğœ™ (qğ‘–), ğœ™ (kğ‘–) by the sum of their components before updating the memory matrix) instead of normalizing with the denominator in Eq. (16) for this aggregation rule. 14 

### 4.3 Query Prototyping and Memory Compression
Apart from using sparse attention or kernel-based linearized attention, one could also reduce the complexity of attention by reducing the number of queries or key-value pairs, which leads to query prototyping and memory compression7 methods, respectively.

#### 4.3.1 Attention with Prototype Queries. 
In query prototyping, several prototypes of queries serve as the main source to compute attention distributions. The model either copies the distributions to the positions of represented queries or filling those positions with discrete uniform distributions.

Fig. 8(a) illustrates the computing flow of query prototyping. prototypes queries keys values aggregate Â·Â·Â· (a) Query prototyping compressed values queries keys compressed keys aggregate values Â·Â·Â· Â·Â·Â· (b) Memory compression

Fig. 8. Query prototyping and memory compression.

Clustered Attention [138] groups queries into several clusters and then computes attention distributions for cluster centroids. All queries in a cluster share the attention distribution calculated with the corresponding centroid.

Informer [170] selects prototypes from queries using explicit query sparsity measurement, which is derived from an approximation of the Kullback-Leibler divergence between the queryâ€™s attention distribution and the discrete uniform distribution. Attention distributions are then only calculated for the top-ğ‘¢ queries under query sparsity measurement. The rest of the queries are assigned with discrete uniform distributions.

#### 4.3.2 Attention with Compressed Key-Value Memory. 
Apart from decreasing the number of queries with query prototyping, one can also reduce the complexity by reducing the number of the key-value pairs before applying the attention mechanism, as depicted in Fig. 8(b).

Liu et al. [84] propose Memory Compressed Attention (MCA) that reduces the number of keys and values using a strided convolution. This modification is used as a complement to local attention proposed in the same work (as discussed in Sec. 4.1), in that it can capture global context. The mechanism reduces the number of keys and values by a factor of kernel size ğ‘˜ and thus allowing to process significantly longer sequences than vanilla Transformer given the same computation resources.

Set Transformer [70] and Luna [90] use a number of external trainable global nodes to summarize information from inputs and then the summarized representations serve as a compressed memory that the inputs attend to. This reduces the quadratic complexity of self-attention to linear complexity w.r.t. sequence length. 7The key-value pairs are often referred to as a key-value memory (hence the name memory compression). Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â· Â·Â·Â·


Linformer [142] utilizes linear projections to project keys and values from length ğ‘› to a smaller length ğ‘›ğ‘˜ . This also reduces the complexity of self-attention to linear. The drawback of this approach is that an input sequence length has to be assumed and hence it cannot be used in autoregressive attention.

Poolingformer [165] adopts two-level attention that combines a sliding window attention and a compressed memory attention. The compressed memory module is used after the sliding window attention to increase the receptive field. They explore a few different pooling operations as the compression operation to compress the number of keys and values, including max pooling and pooling with Dynamic Convolution [146].

### 4.4 Low-rank Self-Attention
Some empirical and theoretical analyses [45, 142] report the self-attention matrix A âˆˆ Rğ‘‡Ã—ğ‘‡ is often low-rank8 . The implications of this property are twofold: 1.  The low-rank property could be explicitly modeled with parameterization; 2.  The self-attention matrix could be replaced by a low-rank approximation.

#### 4.4.1 Low-rank Parameterization. 
The fact that the rank of the attention matrix is less than sequence length implies that, for scenarios where the inputs are typically short, setting ğ·ğ‘˜ > ğ‘‡ would be more than an over-parameterization and lead to overfitting. It is thus reasonable to limit the dimension of ğ·ğ‘˜ to explicitly model the low-rank property as an inductive bias. Guo et al. [45] decompose self-attention matrix into a low-rank attention module with small ğ·ğ‘˜ that captures long-range non-local interactions, and a band attention module that captures local dependencies.

#### 4.4.2 Low-rank Approximation. 
Another implication of the low-rank property of the attention matrix is that one can use a low-rank matrix approximation to reduce the complexity of selfattention. A closely related methodology is the low-rank approximation of kernel matrices. We believe some existing works are inspired by kernel approximation.

Some of the aforementioned linearized attention methods in Sec. 4.2 are inspired from kernel approximation with random feature maps. For example, Performer [18] follows the Random Fourier feature map originally proposed to approximate Gaussian kernels. The method first decomposes the attention distribution matrix A into Cğ‘„ GCğ¾ where G is a Gaussian kernel matrix and the random feature map is used to approximate G.

Another line of work follow the idea of NystrÃ¶m method. These NystrÃ¶m-based methods [16, 152] first select ğ‘š landmark nodes from the ğ‘‡ inputs with down-sampling methods (e.g., strided average pooling). Let ËœQ, ËœK be the selected landmark queries and keys, then the follow approximation is used in the attention computation ËœA = softmax  Q ËœKâŠ¤   softmax  ËœQ ËœKâŠ¤   âˆ’1 softmax  ËœQKâŠ¤ . (19)

Note that Mâˆ’1 =  softmax  ËœQ ËœKâŠ¤   âˆ’1 in Eq. (19) does not always exist. To mitigate this issue,

CSALR [16] adds an identity matrix to M to make sure that the inverse always exists. NystrÃ¶mformer [152] uses the Moore-Penrose pseudoinverse of M instead of the inverse so that the approximation can be made for cases where M is singular.

### 4.5 Attention with Prior
Attention mechanism generally outputs an expected attended value as a weighted sum of vectors, where the weights are an attention distribution over the values. Traditionally, the distribution is 8The rank of A is far lower than input length ğ‘‡ . 16  prior attention generated attention final attention fuse = â€¦

Fig. 9. Attention with prior. This type of model fuse generated attention scores with pior attention scores, producing the final attention scores for attention computation. generated from inputs (e.g., softmax(QKâŠ¤) in vanilla Transformer). As a generalized case, attention distribution can also come from other sources, which we refer to as prior. Prior attention distribution can be a supplement or substitute for distribution generated from inputs. We abstract this formulation of attention as attention with prior, as depicted in Fig. 9. In most cases, the fusion of two attention distribution can be done by computing a weighted sum of the scores corresponding to the prior and generated attention before applying softmax.

##### 4.5.1 Prior that Models locality. 
Some types of data (e.g., text) can exhibit a strong preference for the locality. This property can be explicitly encoded as a prior attention. A simple method would be to use a Gaussian distribution over positions. Specifically, one could multiply the generated attention distribution with some Gaussian density and then renormalize, which is equivalent to adding to the generated attention scores A a bias term G, where higher ğºğ‘–ğ‘— indicates a higher prior probability that the ğ‘–-th input attend to the ğ‘—-th input.

Yang et al. [156] proposes to first predict a central position ğ‘ğ‘– for each qğ‘– using a simple feedforward network. The Gaussian bias is then defined to be

ğºğ‘–ğ‘— = âˆ’ (ğ‘— âˆ’ ğ‘ğ‘–)2 2ğœ2 , (20) where ğœ denotes standard deviation for the Gaussian and can be determined as a hyperparameter or predicted from inputs.

Gaussian Transformer [42] assumes the central position to be ğ‘– for each qğ‘– and defines the bias to bes

ğºğ‘–ğ‘— = âˆ’|ğ‘¤(ğ‘– âˆ’ ğ‘—)2 + ğ‘|, (21) where ğ‘¤ â‰¥ 0, ğ‘ â‰¤ 0 are scalar parameters that controls the deviation and reduce the weight for central position, respectively.

#### 4.5.2 Prior from Lower Modules. 
In Transformer architecture, it is often observed the attention distributions are similar in adjacent layers. It is thus natural to provide attention distribution from previous layer as a prior for attention computation. The final attention scores can be defined as Ë†A(ğ‘™) = ğ‘¤1 Â· A(ğ‘™) + ğ‘¤2 Â· ğ‘”(A(ğ‘™âˆ’1)), (22) where A(ğ‘™) denotes the attention scores of the ğ‘™-th layer, ğ‘¤1,ğ‘¤2 âˆˆ R are weight applied to the scores from adjacent layers, and ğ‘” : Rğ‘›Ã—ğ‘› â†’ Rğ‘›Ã—ğ‘› is a function that translate previous scores to the prior to be applied.

Predictive Attention Transformer [143] proposes to apply a 2D-convolutional layer to previous attention scores and compute the final attention scores as a convex combination of the generated attention scores and the convolved scores. This is equivalent to setting ğ‘¤1 = ğ›¼,ğ‘¤2 = 1 âˆ’ ğ›¼ and ğ‘”(Â·) to be a convolutional layer in Eq. (22). They experiment training such a model from scratch and finetune after adapting the pre-trained BERT model, and both sets of experiments show improvements over baseline models.


Realformer [51] uses adds the previous attention scores directly to the generated attention scores, thus resembles a residual skip connection on attention maps. Itâ€™s equivalent to setting ğ‘¤1 = ğ‘¤2 = 1 and ğ‘”(Â·) to be identity map in Eq. (22). They conduct pre-training experiments on this model.

The results show that this model outperforms the baseline BERT model in multiple datasets and surpasses the baseline model even when pre-training budgets are significantly lower.

As an extreme case, Lazyformer [159] proposes to share attention maps between a number of adjacent layers. This is equivalent to setting ğ‘”(Â·) to identity and switch the settings of ğ‘¤1 = 0,ğ‘¤2 = 1 and ğ‘¤1 = 1,ğ‘¤2 = 0 alternatingly. The benefit of this approach is that the attention maps are computed only once and reused several times in the succeeding layers, thus reducing the computation cost. Their pre-training experiments show that the resulting model remains effective while being much more efficient to compute.

#### 4.5.3 Prior as Multi-task Adapters. 
Adapters are task-dependent, trainale modules that are attached in specific locations of a pre-trained network for cross-task efficient parameter sharing [108]. Pilault et al. [98] propose a Conditionally Adaptive Multi-Task Learning (CAMTL) framework that uses a trainable attention prior ğ‘€(zğ‘–) that depends on task encoding zğ‘– âˆˆ Rğ·ğ‘§ ğ‘€(zğ‘–) = ğ‘šÃŠğ‘—=1 ğ´â€²ğ‘— (zğ‘–), ğ´â€²ğ‘— (zğ‘–) = ğ´ğ‘—ğ›¾ğ‘–(zğ‘–) + ğ›½ğ‘–(zğ‘–), (23) where Ã‰ denotes direct sum, ğ´ğ‘— âˆˆ R(ğ‘›/ğ‘š)Ã—(ğ‘›/ğ‘š) are trainable parameters, and ğ›¾ğ‘— , ğ›½ğ‘— : Rğ·ğ‘§ â†’ R(ğ‘›/ğ‘š)Ã—(ğ‘›/ğ‘š) are are Feature Wise Linear Modulation functions [96]. A maximum sequence length ğ‘›ğ‘šğ‘ğ‘¥ is specified in implementation. The prior is formulated as a block diagonal matrix and added to the attention scores of upper layers in pre-trained Transformers to serve as an adapter for parameter-efficient multi-task inductive knowledge transfer.

#### 4.5.4 Attention with Only Prior. 
Some works have explored using an attention distribution that is independent of pair-wise interaction between inputs. In other words, their models exploit only a prior attention distribution.

Zhang et al. [164] design an efficient Transformer decoder variant called average attention network that uses a discrete uniform distribution as the sole source of attention distribution. The values are thus aggregated as a cumulative-average of all values. To improve the expressiveness of the network, they further adds a feed-forward gating layer on top of the average attention module.

The advantage of this approach is that the adapted Transformer decoder can train in a parallel manner as usual Transformers do and decode like an RNN, thus avoiding the O (ğ‘‡ 2) complexity in decoding.

You et al. [161] utilize a Gaussian distribution as the hardcoded attention distribution for attention calculation. The intuition is very similar to Yang et al. [156] and Guo et al. [42] in that attention distribution should be focused on a certain local window. Distinctively, they drop the generated attention completely and use only the Gaussian distribution for attention computation. In this approach, the mean (central position) and variance are designed to be hyperparameters. The experiments show that the hardcoded attention, when applied only to self-attention, can achieve comparable performance to the baseline model in machine translation tasks.

Synthesizer [131] proposes to replace generated attention scores with: 1.  a learnable, randomly initialized attention scores, and 2.  attention scores output by a feed-forward network that is only conditioned on the querying input itself. The experiments on machine translation and language modeling show that these variants can achieve competitive performance with vanilla Transformer.

It is not explained why these variants work but the empirical results are intriguing. 18 

### 4.6 Improved Multi-Head Mechanism
Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. However, there is no mechanism to guarantee that different attention heads indeed capture distinct features.

#### 4.6.1 Head Behavior Modeling. 
A basic motivation for using multi-head attention is to allow the model to jointly attend to information from different representation subspaces at different positions [137]. However, in vanilla Transformer there is no explicit mechanism to guarantee different behavior across attention heads, nor is there any mechanism for heads to interact with each other. A line of work is dedicated to improving multi-head mechanism by introducing incorporating more sophisticated mechanisms that guide the behavior of different attention heads or allow interaction across attention heads.

Li et al. [73] introduce an auxiliary disagreement regularization term into loss function to encourage diversity among different attention heads. Two regularization terms are respectively to maximize cosine distances of the input subspaces and output representations, while the last one is to disperse the positions attended by multiple heads with element-wise multiplication of the corresponding attention matrices.

Several probing works have revealed that pre-trained Transformer models exhibit certain patterns of self-attention that are of little linguistic backing. As a representative work, Kovaleva et al. [68] identify several simple attention patterns in BERT. For instance, many of the attention heads simply pay attention to special BERT tokens [CLS] and [SEP]. As a result, some constraints can be introduced to boost the training of Transformer models. To this end, Deshpande and Narasimhan [27] propose to use an auxiliary loss, which is defined to be the Frobenius norm between attention distribution maps and predefined attention patterns.

Talking-head Attention [119] uses a talking head mechanism that linearly projects the generated attention scores from â„ğ‘˜ to â„ heads, applies softmax in that space, and then projects to â„ğ‘£ heads for value aggregation. The motivation is to encourage the model to move information between attention heads in a learnable fashion.

Collaborative Multi-head Attention [21] uses shared query and key projection Wğ‘„ and Wğ¾ and a mixing vector mğ‘– for the ğ‘–-th head to filter from the projection parameters such that Eq. 3.  is adapted to headğ‘– = Attention(QWğ‘„ diag(mğ‘–), KWğ¾, VWğ‘‰ğ‘– ), (24) where Wğ‘„ and Wğ¾ are shared by all the attention heads.

#### 4.6.2 Multi-head with Restricted Spans. 
Vanilla attention adopts full attention spans assume, where a query can attend to all of the key-value pairs. However, it is often observed that some heads focus their attention distribution mainly in a local context while some other heads attend to broader contexts. It could thus be beneficial to restrict the attention spans:
* Locality cases where locality is an important prior. . Restricting attention spans induce explicit local constraints. This is advantageous in
* Efficiency. If implemented appropriately, such a model can scale to very long sequences without introducing additional memory footprint and computational time.

Restricting attention spans can be expressed as multiplying each attention distribution value with a mask value and then re-normalize, where the mask can be expressed as a non-increasing function that maps a distance to a value in [0, 1]. A vanilla attention assigns a mask value of 1 for all distances, as depicted in Fig. 10(a).

Sukhbaatar et al. [126] propose to use a learnable attention span, as depicted in Fig. 10(b) .

The mask is parameterized by a learnable scalar ğ‘§ and a hyperparameter ğ‘…. The experiments on

 19 ğ‘¥ ğ‘š(ğ‘¥1) (a) mask function for vanilla attention ğ‘¥ ğ‘šğ‘§ (ğ‘¥1) ğ‘§ ğ‘§ + ğ‘… (b) mask function for adaptive span ğ‘¥ ğ‘šğ‘§ (ğ‘¥1) ğ‘¤ (c) mask function for fixed span

Fig. 10. Three types of span masking function ğ‘š(ğ‘¥). The horizontal axis represents distance ğ‘¥ and vertical axis the mask value. character-level language modeling show that the adaptive-span models outperform baseline models while having significantly fewer FLOPS. It is also observed that lower layers generally have smaller learned spans and higher layers otherwise. This indicates that the model can learn a hierarchical composition of features.

Multi-Scale Transformer [44] proposes to use a fixed attention span, with different heads in different layers using a different max span. The fixed attention span is depicted in Fig. 10(c). The attention is restricted within a fixed window which is controlled by a scale value ğ‘¤. They design the scales from an intuitive linguistic perspective and empirical observation from BERT such that higher layers tend to have more large scales (e.g., large span size), and lower layers should be confined with a smaller scale. Their experiments on several tasks show that the model can outperform baseline models while accelerating inference on long sequences.

#### 4.6.3 Multi-head with Refined Aggregation. 
After each attention head computes its output representation, the vanilla multi-head attention [137] concatenates these representation and then apply a linear transformation to the concatenated representation to obtain the final output representation, as formulated in Eq. 2. . Combining Eq. 1. 2.  and 3. , one can see that this concatenate-and-project formulation is equivalent to summation over ğ» re-parameterized attention outputs. To this end, we first divide Wğ‘‚ âˆˆ Rğ·ğ‘šÃ—ğ·ğ‘š into ğ» blocks

Wğ‘‚ = [Wğ‘‚1 ; Wğ‘‚2 ; Â· Â· Â· ; Wğ‘‚ğ» ], (25) where each Wğ‘‚ reformulated as ğ‘– is of dimension ğ·ğ‘£ Ã— ğ·ğ‘š. Itâ€™s thus easy to see that multi-head attention can be

MultiHeadAttn(ğ‘„, ğ¾,ğ‘‰ ) = ğ»âˆ‘ï¸ğ‘–=1

Attention(ğ‘„Wğ‘„ğ‘– , ğ¾Wğ¾ğ‘– ,ğ‘‰Wğ‘‰ğ‘– Wğ‘‚ğ‘– ). (26)

One might argue that this simple aggregate-by-summation paradigm does not fully exploit the expressiveness of multi-head attention and that it is more desirable to use a more complex aggregation.

Gu and Feng [40], Li et al. [74] propose to use routing methods, originally proposed for capsule networks [112], to further aggregate information produced by different attention heads. The outputs of attention heads are first transformed into input capsules, then output capsules are obtained after the iterative routing process. The output capsules are then concatenated as a final output of multi-head attention. These two works both utilizes two routing mechanisms, namely dynamic routing[112] and EM routing[53]. One would notice that iterative routing introduces additional parameters and computational overhead. Li et al. [74] empirically show that applying the routing mechanism only to the lower layers can best balance the translation performance and computational efficiency. 20 

#### 4.6.4 Other Modifications. 
Several other modifications to the multi-head mechanism have been proposed to improve multi-head attention.

Shazeer [117] propose multi-query attention, where key-value pairs are shared among attention heads (i.e., to use only one key projection and one value projection for all attention heads). The advantage of this method is that it reduces the memory bandwidth requirements for decoding and results in a model that is faster to decode, while incurring only minor quality degradation from the baseline.

Bhojanapalli et al. [11] establish that small attention key size can affect its ability to represent arbitrary distribution. They thus propose to disentangle head size from the number of heads â„, as opposed to the common practice that sets the head size to be ğ·ğ‘š/â„. It is observed empirically that setting attention head size to be input sequence length is beneficial. 

## 5 OTHER MODULE-LEVEL MODIFICATIONS å…¶ä»–æ¨¡å—çº§ä¿®æ”¹
### 5.1 Position Representations
Definition 5.1 (permutation equivariant function). Let Î ğ‘› be the set of all permutations of indices {1, 2, Â· Â· Â· ,ğ‘‡ }. A function ğ‘“ : Xğ‘‡ â†’ Yğ‘‡ is said to be permutation equivariant if and only if for any ğœ‹ âˆˆ Î ğ‘‡ ğ‘“ (ğœ‹ğ‘¥) = ğœ‹ ğ‘“ (ğ‘¥). (27)

It is easy to verify that Convolution and Recurrence networks are not permutation equivariant.

However, both self-attention modules and position-wise feed-forward layers in Transformer are permutation equivariant, which could be a problem when it comes to modeling problems other than set-input problems where the structure of inputs is needed. For example, when modeling sequences of text, the ordering of words matters and itâ€™s thus crucial to properly encode the positions of words in Transformer architecture. Therefore, additional mechanisms are required to inject positional information into Transformers. A common design is to first represent positional information using vectors and then infuse the vectors to the model as an additional input.

#### 5.1.1 Absolute Position Representations. 
In vanilla Transformer [137], positional information is encoded as absolute sinusoidal position encodings.For each position index ğ‘¡, the encoding is a vector pğ‘¡ = PE(ğ‘¡) âˆˆ Rğ·ğ‘š , of which every element is a sinusoidal (sin/cos) function of the index with pre-defined frequency.

PE(ğ‘¡)ğ‘– = ( sin(ğœ”ğ‘–ğ‘¡) if ğ‘– is even, cos(ğœ”ğ‘–ğ‘¡) if ğ‘– is odd, (28) where ğœ”ğ‘– is the hand-crafted frequency for each dimension. The position encoding of each position in the sequence is then added to the token embeddings and fed to Transformer.

Another way of representing absolute positions is to learn a set of positional embeddings for each position [28, 37]. Compared to hand-crafted position representation, learned embeddings are more flexible in that position representation can adapt to tasks through back-propagation. But the number of embeddings is limited up to a maximum sequence length determined before training, which makes this approach no longer inductive, i.e., not able to handle sequences longer than sequences seen in the training time[20, 85].

Wang et al. [139] propose to use sinusoidal position representation, but with each frequency ğœ”ğ‘– (in Eq. (28)) learned from data. This approach retains inductiveness but is more flexible than hand-crafted sinusoidal encoding. FLOATER [85] frames positional representation as a continuous dynamical system and adopts Neural ODE to enable end-to-end training with backpropagation.

This method is inductive and flexible while being parameter efficient compared to a fully learnable approach.

The Vanilla approach to incorporating absolute position representations is to add position encodings/embeddings to token embeddings. However, as the input signals propagate through the layers, the positional information might get lost in the upper layers. Later works find it beneficial to add position representations to inputs to each Transformer layer [2, 26, 45, 85].

#### 5.1.2 Relative Position Representations. 
Another line of works focuses on representing positional relationships between tokens instead of positions of individual tokens. The intuition is that in self-attention, pairwise positional relationships between input elements (direction and distance) could be more beneficial than positions of elements. Methods following this principles are called relative positional representation. Shaw et al. [116] propose to add a learnable relative position embedding to keys of attention mechanism kâ€²ğ‘— = kğ‘— + rğ‘–ğ‘—, for ğ‘– = 1, Â· Â· Â· , ğ‘›, (29) rğ‘–ğ‘— = Rclip(ğ‘–âˆ’ğ‘—), (30) clip(ğ‘¥) = max(âˆ’ğ¾, min(ğ‘¥, ğ¾)), (31) where rğ‘–ğ‘— âˆˆ Rğ·ğ‘˜ is the relative position embedding for relation between position ğ‘– and ğ‘— and ğ¾ is the largest offset that determines the number of embeddingg. Typically ğ¾ is set to a length that can accommodate most input sequences. As a special case, InDIGO [39] sets ğ¾ to 3 for their specially designed framework for non-autoregressive generation. As an incremental effort, Music

Transformer [56] further introduce a mechanism to reduce the intermediate memory requirements for this approach. Similar to this approach, T5 Raffel et al. [104] adopt a simplified form of relative position embeddings where each embedding is only a learnable scalar that is added to the corresponding score used for computing the attention weights.

Transformer-XL [24] use a sinusoidal encoding to represent positional relationships but fuses contents and position information by redesign the computation of attention scores9 Ağ‘–ğ‘— = qğ‘–kâŠ¤ğ‘— + qğ‘–  Rğ‘–âˆ’ğ‘—Wğ¾,ğ‘… âŠ¤ + u1kâŠ¤ğ‘— + u2  Rğ‘–âˆ’ğ‘—Wğ¾,ğ‘… âŠ¤ , (32) where Wğ¾,ğ‘… âˆˆ Rğ·ğ‘šÃ—ğ·ğ‘˜ , u1, u2 âˆˆ Rğ·ğ‘˜ are learnable parameters and R is a sinusoidal encoding matrix similar to position encoding in vanilla Transformer. Then softmax function is applied to scores A to provide attention weights. Note that the learnable sinusoidal encoding[139] is also a drop-in replacement to hand-crafted R.

DeBERTa [50] utilizes position embeddings like Shaw et al. [116] and applies the embeddings to the model in a disentangled style similar to Transformer-XL [24] Ağ‘–ğ‘— = qğ‘–kâŠ¤ğ‘— + qğ‘–  rğ‘–ğ‘—Wğ¾,ğ‘… âŠ¤ + kğ‘—  rğ‘–ğ‘—Wğ‘„,ğ‘… âŠ¤ , (33) where Wğ¾,ğ‘… , Wğ‘„,ğ‘… âˆˆ Rğ·ğ‘šÃ—ğ·ğ‘˜ are learnable parameters and rğ‘–ğ‘— is the learnable relative positional embedding as in Eq. (30). The first term is interpreted as a content-to-content attention, and the latter two terms are interpreted as (relative) content-to-position and position-to-content attention, respectively.

#### 5.1.3 Other Representations. 
Some research studies have explored using hybrid positional representations that contains both absolute and relative positional information. Transformer with Untied Position Encoding (TUPE) [63] re-designs the computation of attention scores as a combination 9 the scaling factor is omitted without loss of generality. 22  of a content-to-content term, an absolute position-to-position term and a bias term representing relative positional relationships

Ağ‘–ğ‘— = qğ‘–kâŠ¤ğ‘— +  pğ‘–Wğ‘„,ğ‘ƒ   pğ‘—Wğ¾,ğ‘ƒ  âŠ¤ + ğ‘ğ‘—âˆ’ğ‘–, (34) where Wğ¾,ğ‘ƒ , Wğ‘„,ğ‘ƒ âˆˆ Rğ·ğ‘šÃ—ğ·ğ‘˜ are learnable parameters, pğ‘–, pğ‘— are the position embeddings for positions ğ‘–, ğ‘—, and ğ‘ğ‘—âˆ’ğ‘– is a learnable scalar relative position embedding.

One can also design a single set of positional representations that express both absolute and relative information. Roformer [124] uses Rotary Position Embedding (RoPE) to represent the position of a token by multiplying the affine-transformed embedding of the ğ‘¡-th input ğ‘¥ğ‘¡ by a rotatory matrix RÎ˜,ğ‘¡ qğ‘¡ = xğ‘¡Wğ‘„ RÎ˜,ğ‘¡ kğ‘¡ = xğ‘¡Wğ¾RÎ˜,ğ‘¡, (35)

RÎ˜,ğ‘¡ = ğ·ğ‘˜ /2 ÃŠ ğ‘—=1 M(ğ‘¡, ğœƒ ğ‘—), (36) where Ã‰ denotes direct sum of matrices. Each M(ğ‘¡, ğœƒ ğ‘—) is a 2-D clockwise rotatory matrix of angle ğ‘¡ Â· ğœƒ ğ‘— M(ğ‘¡, ğœƒ ğ‘—) =  cos(ğ‘¡ Â· ğœƒ ğ‘—) sin(ğ‘¡ Â· ğœƒ ğ‘—) âˆ’ sin(ğ‘¡ Â· ğœƒ ğ‘—) cos(ğ‘¡ Â· ğœƒ ğ‘—) . (37)

The key advantage of this formulation is that the induced representation is translation invariant, i.e., the attention score of (qğ‘–, kğ‘—) is only related to their relative position offset qğ‘–kâŠ¤ğ‘— =  xğ‘–Wğ‘„  RÎ˜,ğ‘—âˆ’ğ‘–  xğ‘—Wğ¾  âŠ¤ . (38)

In practice, the embedding matrix multiplication can be implemented by two element-wise multiplication for lower memory footprint. The RoPE uses the form of absolute embedding but can capture relative positional relations. This approach is compatible with linearized attention in Sec. 4.2.

#### 5.1.4 Position Representations without Explicit Encoding. 
Instead of explicitly introducing additional positional encodings, Wang et al. [140] propose to encode positional information in word embeddings, by generalizing embedding to continuous (complex-valued) functions over positions.

R-Transformer [144] model locality of sequential data with a local RNN. Specifically, inputs to each block of R-Transformer are first fed to a local RNN and then to multi-Head self-attention module. The RNN structure introduces ordering information and captures local dependencies as a complement to self-attention.

Conditional positional encoding (CPE) [20] generate conditional position encodings at each layer for ViT with a 2-D convolution with zero-paddings. The intuition behind this approach is that convolution networks can implicitly encode absolute positional information with zeropaddings [60].

#### 5.1.5 Position Representation on Transformer Decoders. 
It is worth noticing that masked selfattention is not permutation equivariant [133]. Thus a model that exploits only the decoder of
Transformer has the potential of sensing positional information without incorporating explicit positional representation. This is confirmed by some empirical results on language modeling tasks [59, 113], where the authors find that removing position encodings even improves performance.


### 5.2 Layer Normalization
Layer Normalization (LN), along with residual connection, is considered as a mechanism to stabilizing training of deep networks (e.g., alleviating ill-posed gradients and model degeneration). There are some studies that are dedicated to analyzing and improving LN module.

Multi-Head Attention Position-wise FFN LayerNorm LayerNorm !â‡¥ (a) post-LN

Multi-Head Attention Position-wise FFN LayerNorm LayerNorm LayerNorm !â‡¥ (b) pre-LN

Fig. 11. Comparison of Transformer Encoder with pre-LN and post-LN.

#### 5.2.1 Placement of Layer Normalization.
In vanilla Transformer, the LN layer lies between the residual blocks, called post-LN [141]. Later Transformer implementations [67, 136] place the LN layer inside the residual connection before the attention or FFN, with an additional LN after the final layer to control the magnitude of final outputs, which is referred to as pre-LN10. The pre-LN has been adopted by numerous following research studies and implementations, e.g., [6, 17, 141].

The difference between pre-LN and post-LN is shown in Fig. 11.

Xiong et al. [151] theoretically investigate the gradients of Transformers and find that the gradients near the output layer are large at initialization in post-LN Transformers, which could be the reason why post-LN Transformers without learning rate warm-up [137] 11 leads to unstable training, whereas pre-LN Transformers do not suffer from the same problem. They thus deduce and empirically verify that warm-up stage can be safely removed for pre-LN Transformers.

Although Post-LN often results in unstable training and divergence, it usually outperforms preLN variants after convergence [83]. Similar to Xiong et al. [151], Liu et al. [83] conduct theoretical and empirical analysis and find that post-LN encoders do not suffer from gradient imbalance. They thus conjecture that the gradient issue is not the direct cause of unstable post-LN Transformer training and further identify the amplification effect in post-LN Transformers â€” at initialization, the heavier dependency on residual branch leads to a larger output shift in post-LN Transformers, thus resulting in unstable training. In light of this finding, they introduce additional parameters to post-LN Transformers to control residual dependencies of Post-LN. These parameters are initialized according to activation variations of sample data so that the output shift of post-LN Transformers is 10To the best of our knowledge, this approach is adopted since v1.1.7 in the Tensor2Tensor implementation [136]. 11Learning rate warm-up refers to starting optimization with an extremely small learning rate and then gradually increasing it to a pre-defined maximum value in a certain number of iterations. 24  not amplified. This approach ensures and boosts convergence of post-LN Transformers and reaches better performance than pre-LN Transformers.

#### 5.2.2 Substitutes of Layer Normalization. 
Xu et al. [153] empirically observe that the learnable parameters in the LN module do not work in most experiments, and even increase the risk of overfitting. They further conclude from controlled experiments that the forward normalization is not the reason why LN works for Transformer. From analysis and experiments, it is concluded that the derivatives of the mean and variance re-center and re-scale the gradients and play a significant role in LN. They thus propose AdaNorm, a normalization technique without learnable parameters z = ğ¶(1 âˆ’ ğ‘˜y) âŠ™ y, (39) y = x âˆ’ ğœ‡ ğœ , (40) where ğ¶, ğ‘˜ are hyperparameters and âŠ™ denotes element-wise multiplication. ğœ‡ and ğœ are the mean and standard deviation of input x, respectively.

Nguyen and Salazar [93] propose to replace the LN module with scaled â„“2 normalization. Given any input x of ğ‘‘-dimension, their approach project it onto a ğ‘‘ âˆ’ 1-sphere of learned radius ğ‘” z = ğ‘” xâˆ¥xâˆ¥, (41) where ğ‘” is a learnable scalar. It is more parameter efficient compared to normal LN and is shown to be effective in machine translation datasets, especially in low-resource settings.

Shen et al. [121] discuss why Batch Normalization (BN) [58] performs poorly in Transformer for text data and conclude that BNâ€™s significant performance degradation stems from the instabilities associated with its batch statistics. They thus propose PowerNorm (PN) that has three modifications over BN: 1.  it relaxes the zero-mean normalization; 2.  it uses the quadratic mean of the signal, instead of the variance; 3.  it uses running statistics for the quadratic mean, instead of using per-batch statistics. Specifically, for the ğ‘¡-th iteration, the PN computes the outputs as z(ğ‘¡) = ğ›¾ âŠ™ y(ğ‘¡) + ğ›½, (42) y(ğ‘¡) = x(ğ‘¡) ğœ“ (ğ‘¡âˆ’1) , (43) (ğœ“ (ğ‘¡))2 = ğ›¼ (ğœ“ (ğ‘¡âˆ’1))2 + (1 âˆ’ ğ›¼) |ğµ1| |ğµ| âˆ‘ï¸ğ‘–=1 (xğ‘–(ğ‘¡))2! , (44) where 0 < ğ›¼ < 1 is the moving average coefficient and ğ›¾, ğ›½ are the learnable parameters as in BN formulation.

#### 5.2.3 Normalization-free Transformer. 
Besides LN, there is another mechanism to construct deeper neural network. ReZero [5] replace LN module with a learnable residual connection. For each module ğ¹ (Â·), ReZero re-scales ğ¹ (Â·) in the residual formulation:

Hâ€² = H + ğ›¼ Â· ğ¹ (H), (45) where ğ›¼ is a learnable parameter with zero-initialization.

Replacing LN in Transformer with ReZero mechanism is verified to induce better dynamic isometry for input signals and leads to faster convergence.

### 5.3 Position-wise FFN
Despite its simplicity, the position-wise feed-forward network (FFN) layers are important for a

Transformer to achieve good performance. Dong et al. [32] observe that simply stacking selfattention modules causes a rank collapse problem, leading to token-uniformity inductive bias, and that the feed-forward layer is one of the important building blocks that mitigate this issue. Various works have explored modifications on the FFN module.

#### 5.3.1 Activation Function in FFN. 
The vanilla Transformer [137] adopts the Rectified Linear Units (ReLU) activation for non-linearity in between the two FFN layers. Over time, several studies have explored different activation other than ReLU.

Ramachandran et al. [106] try to replace ReLU in Transformer with Swish function ğ‘“ (ğ‘¥) = ğ‘¥sigmoid(ğ›½ğ‘¥) and observe that it consistently improve performance on WMT 2014 Englishâ†’German dataset.

GPT [101] replace ReLU with Gaussian Error Linear Unit (GELU) [52] on language pre-training.

It becomes the default practice for many pre-trained language models [28, 50].

Shazeer [118] explore using Gated Linear Units (GLU) [25] and its variants as a drop-in replacement for ReLU in FFN. Their pre-training experiments show that the GLU variants consistently improve vanilla Transformer with ReLU activation. Note that GLU introduces extra parameters and the experiments are conducted with the intermediate dimension of FFN reduced to match the parameter count with baseline.

#### 5.3.2 Adapting FFN for Larger Capacity. 
Several works have focused on expanding FFNs in order for a larger model capacity. The basic idea is to replace FFNs with similar structures with much more parameters.

Lample et al. [69] replace some of the FFNs with the product-key memory layers. A product-key memory is composed of three components: a query network, a key selection module containing two sets of sub-keys, and a value lookup table. The model first projects an input to a latent space using the query network, and then compares the generated query to keys that are Cartesian product of the two sets of sub-keys from key selection module to get ğ‘˜ nearest neighbors, and finally finds the corresponding values in a value lookup table using the ğ‘˜ nearest keys and aggregates them to produce the final output. This process resembles the attention mechanism, in that the generated query attends to a large number of global key-value pairs. They thus propose a multihead mechanism for the key-product memory to further enlarge the capacity of this module. The experiments on large-scale language modeling suggest that this mechanism significantly improves performance with negligible computational overhead.

Several studies exploits the idea of Mixture-of-Experts (MoE)[120] to increase the capacity of

FFNs. Gshard[71] uses sparsely-gated MoE layers to replace FFNs in Transformer. Each MoE layer consists of several FFNs (each called an expert) that are the same structure as position-wise FFNs in vanilla Transformer. The output of the layer is a weighted sum of the outputs of the FFNs, using gate values computed by a routing function ğ‘”(Â·). They design a learnable routing function that assigns tokens to experts, with auxiliary loss to satisfy balanced loads between experts and efficiency at the scale of length such that the experts can be distributed across multiple devices. For each forward pass of the MoE layer, only the experts with top-ğ‘˜ gate values are activated.

Instead of using ğ‘˜ experts for each forward pass, Switch Transformer [36] proposes to route using only a single expert with the largest gate value, leading to a much smaller computational footprint. The authors also design an auxiliary loss to encourage load balance between experts. It is reported to speed up pre-training by a large margin compared to the non-MoE counterpart while having a similar number of FLOPS. 26 

Yang et al. [155] propose to replace top-ğ‘˜ routing with expert prototyping strategy. Specifically, the proposed strategy splits experts into ğ‘˜ different groups and applies top-1 routing within each group. The outputs of prototype groups are combined linearly to form the final output of the

MoE layer. This strategy is proved to improve the model quality while maintaining constant computational costs.

As opposed to using a learnable routing function for expert assignment, Roller et al. [110] design hash layers where tokens are hashed into a fixed number of buckets, each bucket corresponding to an expert. This approach requires no routing parameters or any auxiliary loss function, while showing competitive results with existing methods such as Switch Transformer [36].

#### 5.3.3 Dropping FFN Layers. 
Notably, one might argue that under some circumstances, FFN layers can be dropped completely, resulting in a simplified network.

Sukhbaatar et al. [127] demonstrate that replacing the ReLU activation with Softmax and dropping the bias term in FFN effectively turns FFN into an attention module where position-wise inputs attend to a global key-value memory of ğ·ffn slots. They thus propose to drop the FFN module and add to the attention module a set of global key-value pairs, which are learnable parameters concatenated with key and values generated by inputs. This approach simplifies the structure of the network with no loss of performance.

Yang et al. [157] empirically show that FFNs in the decoder of Transformer, despite its large number of parameters, is not efficient and can be removed safely with only slight or no loss of performance. This approach significantly boosts the training and inference speed. 

## 6 ARCHITECTURE-LEVEL VARIANTS æ¶æ„çº§å˜ä½“
In this section, we introduce the X-formers that modify the vanilla Transformer beyond modules.

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»åœ¨æ¨¡å—ä¹‹å¤–ä¿®æ”¹åŸå§‹Transformer çš„ X-formerã€‚

### 6.1 Adapting Transformer to Be Lightweight è½»é‡åŒ–
Apart from the efforts made at the module level to alleviate computation overheads, there are several attempts to adapt Transformer to be lightweight by modifications at a higher level.

Similar to low-rank self-attention [45] that decomposes attention into a locality-constrained attention and a low-rank global attention, Lite Transformer [148] proposes to replace each attention module in Transformer with a two-branch structure, where one branch uses attention to capture long-range contexts while the other branch uses depth-wise convolution and linear layers to capture local dependencies. The architecture is lightweight both in terms of model size and computation, and is thus more suitable for mobile devices.

Funnel Transformer [23] utilizes a funnel-like encoder architecture where the length of the hidden sequence is gradually reduced using pooling along the sequence dimension, and then recovered using up-sampling. The architecture effectively reduces the FLOPs and memory compared to the vanilla Transformer encoder. Naturally, one can use this architecture to build a deeper or wider model using the same computation resources.

DeLighT [91] replaces the standard Transformer block with DeLighT block, which consists of three sub-modules: 1.  a â€œexpand-and-reduceâ€ DeLighT transformation module to learn wider representations with low computation requirements; 2.  a single-head self-attention to learn pairwise interaction; 3.  a lightweight â€œreduce-and-expandâ€ FFN (as opposed to vanilla Transformer that first expands the dimension of hidden representations and then reduces them back to ğ·ğ‘š).

They also propose a block-wise scaling strategy that allows for shallower and narrower blocks near the input and wider and deeper blocks near the output. The induced network is much deeper than the vanilla Transformer but with fewer parameters and operations.


### 6.2 Strengthening Cross-Block Connectivity  åŠ å¼ºè·¨åŒºå—è¿æ¥
In vanilla Transformer, each block takes outputs from the previous block as inputs and outputs a sequence of hidden representations. One might be interested in creating more paths along which input signals can run through the networks. In Sec. 4.5.2, we introduced Realformer [51] and

Predictive Attention Transformer [143] that reuses attention distributions from previous block to guide attention of current block. This can be seen as creating a forward path between adjacent

Transformer blocks.

In a deep Transformer encoder-decoder model, the cross-attention modules in the decoder only utilize the final outputs of the encoder, therefore the error signal will have to traverse along the depth of the encoder. This makes Transformer more susceptible to optimization issues (e.g., vanishing gradients). Transparent Attention [8] uses a weighted sum of encoder representations at all encoder layers (including the embedding layer) in each cross-attention module. For the ğ‘—-th decoder block, the cross-attention module is modified to attend to ËœH(ğ‘—) = ğ‘âˆ‘ï¸ğ‘–=0 exp(ğ‘¤ğ‘–ğ‘—) Ã ğ‘ğ‘˜=0 exp(ğ‘¤ğ‘˜ ğ‘—) H(ğ‘–), (46) where each ğ‘¤ğ‘–ğ‘— is a trainable parameter. This effectively shortens the path from each layer in the encoder to the error signal and thus eases the optimization of deeper Transformer models.

Another issue associated with vanilla Transformer is that each position can only attend to history representations from lower layers. Feedback Transformer [34] proposes to add a feedback mechanism to Transformer decoder, where each position attends to a weighted sum of history representations from all layers Ëœhğ‘– = ğ‘âˆ‘ï¸ğ‘™=0 exp(ğ‘¤ğ‘™) Ã ğ‘ğ‘˜=0 exp(ğ‘¤ğ‘˜ ) hğ‘–(ğ‘™) . (47)

### 6.3 Adaptive Computation Time  è®¡ç®—æ—¶é—´è‡ªé€‚åº”
Vanilla Transformer, like most neural models, utilizes a fixed (learned) computation procedure to process each input. An intriguing and promising modification is to make computation time conditioned on the inputs, i.e., to introduce Adaptive Computation Time (ACT) [38] into Transformer models. Such modifications potentially give rise to the following advantages:
* Feature refinement for hard examples. For data that are hard to process, a shallow representation might not be adequate to fulfill the task at hand. It would be more ideal to apply more computations to acquire a deeper and more refined representation.
* Efficiency for easy examples. When processing easy examples, a shallow representation might be enough for the task. In this case, it would be beneficial if the network can learn to extract features using reduced computation time.

Universal Transformer (UT) [26] incorporates a recurrence-over-depth mechanism that iteratively refines representations for all symbols using a module that is shared over depth, as illustrated in Fig. 12(a). It also adds a per-position dynamic halting mechanism that calculates a halting probability for each symbol at every time step. If a symbolâ€™s halting probability is greater than a predefined threshold, then the symbolâ€™s representation will remain unchanged for subsequent timesteps. The recurrence is stopped when all symbols halt or when a predefined maximum step is reached.

Conditional Computation Transformer (CCT) [7] adds a gating module at each self-attention and feed-forward layer to decide whether to skip the current layer, as illustrated in Fig. 12(b). The authors also introduce an auxiliary loss that encourages the model to adjust the gating modules to match the practical computation cost to the available computation budget. 28  yes output no


Fig. 12. Three typical ACT paradigms.

Similar to the dynamic halting mechanism used in UT, there is a line of work dedicated to adapting the number of layers to each input in order to achieve a good speed-accuracy trade-off, which is called early exit mechanism, as illustrated in Fig. 12(c). A commonly used technique is to add an internal classifier at each layer and jointly train all classifiers. The core of these methods is the criteria used to decide whether to exit at each layer. DeeBERT [150] uses the entropy of the output probability distribution of the current layer to determine whether to exit. PABEE [171] counts the number of times that the predictions remain unchanged to decide whether to exit. Li et al. [79] design a window-based uncertainty criterion to achieve token-level partial exiting for sequence labeling tasks. Sun et al. [129] introduces a voting-based exiting strategy that considers at each layer predictions of all the past internal classifiers to infer the correct label and to decide whether to exit.

### 6.4 Transformers with Divide-and-Conquer Strategies åˆ†è€Œæ²»ä¹‹ç­–ç•¥
The quadratic complexity of self-attention on sequences length can significantly limit the performance of some downstream tasks. For example, language modeling usually needs long-range context. Apart from the techniques introduced in Sec. 4, another effective way of dealing with long sequences is to use divide-and-conquer strategy, i.e., to decompose an input sequence into finer segments that can be efficiently processed by Transformer or Transformer modules. We identify two representative class of methods, recurrent and hierarchical Transformers, as illustrated in

Fig. 13. These techniques can be understood as a wrapper for the Transformer model in which Transformer acts as an elementary component that is reused to process different input segments.

Fig. 13. Illustrations of recurrent and hierarchical Transformers.


#### 6.4.1 Recurrent Transformers.  å¾ªç¯Transformers
In recurrent Transformers, a cache memory is maintained to incorporate the history information. While processing a segment of text, the network reads from the cache as an additional input. After the processing is done, the network writes to the memory by simply copying hidden states or using more complex mechanisms. The abstract process is illustrated in Fig. 13(a).

Transformer-XL [24] address the limitation of a fixed length context by caching representations from the previous segment and reuse it as an extended context when the model processes the current segment. For the ğ‘™-th layer and the (ğœ + 1)-th segment, the input representation H(ğ‘™âˆ’1) ğœ+1 is concatenated with the representation H(ğ‘™âˆ’1) ğœ from previous segment to produce the keys and values ËœH(ğ‘™) ğœ+1 = [SG(H(ğ‘™âˆ’1) ğœ ) â—¦ H(ğ‘™âˆ’1) ğœ+1 ], (48)

K(ğ‘™) ğœ+1, V(ğ‘™) ğœ+1 = ËœH(ğ‘™) ğœ+1Wğ¾, ËœH(ğ‘™) ğœ+1Wğ‘‰ , (49) where H(0) ğœ is defined as the word embedding sequence, SG(Â·) denotes stop-gradient operation and [X â—¦ Y] denotes concatenating the two vector sequences along the time dimension. This approach extends the maximum context length by ğ¿ Ã— ğ‘mem where ğ¿ is the number of layers and ğ‘mem is the length of cached memory sequence.

Compressive Transformer [103] extends this idea further by extending the cache with two levels of memory. In Transformer-XL, the activations from the previous segment are cached as a memory that is used to augment the current segment, and activations from older segments are discarded.

Compressive Transformer, on the other hand, applies a compression operation (e.g., Convolution,

Pooling, etc.) on older activations and stores them in the compressed memory. In order to avoid the expensive backpropagating-through-time (BPTT) from training compression sub-network with gradients from the loss, they propose to use local loss functions where original memories are constructed from the compressed memories. This approach further extends the theoretical maximum history context length from ğ¿ Ã—ğ‘mem of Transformer-XL to ğ¿ Ã— (ğ‘mem +ğ‘ Ã—ğ‘cm), where ğ‘ is the compression rate and ğ‘cm is the length of compressed memory.

Memformer [147] extends the recurrence mechanism from decoder-only architecture to an encoder-decoder architecture. They introduce to the encoder a memory cross attention similar to the cross attention in vanilla Transformer to allow the Transformer encoder to attend to the memory. They also introduce a memory slot attention on top of the encoder output to explicitly write the memory for the next segment. To avoid BPTT over a long range of timesteps, they propose

Memory Replay Back-Propagation (MRBP) algorithm, which replays the memory at each timestep to accomplish gradient back-propagation over long unrolls.

Yoshida et al. [160] propose a simple fine-tuning mechanism to add recurrence to a pre-trained language model (e.g., GPT-2 [102]). They first compress the representations produced by the ğœ-th segment into one single vector representation, using a weighted average of pooled representations from each layer ğ‘™ âˆˆ {1, Â· Â· Â· , ğ¿} zğœ = ğ¿âˆ‘ï¸ğ‘™=1 ğ‘¤ğ‘™ ğ‘‡ğœ âˆ‘ï¸ğ‘—=1 h(ğ‘—ğ‘™), (50) where ğ‘‡ğœ denotes the sequence length of the ğœ-th segment, ğ‘¤ğ‘™ = softmax(ğ›¼)ğ‘™ is the weight softmaxnormalized from learnable parameters ğ›¼ = [ğ›¼1, Â· Â· Â· , ğ›¼ğ¿]. This compressed representation is then fed to a feed-forward network to produce the memory state hprev,ğœ for the ğœ-th segment, which is then prepended to the key-value inputs of a specific attention layer. This approach effectively extends the context length of a pre-trained language model, without significant change of the architecture of the original model. 30 

ERNIE-Doc [30] proposes an enhanced recurrence mechanism based on the recurrence mechanism used in Transformer-XL, by replacing the memory with the history representations from the ğ‘™-th layer. ËœH(ğ‘™) ğœ+1 = [SG(H(ğ‘™) ğœ ) â—¦ H(ğ‘™âˆ’1) ğœ+1 ], (51) as opposed to using representations from the leads to a larger effective context length. (ğ‘™ âˆ’1)-th layer in Eq. (48). This modification essentially

#### 6.4.2 Hierarchical Transformers.  åˆ†å±‚
Hierarchical Transformer decomposes inputs hierarchically into elements of finer granularity. Low-level features are first fed to a Transformer encoder, producing output representations that are then aggregated (using pooling or other operations) to form a high-level feature, which is then processed by a high-level Transformer. This class of methods can be understood as a process of hierarchical abstraction. The overview of this approach is depicted in Fig. 13(b). The advantages of this approach are twofold: 1.  Hierarchical modeling allows the model to handle long inputs with limited resources; 2.  It has the potential to generate richer representations that are beneficial to tasks.

#### 6.5.2.1 Hierarchical for long sequence inputs.  é•¿åºåˆ—è¾“å…¥çš„åˆ†å±‚
For tasks with inherently long input length, one can use hierarchical Transformers for effective modeling of long-range dependencies. For documentlevel machine translation tasks, Miculicich et al. [92] introduce dependencies on the previous sentences from both the source and target sides when translating a sentence. They use an attention mechanism as the aggregation operation to summarize low-level information. For document summarization, HIBERT [166] encodes a document of text by first learn sentence representations for all sentences and then use these sentence representations to encode document-level representations that are then used to generate the summary. The model uses the last hidden representation (corresponding to the EOS token) as the representation for each sentence. Liu and Lapata [86] propose a similar hierarchical Transformer for multi-document summarization where the extracted low-level representations are aggregated using an attention layer with a global trainable query node and low-level representations as the source of key-value pairs. Hi-Transformer [145] first utilizes a sentence Transformer and a document Transformer to hierarchically learn document context-aware sentence representations. The document context-aware sentence representations are then fed to another sentence Transformer to further improve the sentence context modeling.

#### 6.5.2.2 Hierarchical for richer representations.  åˆ†å±‚ä»¥è·å¾—æ›´ä¸°å¯Œçš„è¡¨ç¤º
One might also be interested in using hierarchical models to acquire richer representations that are beneficial to the tasks at hand. For example,

TENER [154] uses a low-level Transformer encoder to encode character features, which is then concatenated with word embeddings as the inputs to the high-level Transformer encoder. This incorporates more features and alleviates the problems of data sparsity and out-of-vocabulary (OOV). Recently emerging Vision Transformer [33] divides an input image into several patches that serve as the basic input elements of Transformer, which potentially loses intrinsic pixellevel information within patches. To address this issue, Transformer in Transformer (TNT) [48] uses at each layer an inner Transformer block that transforms pixel representations and an outer

Transformer block that takes fused vectors of patch representations and pixel representations as input.

### 6.5 Exploring Alternative Architecture æ¢ç´¢æ›¿ä»£æ¶æ„
Despite the success of Transformer architecture, one might question whether the current Transformer architecture is optimal. Interestingly, several studies have explored alternative architectures for Transformer. 

Lu et al. [89] interpret Transformer as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system and design Macaron Transformer, which replaces each Transformer block with a FFN-attention-FFN variant.

Sandwich Transformer [99] explores reorganizing attention modules and FFN modules such that attention modules are mainly located in lower layers and FFN modules in upper layers. The induced model improves perplexity on multiple language modeling benchmarks, without increasing parameters, memory or training time.

Mask Attention Network (MAN) [35] prepends a dynamic mask attention module to the selfattention module in each Transformer block. The mask is conditioned on token representations, the relative distance between tokens and head indices. The proposed dynamic mask attention is shown to effectively model locality in text data and the induced model consistently outperforms the baseline model in machine translation and abstractive summarization.

Notably, thereâ€™s a line of work that uses Neural Architecture Search (NAS) to search for alternative Transformer architectures. The Evolved Transformer (ET) [123] employs evolution-based architecture search with the standard Transformer architecture seeding the initial population. The searched model demonstrates consistent improvement over Transformer on several language tasks. As another representative work, DARTSformer[167] applies differentiable architecture search (DARTS) [82], combined with a multi-split reversible network and a backpropagation-withreconstruction algorithm for memory efficiency. The resulting model consistently outperforms standard Transformer and compares favorably to larger ET models, with a significantly reduced search cost. 

## 7 PRE-TRAINED TRANSFORMERS
As a key difference from convolutional networks and recurrent networks that inherently incorporates the inductive bias of locality, Transformer does not make any assumption about how the data is structured. On the one hand, this effectively makes Transformer a very universal architecture that has the potential of capturing dependencies of different ranges. On the other hand, this makes Transformer prone to overfitting when the data is limited. One way to alleviate this issue is to introduce inductive bias into the model.

ä½œä¸ºå·ç§¯ç½‘ç»œå’Œå¾ªç¯ç½‘ç»œçš„ä¸€ä¸ªå…³é”®åŒºåˆ«ï¼ŒTransformeræ²¡æœ‰å¯¹æ•°æ®çš„ç»“æ„è¿›è¡Œä»»ä½•å‡è®¾ã€‚ä¸€æ–¹é¢ï¼Œè¿™æœ‰æ•ˆåœ°ä½¿Transformeræˆä¸ºä¸€ä¸ªéå¸¸é€šç”¨çš„æ¶æ„ï¼Œæœ‰å¯èƒ½æ•è·ä¸åŒèŒƒå›´çš„ä¾èµ–å…³ç³»ã€‚å¦ä¸€æ–¹é¢ï¼Œè¿™ä½¿å¾—Transformeråœ¨æ•°æ®æœ‰é™æ—¶å®¹æ˜“è¿‡åº¦æ‹Ÿåˆã€‚ç¼“è§£è¿™ä¸€é—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯åœ¨æ¨¡å‹ä¸­å¼•å…¥å½’çº³åå·®ã€‚

Recent studies suggest that Transformer models that are pre-trained on large corpora can learn universal language representations that are beneficial for downstream tasks [100]. The models are pre-trained using various self-supervised objectives, e.g., predicting a masked word given its context. After pre-training a model, one can simply fine-tune it on downstream datasets, instead of training a model from scratch. To illustrate typical ways of using Transformers in pre-training, we identify some of the pre-trained Transformers and categorize them as follows.
* Encoder only. A line of work uses the Transformer encoder as its backbone architecture. BERT [28] is a representative PTM that is typically used for natural language understanding tasks. It utilizes Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) as the self-supervised training objective. RoBERTa [87] further adapts the training of BERT and removes the NSP objective as it is found to hurt performance on downstream tasks.
* Decoder only. Several studies focus on pre-training Transformer decoders on language modeling. For example, the Generative Pre-trained Transformer (GPT) series (i.e., GPT [101], GPT-2 [102], and GPT-3 [12]) is dedicated to scaling pre-trained Transformer decoders and has recently illustrated that a large-scale PTM can achieve impressive few-shot performance with the task and examples fed to the model as constructed prompts [12]. 
* Encoder-Decoder architecture. BART [ . There are also PTMs that adopt Transformer encoder-decoder as the overall 72] extends the denoising objective of BERT to encoder-decoder architecture. The benefit of using an encoder-decoder architecture is that the inducing model is equipped with the ability to perform both natural language understanding and generation. T5 [104] adopts similar architecture and was one of the earliest studies that use task-specific text prefix in downstream tasks.

æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¤§å‹è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„Transformeræ¨¡å‹å¯ä»¥å­¦ä¹ å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ç›Šçš„é€šç”¨è¯­è¨€è¡¨ç¤º[100]ã€‚ä½¿ç”¨å„ç§è‡ªæˆ‘ç£ç›®æ ‡å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒï¼Œä¾‹å¦‚ï¼Œåœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹é¢„æµ‹é®è”½å•è¯ã€‚åœ¨å¯¹æ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒä¹‹åï¼Œå¯ä»¥ç®€å•åœ°åœ¨ä¸‹æ¸¸æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œè€Œä¸æ˜¯ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚ä¸ºäº†è¯´æ˜åœ¨é¢„è®­ç»ƒä¸­ä½¿ç”¨Transformersçš„å…¸å‹æ–¹æ³•ï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸€äº›é¢„è®­ç»ƒçš„Transformersï¼Œå¹¶å°†å…¶åˆ†ç±»å¦‚ä¸‹ã€‚
* ä»…ç¼–ç å™¨ã€‚ä¸€é¡¹å·¥ä½œä½¿ç”¨Transformerç¼–ç å™¨ä½œä¸ºå…¶ä¸»å¹²æ¶æ„ã€‚BERT[28]æ˜¯ä¸€ç§å…¸å‹çš„PTMï¼Œé€šå¸¸ç”¨äºè‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ã€‚å®ƒåˆ©ç”¨æ©ç è¯­è¨€å»ºæ¨¡(MLM)å’Œä¸‹ä¸€å¥é¢„æµ‹(NSP)ä½œä¸ºè‡ªç›‘ç£è®­ç»ƒç›®æ ‡ã€‚RoBERTa[87]è¿›ä¸€æ­¥è°ƒæ•´äº†BERTçš„è®­ç»ƒï¼Œå¹¶åˆ é™¤äº†NSPç›®æ ‡ï¼Œå› ä¸ºå‘ç°å®ƒä¼šå½±å“ä¸‹æ¸¸ä»»åŠ¡çš„æ€§èƒ½ã€‚
* ä»…è§£ç å™¨ã€‚ä¸€äº›ç ”ç©¶ä¾§é‡äºå¯¹Transformerè§£ç å™¨è¿›è¡Œè¯­è¨€å»ºæ¨¡çš„é¢„è®­ç»ƒã€‚ä¾‹å¦‚ï¼ŒGenerative Pre-trained Transformer(GPT)ç³»åˆ—(å³GPT[101]ã€GPT-2[102]å’ŒGPT-3[12])ä¸“é—¨ç”¨äºç¼©æ”¾é¢„è®­ç»ƒçš„Transformerè§£ç å™¨ï¼Œå¹¶ä¸”æœ€è¿‘å·²ç»è¡¨æ˜ï¼Œå¤§è§„æ¨¡PTMå¯ä»¥å®ç°ä»¤äººå°è±¡æ·±åˆ»çš„å°‘æ ·æœ¬æ€§èƒ½ï¼Œå°†ä»»åŠ¡å’Œæ ·æœ¬ä½œä¸ºæ„å»ºçš„æç¤ºæä¾›ç»™æ¨¡å‹[12]ã€‚
* ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚BART[72],ä¹Ÿæœ‰PTMé‡‡ç”¨Transformerç¼–ç å™¨-è§£ç å™¨ï¼Œå› ä¸ºæ•´ä½“å°†BERTçš„å»å™ªç›®æ ‡æ‰©å±•åˆ°ç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€‚ä½¿ç”¨ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„å¥½å¤„æ˜¯ï¼Œè¯±å¯¼æ¨¡å‹é…å¤‡æœ‰æ‰§è¡Œè‡ªç„¶è¯­è¨€ç†è§£å’Œç”Ÿæˆçš„èƒ½åŠ›ã€‚T5[104]é‡‡ç”¨äº†ç±»ä¼¼çš„æ¶æ„ï¼Œæ˜¯åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­ä½¿ç”¨ä»»åŠ¡ç‰¹å®šæ–‡æœ¬å‰ç¼€çš„æœ€æ—©ç ”ç©¶ä¹‹ä¸€ã€‚

Some of the Transformer architecture variants can also be applied to Transformer-based PTMs. For instance, BigBird [163] introduced in Sec. 4.1 is a encoder-based PTM that uses compound position-based sparse attention to enable long sequence inputs. GPT-3 [12] uses alternating dense and locally banded sparse attention (which was also introduced in Sec. 4.1) in self-attention modules. Switch Transformer [36] is an encoder-based PTM that replaces FFN layers with mixture-of-experts layers and can increase parameter count while keeping the FLOPs per example constant. 

ä¸€äº›Transformeræ¶æ„å˜ä½“ä¹Ÿå¯ä»¥åº”ç”¨äºåŸºäºTransformerçš„PTMã€‚ä¾‹å¦‚ï¼Œç¬¬4.1èŠ‚ä¸­ä»‹ç»çš„BigBird[163]æ˜¯ä¸€ç§åŸºäºç¼–ç å™¨çš„PTMï¼Œå®ƒä½¿ç”¨åŸºäºå¤åˆä½ç½®çš„ç¨€ç–æ³¨æ„åŠ›æ¥å®ç°é•¿åºåˆ—è¾“å…¥ã€‚GPT-3[12]åœ¨è‡ªæ³¨æ„æ¨¡å—ä¸­ä½¿ç”¨äº¤æ›¿çš„å¯†é›†å’Œå±€éƒ¨å¸¦çŠ¶ç¨€ç–æ³¨æ„(è¿™ä¹Ÿåœ¨ç¬¬4.1èŠ‚ä¸­ä»‹ç»)ã€‚Switch-Transformer[36]æ˜¯ä¸€ç§åŸºäºç¼–ç å™¨çš„PTMï¼Œå®ƒå°†FFNå±‚æ›¿æ¢ä¸ºä¸“å®¶å±‚çš„æ··åˆå±‚ï¼Œå¯ä»¥å¢åŠ å‚æ•°è®¡æ•°ï¼ŒåŒæ—¶ä¿æŒæ¯ä¸ªæ ·æœ¬çš„FLOPæ’å®šã€‚

## 8 APPLICATIONS OF TRANSFORMER
Transformer was originally designed for machine translation but has been widely adopted in various fields besides NLP, including CV and audio processing, due to its flexible architecture. 
1.  Natural Language Processing. Transformer and its variants have been extensively explored and applied in NLP tasks, e.g., machine translation [35, 91, 104, 123, 137], language modeling [24, 103, 111, 122] and named entity recognition [80, 154]. Massive effort has been dedicated to pre-training Transformer models on large-scale text corpora, which we believe is one of the major reasons of Transformerâ€™s wide application in NLP. 
2.  Computer Vision. Transformer have also been adapted for various vision tasks, e.g., image classification [14, 33, 88], object detection [13, 88, 168, 172], image generation [61, 94] and video processing [3, 115]. Han et al. [47] and Khan et al. [64] provide reviews on existing work of visual Transformers. We encourage readers to refer to these surveys for further understand the current research progress on Transformers in CV. 
3.  Audio Applications. Transformer can also be extended for audio-related applications, e.g., speech recognition [15, 31, 41, 97], speech synthesis [57, 76, 169], speech enhancement [65, 162] and music generation [56]. 
4.  Multimodal Applications. Owing to its flexible architecture, Transformer has also been applied in various multimodal scenarios, e.g., visual question answering [55, 75, 77, 125], visual commonsense reasoning [75, 125], caption generation [22, 81, 128], speech-to-text translation [46] and text-to-image generation [29, 81, 107]. 

Transformeræœ€åˆæ˜¯ä¸ºæœºå™¨ç¿»è¯‘è€Œè®¾è®¡çš„ï¼Œä½†ç”±äºå…¶çµæ´»çš„æ¶æ„ï¼Œå®ƒå·²è¢«å¹¿æ³›åº”ç”¨äºé™¤NLPä¹‹å¤–çš„å„ä¸ªé¢†åŸŸï¼ŒåŒ…æ‹¬CVå’ŒéŸ³é¢‘å¤„ç†ã€‚
1. è‡ªç„¶è¯­è¨€å¤„ç†ã€‚TransformeråŠå…¶å˜ä½“å·²è¢«å¹¿æ³›æ¢ç´¢å¹¶åº”ç”¨äºNLPä»»åŠ¡ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘[35ã€91ã€104ã€123ã€137]ã€è¯­è¨€å»ºæ¨¡[24ã€103ã€111ã€122]å’Œå‘½åå®ä½“è¯†åˆ«[80ã€154]ã€‚å¤§é‡å·¥ä½œè‡´åŠ›äºåœ¨å¤§è§„æ¨¡æ–‡æœ¬è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒTransformeræ¨¡å‹ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯Transformeråœ¨NLPä¸­å¹¿æ³›åº”ç”¨çš„ä¸»è¦åŸå› ä¹‹ä¸€ã€‚
2. è®¡ç®—æœºè§†è§‰ã€‚Transformerè¿˜é€‚ç”¨äºå„ç§è§†è§‰ä»»åŠ¡ï¼Œä¾‹å¦‚å›¾åƒåˆ†ç±»[14ï¼Œ33ï¼Œ88]ã€å¯¹è±¡æ£€æµ‹[13ï¼Œ88ï¼Œ168ï¼Œ172]ã€å›¾åƒç”Ÿæˆ[61ï¼Œ94]å’Œè§†é¢‘å¤„ç†[3ï¼Œ115]ã€‚Hanet al [47]å’ŒKhanet al [64]å¯¹è§†è§‰Transformerçš„ç°æœ‰å·¥ä½œè¿›è¡Œäº†å®¡æŸ¥ã€‚æˆ‘ä»¬é¼“åŠ±è¯»è€…å‚è€ƒè¿™äº›è°ƒæŸ¥ï¼Œä»¥è¿›ä¸€æ­¥äº†è§£CV.
3. éŸ³é¢‘åº”ç”¨ã€‚Transformerè¿˜å¯ä»¥æ‰©å±•åˆ°éŸ³é¢‘ç›¸å…³åº”ç”¨ï¼Œä¾‹å¦‚è¯­éŸ³è¯†åˆ«[15ã€31ã€41ã€97]ã€è¯­éŸ³åˆæˆ[57ã€76ã€169]ã€è¯­éŸ³å¢å¼º[65ã€162]å’ŒéŸ³ä¹ç”Ÿæˆ[56]ã€‚
4. å¤šæ¨¡å¼åº”ç”¨ã€‚ç”±äºå…¶çµæ´»çš„æ¶æ„ï¼ŒTransformerè¿˜è¢«åº”ç”¨äºå„ç§å¤šæ¨¡å¼åœºæ™¯ï¼Œä¾‹å¦‚ï¼Œè§†è§‰é—®ç­”[55ï¼Œ75ï¼Œ77ï¼Œ125]ï¼Œè§†è§‰å¸¸è¯†æ¨ç†[75ï¼Œ125]ã€å­—å¹•ç”Ÿæˆ[22ï¼Œ81ï¼Œ128]ã€è¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘[46]å’Œæ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆ[29ï¼Œ81ï¼Œ107]ã€‚

## 9 CONCLUSION AND FUTURE DIRECTIONS
In this survey, we conduct a comprehensive overview of X-formers and propose a new taxonomy. Most of the existing works improve Transformer from different perspectives, such as efficiency, generalization, and applications. The improvements include incorporating structural prior, designing lightweight architecture, pre-training, and so on.

åœ¨æœ¬æ¬¡è°ƒæŸ¥ä¸­ï¼Œæˆ‘ä»¬å¯¹X-formersè¿›è¡Œäº†å…¨é¢çš„æ¦‚è¿°ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„åˆ†ç±»æ³•ã€‚å¤§å¤šæ•°ç°æœ‰çš„å·¥ä½œä»ä¸åŒçš„è§’åº¦æ”¹è¿›äº†Transformerï¼Œä¾‹å¦‚æ•ˆç‡ã€é€šç”¨æ€§å’Œåº”ç”¨ç¨‹åºã€‚è¿™äº›æ”¹è¿›åŒ…æ‹¬ç»“åˆç»“æ„å…ˆéªŒã€è®¾è®¡è½»é‡çº§æ¶æ„ã€é¢„è®­ç»ƒç­‰ã€‚

Although X-formers have proven their power for various tasks, challenges still exist. Besides the current concerns (e.g. efficiency and generalization), the further improvements of Transformer may lie in the following directions: 
1.  Theoretical Analysis. The architecture of Transformer has been demonstrated to be capable of supporting large-scale training datasets with enough parameters. Many works show that Transformer has a larger capacity than CNNs and RNNs and hence has the ability to handle a huge amount of training data. When Transformer is trained on sufficient data, it usually has better performances than CNNs or RNNs. An intuitive explanation is that Transformer has few prior assumptions on the data structure and therefore is more flexible than CNNs and RNNs. However, the theoretical reason is unclear and we need some theoretical analysis of Transformer ability. 
2.  Better Global Interaction Mechanism beyond Attention. A main advantage of Transformer is the use of the attention mechanism to model the global dependencies among nodes within input data. However, many studies have shown that full attention is unnecessary for most nodes. It is, to some degree, inefficient to indistinguishably calculate attention for all nodes. Therefore, there is still plenty of room for improvements in efficiently modeling global interactions. On the one hand, the self-attention module can be regarded as a fully-connected neural network with dynamical connection weights, which aggregates non-local information with dynamic routing. Therefore, other dynamic routing mechanisms are alternative approaches worth exploring. On the other hand, the global interaction can also be modeled by other types of neural networks, such as memory-enhanced models. 
3.  Unified Framework for Multimodal Data. In many application scenarios, integrating multimodal data is useful and necessary to boost the task performance. Moreover, the general AI also needs the ability to capture the semantic relations across different modalities. Since Transformer achieves great success on text, image, video, and audio, we have a chance to build a unified framework and better capture the inherent connections among multimodal data. However, the design of the intra-modal and cross-modal attention still remains to be improved.

å°½ç®¡X-formerså·²ç»è¯æ˜äº†ä»–ä»¬åœ¨å„ç§ä»»åŠ¡ä¸­çš„èƒ½åŠ›ï¼Œä½†æŒ‘æˆ˜ä»ç„¶å­˜åœ¨ã€‚é™¤äº†å½“å‰çš„é—®é¢˜(ä¾‹å¦‚æ•ˆç‡å’Œé€šç”¨æ€§)ï¼ŒTransformerçš„è¿›ä¸€æ­¥æ”¹è¿›å¯èƒ½åœ¨ä»¥ä¸‹æ–¹é¢ï¼š
1. ç†è®ºåˆ†æã€‚Transformerçš„æ¶æ„å·²è¢«è¯æ˜èƒ½å¤Ÿæ”¯æŒå…·æœ‰è¶³å¤Ÿå‚æ•°çš„å¤§è§„æ¨¡è®­ç»ƒæ•°æ®é›†ã€‚è®¸å¤šç ”ç©¶è¡¨æ˜ï¼ŒTransformerçš„å®¹é‡å¤§äºCNNå’ŒRNNï¼Œå› æ­¤èƒ½å¤Ÿå¤„ç†å¤§é‡è®­ç»ƒæ•°æ®ã€‚å½“Transformeråœ¨è¶³å¤Ÿçš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒæ—¶ï¼Œå®ƒé€šå¸¸æ¯”CNNæˆ–RNNå…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚ç›´è§‚çš„è§£é‡Šæ˜¯ï¼ŒTransformerå¯¹æ•°æ®ç»“æ„å‡ ä¹æ²¡æœ‰é¢„å…ˆçš„å‡è®¾ï¼Œå› æ­¤æ¯”CNNå’ŒRNNæ›´çµæ´»ã€‚ç„¶è€Œï¼Œç†è®ºåŸå› å°šä¸æ¸…æ¥šï¼Œæˆ‘ä»¬éœ€è¦å¯¹å˜å‹å™¨èƒ½åŠ›è¿›è¡Œä¸€äº›ç†è®ºåˆ†æã€‚
2. è¶…è¶Šæ³¨æ„åŠ›çš„æ›´å¥½çš„å…¨å±€äº’åŠ¨æœºåˆ¶ã€‚Transformerçš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥å»ºæ¨¡è¾“å…¥æ•°æ®ä¸­èŠ‚ç‚¹ä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚ç„¶è€Œï¼Œè®¸å¤šç ”ç©¶è¡¨æ˜ï¼Œå¯¹å¤§å¤šæ•°èŠ‚ç‚¹æ¥è¯´ï¼Œå®Œå…¨å…³æ³¨æ˜¯ä¸å¿…è¦çš„ã€‚åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œæ— æ³•åŒºåˆ†åœ°è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„æ³¨æ„åŠ›æ˜¯ä½æ•ˆçš„ã€‚å› æ­¤ï¼Œåœ¨æœ‰æ•ˆåœ°å»ºæ¨¡å…¨å±€äº¤äº’æ–¹é¢ä»æœ‰å¾ˆå¤§çš„æ”¹è¿›ç©ºé—´ã€‚ä¸€æ–¹é¢ï¼Œè‡ªæ³¨æ„æ¨¡å—å¯ä»¥è¢«è§†ä¸ºå…·æœ‰åŠ¨æ€è¿æ¥æƒé‡çš„å®Œå…¨è¿æ¥çš„ç¥ç»ç½‘ç»œï¼Œå®ƒé€šè¿‡åŠ¨æ€è·¯ç”±èšåˆéå±€éƒ¨ä¿¡æ¯ã€‚å› æ­¤ï¼Œå…¶ä»–åŠ¨æ€è·¯ç”±æœºåˆ¶æ˜¯å€¼å¾—æ¢ç´¢çš„æ›¿ä»£æ–¹æ³•ã€‚å¦ä¸€æ–¹é¢ï¼Œå…¨å±€äº¤äº’ä¹Ÿå¯ä»¥é€šè¿‡å…¶ä»–ç±»å‹çš„ç¥ç»ç½‘ç»œæ¥å»ºæ¨¡ï¼Œä¾‹å¦‚è®°å¿†å¢å¼ºæ¨¡å‹ã€‚
3. å¤šå¼è”è¿æ•°æ®ç»Ÿä¸€æ¡†æ¶ã€‚åœ¨è®¸å¤šåº”ç”¨åœºæ™¯ä¸­ï¼Œé›†æˆå¤šæ¨¡æ€æ•°æ®å¯¹äºæé«˜ä»»åŠ¡æ€§èƒ½æ˜¯æœ‰ç”¨çš„ï¼Œä¹Ÿæ˜¯å¿…è¦çš„ã€‚æ­¤å¤–ï¼Œé€šç”¨AIè¿˜éœ€è¦æ•è·ä¸åŒæ¨¡æ€ä¹‹é—´çš„è¯­ä¹‰å…³ç³»çš„èƒ½åŠ›ã€‚ç”±äºTransformeråœ¨æ–‡æœ¬ã€å›¾åƒã€è§†é¢‘å’ŒéŸ³é¢‘æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œæˆ‘ä»¬æœ‰æœºä¼šæ„å»ºä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶ï¼Œæ›´å¥½åœ°æ•æ‰å¤šæ¨¡æ€æ•°æ®ä¹‹é—´çš„å†…åœ¨è”ç³»ã€‚ç„¶è€Œï¼Œæ¨¡æ€å†…å’Œè·¨æ¨¡æ€æ³¨æ„çš„è®¾è®¡ä»æœ‰å¾…æ”¹è¿›ã€‚

Finally, we wish this survey to be a hands-on reference for better understanding the current research progress on Transformers and help readers to further improve Transformers for various applications.

æœ€åï¼Œæˆ‘ä»¬å¸Œæœ›æœ¬æ¬¡è°ƒæŸ¥èƒ½å¤Ÿæˆä¸ºæ›´å¥½åœ°äº†è§£Transformerså½“å‰ç ”ç©¶è¿›å±•çš„å®é™…å‚è€ƒï¼Œå¹¶å¸®åŠ©è¯»è€…è¿›ä¸€æ­¥æ”¹è¿›å˜å‹å™¨çš„å„ç§åº”ç”¨ã€‚

## References
1. Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. ETC: Encoding Long and Structured Inputs in Transformers. In Proceedings of EMNLP. Online, 268â€“284. https://doi.org/10.18653/v1/2020.emnlp-main.19
2. Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-Level Language Modeling with Deeper Self-Attention. In Proceedings of AAAI. 3159â€“3166. https://doi.org/10.1609/aaai.v33i01.33013159
3. Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuÄiÄ‡, and Cordelia Schmid. 2021. ViViT: A Video Vision Transformer. arXiv:2103.15691 cs.CV.
4. Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR abs/1607.06450 (2016). arXiv:1607.06450
5. Thomas Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, Garrison W. Cottrell, and Julian J. McAuley.2020. ReZero is All You Need: Fast Convergence at Large Depth. CoRR abs/2003.04887 (2020). arXiv:2003.04887
6. Alexei Baevski and Michael Auli. 2019. Adaptive Input Representations for Neural Language Modeling. In Proceedings of ICLR. https://openreview.net/forum?id=ByxZX20qFQ
7. Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 2020. Controlling Computation versus Quality for Neural Sequence Models. arXiv:2002.07106 cs.LG.
8. Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. 2018. Training Deeper Neural Machine Translation Models with Transparent Attention. In Proceedings of EMNLP. Brussels, Belgium, 3028â€“3033. https://doi.org/10.18653/ v1/D18-1338
9. Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. 2018. Relational inductive biases, deep learning, and graph networks. arXiv:1806.01261 cs.LG.
10. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. arXiv:2004.05150 cs.CL.
11. Srinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank J. Reddi, and Sanjiv Kumar. 2020. Low-Rank Bottleneck in Multi-head Attention Models. In Proceedings of ICML. 864â€“873. http://proceedings.mlr.press/v119/ bhojanapalli20a.html
12. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, 34  Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Proceedings of NeurIPS. 1877â€“1901. https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf
13. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-End Object Detection with Transformers. In Proceedings of ECCV. 213â€“229. https://doi.org/10.1007/978-3- 030-58452-8_13
14. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. 2020. Generative Pretraining From Pixels. In Proceedings of ICML. 1691â€“1703. http://proceedings.mlr.press/v119/chen20s.html
15. Xie Chen, Yu Wu, Zhenghao Wang, Shujie Liu, and Jinyu Li. 2021. Developing Real-time Streaming Transformer Transducer for Speech Recognition on Large-scale Dataset. arXiv:2010.11395 cs.CL.
16. Ziye Chen, Mingming Gong, Lingjuan Ge, and Bo Du. 2020. Compressed Self-Attention for Deep Metric Learning with Low-Rank Approximation. In Proceedings of IJCAI. 2058â€“2064. https://doi.org/10.24963/ijcai.2020/285
17. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers. arXiv:1904.10509 cs.LG.
18. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers. arXiv:2006.03555 cs.LG.
19. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Rethinking Attention with Performers. arXiv:2009.14794 cs.LG.
20. Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. 2021. Conditional Positional Encodings for Vision Transformers. arXiv:2102.10882 cs.CV.
21. Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. 2020. Multi-Head Attention: Collaborate Instead of Concatenate. CoRR abs/2006.16362 (2020). arXiv:2006.16362
22. Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. Meshed-Memory Transformer for Image Captioning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, 10575â€“10584. https://doi.org/10.1109/CVPR42600.2020.01059
23. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. 2020. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. In Proceedings of NeurIPS. https://proceedings.neurips.cc/paper/2020/hash/ 2cd2915e69546904e4e5d4a2ac9e1652-Abstract.html
24. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. TransformerXL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings of ACL. Florence, Italy, 2978â€“2988. https://doi.org/10.18653/v1/P19-1285
25. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. 2017. Language Modeling with Gated Convolutional Networks. In Proceedings of ICML. 933â€“941. http://proceedings.mlr.press/v70/dauphin17a.html
26. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal Transformers. In Proceedings of ICLR. https://openreview.net/forum?id=HyzdRiR9Y7
27. Ameet Deshpande and Karthik Narasimhan. 2020. Guiding Attention for Self-Supervised Learning with Transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020. Online, 4676â€“4686. https://doi.org/10.18653/ v1/2020.findings-emnlp.419
28. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of HLT-NAACL. Minneapolis, Minnesota, 4171â€“4186. https://doi.org/10.18653/v1/N19-1423
29. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. 2021. CogView: Mastering Text-to-Image Generation via Transformers. arXiv:2105.13290 cs.CV.
30. Siyu Ding, Junyuan Shang, Shuohuan Wang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE-DOC: The Retrospective Long-Document Modeling Transformer. (2020). arXiv:2012.15688 cs.CL.
31. Linhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition. In Proceedings of ICASSP. 5884â€“5888. https://doi.org/10.1109/ICASSP.2018.8462506
32. Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. CoRR abs/2103.03404 (2021). arXiv:2103.03404
33. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929 cs.CV.
34. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. 2021. Addressing Some Limitations of Transformers with Feedback Memory. https://openreview.net/forum?id=OCm0rwa1lx1  35
35. Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang, and Xuanjing Huang. 2021. Mask Attention Networks: Rethinking and Strengthen Transformer. In Proceedings of NAACL. 1692â€“1701. https://www.aclweb.org/anthology/2021.naacl-main.135
36. William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. CoRR abs/2101.03961 (2021). arXiv:2101.03961
37. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. In Proceedings of ICML. 1243â€“1252.
38. Alex Graves. 2016. Adaptive Computation Time for Recurrent Neural Networks. CoRR abs/1603.08983 (2016). arXiv:1603.08983
39. Jiatao Gu, Qi Liu, and Kyunghyun Cho. 2019. Insertion-based Decoding with Automatically Inferred Generation Order. Trans. Assoc. Comput. Linguistics 7 (2019), 661â€“676. https://transacl.org/ojs/index.php/tacl/article/view/1732
40. Shuhao Gu and Yang Feng. 2019. Improving Multi-head Attention with Capsule Networks. In Proceedings of NLPCC. 314â€“326. https://doi.org/10.1007/978-3-030-32233-5_25
41. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. In Proceedings of Interspeech. 5036â€“5040. https://doi.org/10.21437/Interspeech.2020-3015
42. Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaussian Transformer: A Lightweight Approach for Natural Language Inference. In Proceedings of AAAI. 6489â€“6496. https://doi.org/10.1609/aaai.v33i01.33016489
43. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer. In Proceedings of HLT-NAACL. 1315â€“1325. https://www.aclweb.org/anthology/N19-1133
44. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, and Zheng Zhang. 2020. Multi-Scale Self-Attention for Text Classification. In Proceedings of AAAI. 7847â€“7854. https://aaai.org/ojs/index.php/AAAI/article/view/6290
45. Qipeng Guo, Xipeng Qiu, Xiangyang Xue, and Zheng Zhang. 2019. Low-Rank and Locality Constrained SelfAttention for Sequence Modeling. IEEE/ACM Trans. Audio, Speech and Lang. Proc. 27, 12 (2019), 2213â€“2222. https: //doi.org/10.1109/TASLP.2019.2944078
46. Chi Han, Mingxuan Wang, Heng Ji, and Lei Li. 2021. Learning Shared Semantic Space for Speech-to-Text Translation. arXiv:2105.03095 cs.CL.
47. Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao. 2021. A Survey on Visual Transformer. arXiv:2012.12556 cs.CV.
48. Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. 2021. Transformer in Transformer. arXiv:2103.00112 cs.CV.
49. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings CVPR. 770â€“778. https://doi.org/10.1109/CVPR.2016.90
50. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DeBERTa: Decoding-enhanced BERT with Disentangled Attention. arXiv:2006.03654
51. Ruining He, Anirudh Ravula, Bhargav Kanagal, and Joshua Ainslie. 2020. RealFormer: Transformer Likes Residual Attention. arXiv:2012.11747 cs.LG.
52. Dan Hendrycks and Kevin Gimpel. 2020. Gaussian Error Linear Units (GELUs). arXiv:1606.08415 cs.LG.
53. Geoffrey E. Hinton, Sara Sabour, and Nicholas Frosst. 2018. Matrix capsules with EM routing. In Proceedings of ICLR. https://openreview.net/forum?id=HJWLfGWRb
54. Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. 2019. Axial Attention in Multidimensional Transformers. CoRR abs/1912.12180 (2019). arXiv:1912.12180
55. Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Marcus Rohrbach. 2020. Iterative Answer Prediction With PointerAugmented Multimodal Transformers for TextVQA. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. 9989â€“9999. https://doi.org/10.1109/CVPR42600.2020.01001
56. Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. 2019. Music Transformer. In Proceedings of ICLR. https://openreview.net/forum?id=rJe4ShAcF7
57. Hyeong Rae Ihm, Joun Yeop Lee, Byoung Jin Choi, Sung Jun Cheon, and Nam Soo Kim. 2020. Reformer-TTS: Neural Speech Synthesis with Reformer Network. In Proceedings of Interspeech, Helen Meng, Bo Xu, and Thomas Fang Zheng (Eds.). 2012â€“2016. https://doi.org/10.21437/Interspeech.2020-2189
58. Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of ICML. 448â€“456. http://proceedings.mlr.press/v37/ioffe15.html
59. Kazuki Irie, Albert Zeyer, Ralf SchlÃ¼ter, and Hermann Ney. 2019. Language Modeling with Deep Transformers. In Proceedings of Interspeech. 3905â€“3909. https://doi.org/10.21437/Interspeech.2019-2225 36 
60. Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce. 2020. How much Position Information Do Convolutional Neural Networks Encode?. In Proceedings of ICLR. https://openreview.net/forum?id=rJeB36NKvB
61. Yifan Jiang, Shiyu Chang, and Zhangyang Wang. 2021. TransGAN: Two Transformers Can Make One Strong GAN. arXiv:2102.07074 cs.CV.
62. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of ICML. 5156â€“5165. http://proceedings.mlr.press/ v119/katharopoulos20a.html
63. Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking Positional Encoding in Language Pre-training. arXiv:2006.15595 cs.CL.
64. Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah.2021. Transformers in Vision: A Survey. arXiv:2101.01169 cs.CV.
65. Jaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee. 2020. T-GSA: Transformer with Gaussian-Weighted SelfAttention for Speech Enhancement. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020. IEEE, 6649â€“6653. https://doi.org/10.1109/ICASSP40776.2020.9053591
66. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer. In Proceedings of ICLR. https://openreview.net/forum?id=rkgNKkHtvB
67. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. In Proceedings of ACL. 67â€“72. https://www.aclweb.org/anthology/P17-4012
68. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the Dark Secrets of BERT. In Proceedings of EMNLP-IJCNLP. 4364â€“4373. https://doi.org/10.18653/v1/D19-1445
69. Guillaume Lample, Alexandre Sablayrolles, Marcâ€™Aurelio Ranzato, Ludovic Denoyer, and HervÃ© JÃ©gou. 2019. Large Memory Layers with Product Keys. In Proceedings of NeurIPS. 8546â€“8557. https://proceedings.neurips.cc/paper/2019/ hash/9d8df73a3cfbf3c5b47bc9b50f214aff-Abstract.html
70. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. 2019. Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. In Proceedings of ICML. 3744â€“3753. http://proceedings.mlr.press/v97/lee19d.html
71. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. CoRR abs/2006.16668 (2020). arXiv:2006.16668
72. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of ACL. 7871â€“7880. https://doi.org/10.18653/v1/2020.acl-main.703
73. Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Lyu, and Tong Zhang. 2018. Multi-Head Attention with Disagreement Regularization. In Proceedings of EMNLP. Brussels, Belgium, 2897â€“2903. https://doi.org/10.18653/v1/D18-1317
74. Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Lyu, and Zhaopeng Tu. 2019. Information Aggregation for Multi-Head Attention with Routing-by-Agreement. In Proceedings of HLT-NAACL. 3566â€“3575. https://doi.org/10. 18653/v1/N19-1359
75. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A Simple and Performant Baseline for Vision and Language. arXiv:1908.03557 cs.CV.
76. Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019. Neural Speech Synthesis with Transformer Network. In Proceedings of AAAI. 6706â€“6713. https://doi.org/10.1609/aaai.v33i01.33016706
77. Wei Li, Can Gao, Guocheng Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2020. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. arXiv preprint arXiv:2012.15409 (2020).
78. Xiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu, and Jiwei Li. 2020. SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection. In Proceedings of NeurIPS. https://proceedings.neurips.cc/paper/2020/ hash/c5c1bda1194f9423d744e0ef67df94ee-Abstract.html
79. Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. 2021. Accelerating BERT Inference for Sequence Labeling via Early-Exit. arXiv:2105.13878 cs.CL.
80. Xiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing Huang. 2020. FLAT: Chinese NER Using Flat-Lattice Transformer. In Proceedings of ACL. 6836â€“6842. https://doi.org/10.18653/v1/2020.acl-main.611
81. Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Li, Xiaodong Deng, Jie Liu, Jinbao Xue, Huiling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. 2021. M6: A Chinese Multimodal Pretrainer. arXiv:2103.00823 cs.CL.
82. Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable Architecture Search. In Proceedings of ICLR. https://openreview.net/forum?id=S1eYHoC5FX  37
83. Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020. Understanding the Difficulty of Training Transformers. In Proceedings of EMNLP. 5747â€“5763. https://doi.org/10.18653/v1/2020.emnlp-main.463
84. Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing Long Sequences. In Proceedings of ICLR. https://openreview.net/forum?id= Hyg0vbWC-
85. Xuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and Cho-Jui Hsieh. 2020. Learning to Encode Position for Transformer with Continuous Dynamical Model. In Proceedings of ICML. 6327â€“6335. http://proceedings.mlr.press/v119/liu20n.html
86. Yang Liu and Mirella Lapata. 2019. Hierarchical Transformers for Multi-Document Summarization. In Proceedings of ACL. Florence, Italy, 5070â€“5081. https://doi.org/10.18653/v1/P19-1500
87. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 cs.CL.
88. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2103.14030 cs.CV.
89. Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2020. Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View. https://openreview.net/forum? id=SJl1o2NFwS
90. Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. Luna: Linear Unified Nested Attention. arXiv:2106.01540 cs.LG.
91. Sachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2020. DeLighT: Very Deep and Light-weight Transformer. arXiv:2008.00623 cs.LG.
92. Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and James Henderson. 2018. Document-Level Neural Machine Translation with Hierarchical Attention Networks. In Proceedings of EMNLP. Brussels, Belgium, 2947â€“2954. https: //doi.org/10.18653/v1/D18-1325
93. Toan Q. Nguyen and Julian Salazar. 2019. Transformers without Tears: Improving the Normalization of Self-Attention. CoRR abs/1910.05895 (2019). arXiv:1910.05895
94. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image Transformer. In Proceedings of ICML. 4052â€“4061. http://proceedings.mlr.press/v80/parmar18a.html
95. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random Feature Attention. In Proceedings of ICLR. https://openreview.net/forum?id=QtTKTdVrFBB
96. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. 2018. FiLM: Visual Reasoning with a General Conditioning Layer. In Proceedings of AAAI. 3942â€“3951. https://www.aaai.org/ocs/index.php/AAAI/ AAAI18/paper/view/16528
97. Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus MÃ¼ller, and Alex Waibel. 2019. Very Deep Self-Attention Networks for End-to-End Speech Recognition. In Proceedings of Interspeech. 66â€“70. https://doi.org/10.21437/ Interspeech.2019-2702
98. Jonathan Pilault, Amine El hattami, and Christopher Pal. 2021. Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data. In Proceedings of ICLR. https://openreview.net/ forum?id=de11dbHzAMF
99. Ofir Press, Noah A. Smith, and Omer Levy. 2020. Improving Transformer Models by Reordering their Sublayers. In Proceedings of ACL. Online, 2996â€“3005. https://doi.org/10.18653/v1/2020.acl-main.270
100. Xipeng Qiu, TianXiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained Models for Natural Language Processing: A Survey. SCIENCE CHINA Technological Sciences 63, 10 (2020), 1872â€“1897. https://doi.org/10.1007/s11431-020-1647-3
101. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. (2018).
102. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
103. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. 2020. Compressive Transformers for Long-Range Sequence Modelling. In Proceedings of ICLR. https://openreview.net/forum?id= SylKikSYDH
104. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:1910.10683 cs.LG.
105. Ali Rahimi and Benjamin Recht. 2007. Random Features for Large-Scale Kernel Machines. In Proceedings of NeurIPS. 1177â€“1184. https://proceedings.neurips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html
106. Prajit Ramachandran, Barret Zoph, and Quoc V. Le. 2018. Searching for Activation Functions. In Proceedings of ICLR. https://openreview.net/forum?id=Hkuq2EkPf 38 
107. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.2021. Zero-Shot Text-to-Image Generation. arXiv:2102.12092 cs.CV.
108. Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Proceedings of NeurIPS. 506â€“516. https://proceedings.neurips.cc/paper/2017/hash/ e7b24b112a44fdd9ee93bdf998c6ca0e-Abstract.html
109. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. 2021. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences 118, 15 (2021). https: //doi.org/10.1073/pnas.2016239118
110. Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, and Jason Weston. 2021. Hash Layers For Large Sparse Models. arXiv:2106.04426 cs.LG.
111. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient Content-Based Sparse Attention with Routing Transformers. arXiv:2003.05997 cs.LG.
112. Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. 2017. Dynamic Routing Between Capsules. In Proceedings of NeurIPS. 3856â€“3866. https://proceedings.neurips.cc/paper/2017/hash/2cad8fa47bbef282badbb8de5374b894-Abstract. html
113. Imanol Schlag, Kazuki Irie, and JÃ¼rgen Schmidhuber. 2021. Linear Transformers Are Secretly Fast Weight Memory Systems. CoRR abs/2102.11174 (2021). arXiv:2102.11174
114. Philippe Schwaller, Teodoro Laino, ThÃ©ophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee. 2019. Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. ACS Central Science 5, 9 (2019), 1572â€“1583. https://doi.org/10.1021/acscentsci.9b00576
115. Jie Shao, Xin Wen, Bingchen Zhao, and Xiangyang Xue. 2021. Temporal Context Aggregation for Video Retrieval With Contrastive Learning. In Proceedings of WACV. 3268â€“3278.
116. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In Proceedings of HLT-NAACL. New Orleans, Louisiana, 464â€“468. https://doi.org/10.18653/v1/N18-2074
117. Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You Need. CoRR abs/1911.02150 (2019). arXiv:1911.02150
118. Noam Shazeer. 2020. GLU Variants Improve Transformer. arXiv:2002.05202 cs.LG.
119. Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, and Le Hou. 2020. Talking-Heads Attention. CoRR abs/2003.02436 (2020). arXiv:2003.02436
120. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean.2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. In Proceedings of ICLR.
 https://openreview.net/forum?id=B1ckMDqlg
121. Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2020. PowerNorm: Rethinking Batch Normalization in Transformers. In Proceedings of ICML. 8741â€“8751. http://proceedings.mlr.press/v119/shen20e.html
122. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2020. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv:1909.08053 cs.CL.
123. David R. So, Quoc V. Le, and Chen Liang. 2019. The Evolved Transformer. In Proceedings of ICML. 5877â€“5886. http://proceedings.mlr.press/v97/so19a.html
124. Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864
125. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In Proceedings of ICLR. https://openreview.net/forum?id=SygXPaEYvH
126. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive Attention Span in Transformers. In Proceedings of ACL. Florence, Italy, 331â€“335. https://doi.org/10.18653/v1/P19-1032
127. Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. 2019. Augmenting Self-attention with Persistent Memory. arXiv:1907.01470 cs.LG.
128. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. VideoBERT: A Joint Model for Video and Language Representation Learning. In Proceedings of ICCV. 7463â€“7472. https://doi.org/10.1109/ICCV.2019. 00756
129. Tianxiang Sun, Yunhua Zhou, Xiangyang Liu, Xinyu Zhang, Hao Jiang, Zhao Cao, Xuanjing Huang, and Xipeng Qiu. 2021. Early Exiting with Ensemble Internal Classifiers. arXiv:2105.13792 cs.CL.
130. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Proceedings of NeurIPS. 3104â€“3112. https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2- Abstract.html
131. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2020. Synthesizer: Rethinking Self-Attention in Transformer Models. CoRR abs/2005.00743 (2020). arXiv:2005.00743  39
132. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse Sinkhorn Attention. In Proceedings of ICML. 9438â€“9447. http://proceedings.mlr.press/v119/tay20a.html
133. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer Dissection: An Unified Understanding for Transformerâ€™s Attention via the Lens of Kernel. In Proceedings of EMNLP-IJCNLP. Hong Kong, China, 4344â€“4353. https://doi.org/10.18653/v1/D19-1443
134. AÃ¤ron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. In Proceedings of ISCA. 125. http://www.isca-speech.org/archive/SSW_2016/abstracts/ssw9_DS-4_van_den_Oord.html
135. AÃ¤ron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018). arXiv:1807.03748
136. Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones, Åukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam Shazeer, and Jakob Uszkoreit. 2018. Tensor2Tensor for Neural Machine Translation. In Proceedings of AMTA. 193â€“199. https://www.aclweb.org/anthology/W18-1819
137. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Proceedings of NeurIPS. 5998â€“6008. https://proceedings.neurips.cc/ paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html
138. Apoorv Vyas, Angelos Katharopoulos, and FranÃ§ois Fleuret. 2020. Fast Transformers with Clustered Attention. arXiv:2007.04825 cs.LG.
139. Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and Jakob Grue Simonsen. n.d... On Position Embeddings in BERT, url = https://openreview.net/forum?id=onxoVA9FxMw, year = 2021. In Proceedings of ICLR.
140. Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen. 2020. Encoding word order in complex embeddings. In Proceedings of ICLR. https://openreview.net/forum?id=Hke-WTVtwr
141. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning Deep Transformer Models for Machine Translation. In Proceedings of ACL. 1810â€“1822. https://doi.org/10.18653/v1/p19-1176
142. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Linformer: Self-Attention with Linear Complexity. arXiv:2006.04768 cs.LG.
143. Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang Zhang, Jing Bai, Jing Yu, Ce Zhang, and Yunhai Tong. 2021. Predictive Attention Transformer: Improving Transformer with Attention Map Prediction. https://openreview.net/ forum?id=YQVjbJPnPc9
144. Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. 2019. R-Transformer: Recurrent Neural Network Enhanced Transformer. CoRR abs/1907.05572 (2019). arXiv:1907.05572
145. Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. arXiv:2106.01040 cs.CL.
146. Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. 2019. Pay Less Attention with Lightweight and Dynamic Convolutions. In Proceedings of ICLR. https://openreview.net/forum?id=SkVhlh09tX
147. Qingyang Wu, Zhenzhong Lan, Jing Gu, and Zhou Yu. 2020. Memformer: The Memory-Augmented Transformer. arXiv:2010.06891 cs.CL.
148. Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. 2020. Lite Transformer with Long-Short Range Attention. In Proceedings of ICLR. https://openreview.net/forum?id=ByeMPlHKPH
149. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst. 32, 1 (2021), 4â€“24. https://doi.org/10. 1109/TNNLS.2020.2978386
150. Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. 2020. DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference. In Proceedings of ACL. 2246â€“2251. https://doi.org/10.18653/v1/2020.acl-main.204
151. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. 2020. On Layer Normalization in the Transformer Architecture. In Proceedings of ICML. 10524â€“10533. http://proceedings.mlr.press/v119/xiong20b.html
152. Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. NystrÃ¶mformer: A NystrÃ¶m-based Algorithm for Approximating Self-Attention. (2021).
153. Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. 2019. Understanding and Improving Layer Normalization. In Proceedings of NeurIPS. 4383â€“4393. https://proceedings.neurips.cc/paper/2019/hash/ 2f4fe03d77724a7217006e5d16728874-Abstract.html
154. Hang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu. 2019. TENER: Adapting transformer encoder for named entity recognition. arXiv preprint arXiv:1911.04474 (2019).
155. An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang. 2021. Exploring Sparse Expert Models and Beyond. 40  arXiv:2105.15082 cs.LG.
156. Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. 2018. Modeling Localness for Self-Attention Networks. In Proceedings of EMNLP. Brussels, Belgium, 4449â€“4458. https://doi.org/10.18653/v1/D18- 1475
157. Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, and Zhaopeng Tu. 2020. On the Sub-layer Functionalities of Transformer Decoder. In Findings of EMNLP. Online, 4799â€“4811. https://doi.org/10.18653/v1/2020. findings-emnlp.432
158. Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. BP-Transformer: Modelling Long-Range Context via Binary Partitioning. arXiv:1911.04070 cs.CL.
159. Chengxuan Ying, Guolin Ke, Di He, and Tie-Yan Liu. 2021. LazyFormer: Self Attention with Lazy Update. CoRR abs/2102.12702 (2021). arXiv:2102.12702
160. Davis Yoshida, Allyson Ettinger, and Kevin Gimpel. 2020. Adding Recurrence to Pretrained Transformers for Improved Efficiency and Context Size. CoRR abs/2008.07027 (2020). arXiv:2008.07027
161. Weiqiu You, Simeng Sun, and Mohit Iyyer. 2020. Hard-Coded Gaussian Attention for Neural Machine Translation. In Proceedings of ACL. Online, 7689â€“7700. https://doi.org/10.18653/v1/2020.acl-main.687
162. Weiwei Yu, Jian Zhou, HuaBin Wang, and Liang Tao. 2021. SETransformer: Speech Enhancement Transformer. Cognitive Computation (02 2021). https://doi.org/10.1007/s12559-020-09817-2
163. Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. 2020. Big Bird: Transformers for Longer Sequences. arXiv:2007.14062 cs.LG.
164. Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accelerating Neural Transformer via an Average Attention Network. In Proceedings of ACL. Melbourne, Australia, 1789â€“1798. https://doi.org/10.18653/v1/P18-1166
165. Hang Zhang, Yeyun Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long Document Modeling with Pooling Attention. arXiv:2105.04371
166. Xingxing Zhang, Furu Wei, and Ming Zhou. 2019. HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization. In Proceedings of ACL. Florence, Italy, 5059â€“5069. https://doi.org/10. 18653/v1/P19-1499
167. Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, and Weizhu Chen. 2021. Memory-Efficient Differentiable Transformer Architecture Search. arXiv:2105.14669 cs.LG.
168. Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. 2020. End-to-End Object Detection with Adaptive Clustering Transformer. CoRR abs/2011.09315 (2020). arXiv:2011.09315
169. Yibin Zheng, Xinhui Li, Fenglong Xie, and Li Lu. 2020. Improving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer. In Proceedings of ICASSP. 6734â€“6738. https://doi.org/10.1109/ICASSP40776.2020.9054148
170. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. In Proceedings of AAAI.
171. Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and Furu Wei. 2020. BERT Loses Patience: Fast and Robust Inference with Early Exit. arXiv:2006.04152
172. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020. Deformable DETR: Deformable Transformers for End-to-End Object Detection. CoRR abs/2010.04159 (2020). arXiv:2010.04159
