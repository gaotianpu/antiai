# ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators
ELECTRA：预训练文本编码器作为鉴别器而不是生成器 https://arxiv.org/abs/2003.10555

Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.

掩码语言建模(MLM)预训练方法(如BERT)通过用[MASK]替换一些标记来破坏输入，然后训练模型以重建原始标记。虽然它们在迁移到下游NLP任务时产生了良好的结果，但它们通常需要大量的计算才能有效。作为替代方案，我们提出了一种更具样本效率的预训练任务，称为替换令牌检测。我们的方法不是掩码输入，而是通过用从小型生成器网络中采样的可信替代物替换一些令牌来破坏输入。然后，我们训练一个判别模型来预测损坏的令牌的原始身份，而不是训练一个预测损坏的输入中的每个令牌是否被生成器样本替换的模型。彻底的实验表明，这种新的预训练任务比MLM更有效，因为该任务是在所有输入令牌上定义的，而不仅仅是被掩码的小子集。因此，在相同的模型大小、数据和计算条件下，我们的方法学习的上下文表示大大优于BERT学习的上下文表达。小型车型的收益尤其强劲; 例如，我们在一个GPU上训练一个模型4天，该模型在GLUE自然语言理解基准上优于GPT(使用30倍以上的计算进行训练)。我们的方法在规模上也很好，它的性能与RoBERTa和XLNet相当，但使用的计算量不到它们的1/4，并且在使用相同的计算量时表现优于它们。