# Retentive Network: A Successor to Transformer for Large Language Models 
RetNetï¼šå¤§å‹è¯­è¨€æ¨¡å‹è½¬æ¢å™¨çš„ç»§æ‰¿è€… 2023.7.17 https://arxiv.org/abs/2307.08621

## Abstract
In this work, we propose Retentive Network (RETNET) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†Retentive Network(RETNET)ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„ï¼ŒåŒæ—¶å®ç°äº†è®­ç»ƒå¹¶è¡Œæ€§ã€ä½æˆæœ¬æ¨ç†å’Œè‰¯å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šæ¨å¯¼äº†å¾ªç¯å’Œæ³¨æ„åŠ›ä¹‹é—´çš„è”ç³»ã€‚ç„¶åï¼Œæˆ‘ä»¬æå‡ºäº†åºåˆ—å»ºæ¨¡çš„ä¿ç•™(retention)æœºåˆ¶ï¼Œè¯¥æœºåˆ¶æ”¯æŒä¸‰ç§è®¡ç®—èŒƒå¼ï¼Œå³å¹¶è¡Œã€å¾ªç¯å’Œåˆ†å—å¾ªç¯ã€‚å…·ä½“åœ°è¯´ï¼Œå¹¶è¡Œè¡¨ç¤ºå…è®¸è®­ç»ƒå¹¶è¡Œæ€§ã€‚å¾ªç¯è¡¨ç¤ºå®ç°äº†ä½æˆæœ¬çš„O(1)æ¨ç†ï¼Œåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æé«˜äº†è§£ç ååé‡ã€å»¶è¿Ÿå’ŒGPUå†…å­˜ã€‚åˆ†å—å¾ªç¯è¡¨ç¤ºä¿ƒè¿›äº†å…·æœ‰çº¿æ€§å¤æ‚æ€§çš„é«˜æ•ˆé•¿åºåˆ—å»ºæ¨¡ï¼Œå…¶ä¸­æ¯ä¸ªå—è¢«å¹¶è¡Œç¼–ç ï¼ŒåŒæ—¶å¾ªç¯åœ°æ€»ç»“å—ã€‚è¯­è¨€å»ºæ¨¡å®éªŒç»“æœè¡¨æ˜ï¼ŒRETNETå®ç°äº†è‰¯å¥½çš„ä¼¸ç¼©æ€§ã€å¹¶è¡Œè®­ç»ƒã€ä½æˆæœ¬éƒ¨ç½²å’Œé«˜æ•ˆæ¨ç†ã€‚è¿™äº›æœ‰è¶£çš„ç‰¹æ€§ä½¿RETNETæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­Transformerçš„æœ‰åŠ›ç»§æ‰¿è€…ã€‚ä»£ç å°†åœ¨https://aka.ms/retnet.

![Figure 1](../images/RetNet/fig_1.png)<br/>
Figure 1: Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput, and latency), training parallelism, and favorable scaling curves compared with Transformer. Results of inference cost are reported with 8k as input length. Figure 6 shows more results on different sequence lengths.
å›¾1ï¼šä¸Transformerç›¸æ¯”ï¼ŒRetentiveç½‘ç»œ(RetNet)å®ç°äº†ä½æˆæœ¬çš„æ¨ç†(å³GPUå†…å­˜ã€ååé‡å’Œå»¶è¿Ÿ)ã€è®­ç»ƒå¹¶è¡Œæ€§å’Œè‰¯å¥½çš„ç¼©æ”¾æ›²çº¿ã€‚ä»¥8kä½œä¸ºè¾“å…¥é•¿åº¦æ¥æŠ¥å‘Šæ¨ç†æˆæœ¬çš„ç»“æœã€‚å›¾6æ˜¾ç¤ºäº†ä¸åŒåºåˆ—é•¿åº¦çš„æ›´å¤šç»“æœã€‚

â€œThe only way to discover the limits of the possible is to go beyond them into the impossible. â€  Arthur C. Clarke

â€œå‘ç°å¯èƒ½çš„æé™çš„å”¯ä¸€æ–¹æ³•æ˜¯è¶…è¶Šå®ƒä»¬ï¼Œè¿›å…¥ä¸å¯èƒ½çš„ä¸–ç•Œã€‚â€é˜¿ç‘ŸÂ·CÂ·å…‹æ‹‰å…‹

## 1 Introduction
Transformer [VSP+17] has become the de facto architecture for large language models [BMR+20], which was initially proposed to overcome the sequential training issue of recurrent models [HS97]. However, training parallelism of Transformers is at the cost of inefficient inference, because of the O(N) complexity per step and memory-bound key-value cache [Sha19], which renders Transformers unfriendly to deployment. The growing sequence length increases GPU memory consumption as well as latency and reduces inference speed.

Transformer[VSP+17]å·²æˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹[BMR+20]çš„äº‹å®æ¶æ„ï¼Œæœ€åˆæå‡ºè¯¥æ¶æ„æ˜¯ä¸ºäº†å…‹æœå¾ªç¯æ¨¡å‹[HS97]çš„é¡ºåºè®­ç»ƒé—®é¢˜ã€‚ç„¶è€Œï¼Œç”±äºæ¯ä¸€æ­¥çš„O(N)å¤æ‚æ€§å’Œå†…å­˜ç»‘å®šçš„é”®å€¼ç¼“å­˜[Sha19]ï¼Œè®­ç»ƒTransformersçš„å¹¶è¡Œæ€§æ˜¯ä»¥ä½æ•ˆæ¨ç†ä¸ºä»£ä»·çš„ï¼Œè¿™ä½¿å¾—Transformerså¯¹éƒ¨ç½²ä¸å‹å¥½ã€‚ä¸æ–­å¢é•¿çš„åºåˆ—é•¿åº¦å¢åŠ äº†GPUå†…å­˜æ¶ˆè€—ä»¥åŠå»¶è¿Ÿï¼Œå¹¶é™ä½äº†æ¨ç†é€Ÿåº¦ã€‚

Numerous efforts have continued to develop the next-generation architecture, aiming at retaining training parallelism and competitive performance as Transformers while having efficient O(1) inference. It is challenging to achieve the above goals simultaneously, i.e., the so-called â€œimpossible triangleâ€ as shown in Figure 2.

åœ¨å¼€å‘ä¸‹ä¸€ä»£æ¶æ„æ–¹é¢ï¼Œäººä»¬ç»§ç»­åšå‡ºäº†å¤§é‡åŠªåŠ›ï¼Œæ—¨åœ¨ä¿æŒè®­ç»ƒå¹¶è¡Œæ€§å’Œè½¬æ¢å™¨çš„ç«äº‰æ€§èƒ½ï¼ŒåŒæ—¶å…·æœ‰é«˜æ•ˆçš„O(1)æ¨ç†ã€‚åŒæ—¶å®ç°ä¸Šè¿°ç›®æ ‡æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ï¼Œå³å›¾2æ‰€ç¤ºçš„æ‰€è°“â€œä¸å¯èƒ½ä¸‰è§’â€ã€‚

![Figure 2](../images/RetNet/fig_2.png)<br/>
Figure 2: RetNet makes the â€œimpossible triangleâ€ possible, which achieves training parallelism, good performance, and low inference cost simultaneously.
å›¾2:RetNetä½¿â€œä¸å¯èƒ½çš„ä¸‰è§’â€æˆä¸ºå¯èƒ½ï¼ŒåŒæ—¶å®ç°äº†è®­ç»ƒå¹¶è¡Œæ€§ã€è‰¯å¥½çš„æ€§èƒ½å’Œè¾ƒä½çš„æ¨ç†æˆæœ¬ã€‚

There have been three main strands of research. First, linearized attention [KVPF20] approximates standard attention scores exp(q Â· k) with kernels Ï•(q) Â· Ï•(k), so that autoregressive inference can be rewritten in a recurrent form. However, the modeling capability and performance are worse than Transformers, which hinders the methodâ€™s popularity. The second strand returns to recurrent models for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators [PAA+23] are used for acceleration, however, representation capacity and performance are harmed. The third line of research explores replacing attention with other mechanisms, such as S4 [GGR21], and its variants [DFS+22, PMN+23]. None of the previous work can break through the impossible triangle, resulting in no clear winner compared with Transformers.

ç ”ç©¶ä¸»è¦æœ‰ä¸‰ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œçº¿æ€§åŒ–æ³¨æ„åŠ›[KVPF20]ç”¨æ ¸(q)Â·(k)è¿‘ä¼¼æ ‡å‡†æ³¨æ„åŠ›å¾—åˆ†exp(qÂ·k)ï¼Œä½¿å¾—è‡ªå›å½’æ¨ç†å¯ä»¥ä»¥å¾ªç¯å½¢å¼é‡å†™ã€‚ç„¶è€Œï¼Œå»ºæ¨¡èƒ½åŠ›å’Œæ€§èƒ½æ¯”Transformerså·®ï¼Œè¿™é˜»ç¢äº†è¯¥æ–¹æ³•çš„æµè¡Œã€‚ç¬¬äºŒæ¡å›å½’åˆ°å¾ªç¯æ¨¡å‹ï¼Œä»¥å®ç°é«˜æ•ˆæ¨ç†ï¼ŒåŒæ—¶ç‰ºç‰²è®­ç»ƒå¹¶è¡Œæ€§ã€‚ä½œä¸ºè¡¥æ•‘æªæ–½ï¼Œå…ƒç´ è¿ç®—ç¬¦[PAA+23]è¢«ç”¨äºåŠ é€Ÿï¼Œç„¶è€Œï¼Œè¡¨ç¤ºèƒ½åŠ›å’Œæ€§èƒ½å—åˆ°äº†æŸå®³ã€‚ç¬¬ä¸‰æ¡ç ”ç©¶çº¿æ¢ç´¢ç”¨å…¶ä»–æœºåˆ¶å–ä»£æ³¨æ„åŠ›ï¼Œå¦‚S4[GGR21]åŠå…¶å˜ä½“[DFS+22ï¼ŒPMN+23]ã€‚ä¹‹å‰çš„å·¥ä½œéƒ½æ— æ³•çªç ´ä¸å¯èƒ½ä¸‰è§’ï¼Œå¯¼è‡´ä¸è½¬æ¢å™¨ç›¸æ¯”æ²¡æœ‰æ˜ç¡®çš„èµ¢å®¶ã€‚

In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient longsequence modeling, Transformer-comparable performance, and parallel model training simultaneously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention, which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. First, the parallel representation empowers training parallelism to utilize GPU devices fully. Second, the recurrent representation enables efficient O(1) inference in terms of memory and computation. The deployment cost and latency can be significantly reduced. Moreover, the implementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrent representation can perform efficient long-sequence modeling. We parallelly encode each local block for computation speed while recurrently encoding the global blocks to save GPU memory.

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¿ç•™ç½‘ç»œ(RetNet)ï¼ŒåŒæ—¶å®ç°äº†ä½æˆæœ¬çš„æ¨ç†ã€é«˜æ•ˆçš„é•¿åºåˆ—å»ºæ¨¡ã€Transformerçš„å¯æ¯”æ€§èƒ½å’Œå¹¶è¡Œæ¨¡å‹è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§å¤šå°ºåº¦ä¿æŒæœºåˆ¶æ¥ä»£æ›¿å¤šå¤´æ³¨æ„åŠ›ï¼Œå®ƒæœ‰ä¸‰ç§è®¡ç®—èŒƒå¼ï¼Œå³å¹¶è¡Œã€å¾ªç¯å’Œåˆ†å—å¾ªç¯è¡¨ç¤ºã€‚é¦–å…ˆï¼Œå¹¶è¡Œè¡¨ç¤ºä½¿è®­ç»ƒå¹¶è¡Œæ€§èƒ½å¤Ÿå……åˆ†åˆ©ç”¨GPUè®¾å¤‡ã€‚å…¶æ¬¡ï¼Œå¾ªç¯è¡¨ç¤ºä½¿å¾—èƒ½å¤Ÿåœ¨è®°å¿†å’Œè®¡ç®—æ–¹é¢è¿›è¡Œæœ‰æ•ˆçš„O(1)æ¨ç†ã€‚å¯ä»¥æ˜¾è‘—é™ä½éƒ¨ç½²æˆæœ¬å’Œå»¶è¿Ÿã€‚æ­¤å¤–ï¼Œåœ¨æ²¡æœ‰é”®å€¼ç¼“å­˜æŠ€å·§çš„æƒ…å†µä¸‹ï¼Œå®ç°å¤§å¤§ç®€åŒ–ã€‚ç¬¬ä¸‰ï¼Œåˆ†å—å¾ªç¯è¡¨ç¤ºå¯ä»¥æ‰§è¡Œæœ‰æ•ˆçš„é•¿åºåˆ—å»ºæ¨¡ã€‚ä¸ºäº†è®¡ç®—é€Ÿåº¦ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªå±€éƒ¨å—è¿›è¡Œå¹¶è¡Œç¼–ç ï¼ŒåŒæ—¶å¯¹å…¨å±€å—è¿›è¡Œå¾ªç¯ç¼–ç ä»¥èŠ‚çœGPUå†…å­˜ã€‚
<!-- multi-scale retention å¤šå°ºåº¦ä¿æŒ, å±€éƒ¨å¹¶è¡Œï¼Œå…¨å±€å¾ªç¯ -->

We conduct extensive experiments to compare RetNet with Transformer and its variants. Experimental results on language modeling show that RetNet is consistently competitive in terms of both scaling curves and in-context learning. Moreover, the inference cost of RetNet is length-invariant. For a 7B model and 8k sequence length, RetNet decodes 8.4Ã— faster and saves 70% of memory than Transformers with key-value caches. During training, RetNet also achieves 25-50% memory saving and 7Ã— acceleration than standard Transformer and an advantage towards highly-optimized FlashAttention [DFE+22]. Besides, RetNetâ€™s inference latency is insensitive to batch size, allowing enormous throughput. The intriguing properties make RetNet a strong successor to Transformer for large language models.

æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡çš„å®éªŒæ¥æ¯”è¾ƒRetNetä¸TransformeråŠå…¶å˜ä½“ã€‚è¯­è¨€å»ºæ¨¡çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒRetNetåœ¨ç¼©æ”¾æ›²çº¿å’Œä¸Šä¸‹æ–‡å­¦ä¹ æ–¹é¢éƒ½å…·æœ‰æŒç»­çš„ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼ŒRetNetçš„æ¨ç†æˆæœ¬æ˜¯é•¿åº¦ä¸å˜çš„ã€‚å¯¹äº7Bæ¨¡å‹å’Œ8kåºåˆ—é•¿åº¦ï¼ŒRetNetè§£ç é€Ÿåº¦æ¯”å…·æœ‰é”®å€¼ç¼“å­˜çš„Transformerså¿«8.4å€ï¼ŒèŠ‚çœ70%çš„å†…å­˜ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒRetNetè¿˜å®ç°äº†æ¯”æ ‡å‡†TransformerèŠ‚çœ25-50%çš„å†…å­˜å’Œ7å€çš„åŠ é€Ÿï¼Œå¹¶åœ¨é«˜åº¦ä¼˜åŒ–çš„FlashAttention[DFE+22]æ–¹é¢å…·æœ‰ä¼˜åŠ¿ã€‚æ­¤å¤–ï¼ŒRetNetçš„æ¨ç†å»¶è¿Ÿå¯¹æ‰¹é‡å¤§å°ä¸æ•æ„Ÿï¼Œä»è€Œå…è®¸å·¨å¤§çš„ååé‡ã€‚è¿™äº›æœ‰è¶£çš„ç‰¹æ€§ä½¿RetNetæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­Transformerçš„æœ‰åŠ›ç»§æ‰¿è€…ã€‚

## 2 Retentive Networks ä¿ç•™ç½‘ç»œ
Retentive network (RetNet) is stacked with L identical blocks, which follows a similar layout (i.e., residual connection, and pre-LayerNorm) as in Transformer [VSP+17]. Each RetNet block contains two modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module. We introduce the MSR module in the following sections. Given an input sequence $x = x_1 Â· Â· Â· x_{|x|}$ , RetNet encodes the sequence in an autoregressive way. The input vectors $\{x_i\}^{|x|}_{i=1}$ is first packed into $X_0 = [x_1, Â· Â· Â· , x_{|x|} ] âˆˆ R^{|x|Ã—d_{model}}$  , where $d_{model}$ is hidden dimension. Then we compute contextualized vector representations $X_l = RetNet_l(X^{lâˆ’1})$, l âˆˆ [1, L].

ä¿ç•™ç½‘ç»œ(RetNet)ç”±Lä¸ªç›¸åŒçš„å—å †å è€Œæˆï¼Œå…¶éµå¾ªä¸Transformer[VSP+17]ä¸­ç±»ä¼¼çš„å¸ƒå±€(å³æ®‹å·®è¿æ¥å’Œé¢„å½’ä¸€åŒ–å±‚)ã€‚æ¯ä¸ªRetNetå—åŒ…å«ä¸¤ä¸ªæ¨¡å—ï¼šå¤šå°ºåº¦ä¿ç•™(MSR)æ¨¡å—å’Œå‰é¦ˆç½‘ç»œ(FFN)æ¨¡å—ã€‚æˆ‘ä»¬å°†åœ¨ä»¥ä¸‹éƒ¨åˆ†ä»‹ç»MSRæ¨¡å—ã€‚ç»™å®šè¾“å…¥åºåˆ— $x = x_1 Â· Â· Â· x_{|x|}$ ï¼ŒRetNetä»¥è‡ªå›å½’çš„æ–¹å¼å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚é¦–å…ˆå°†è¾“å…¥å‘é‡ $\{x_i\}^{|x|}_{i=1}$ å‹ç¼©æˆ  $X_0 = [x_1, Â· Â· Â· , x_{|x|} ] âˆˆ R^{|x|Ã—d_{model}}$ ï¼Œå…¶ä¸­ $d_{model}$ æ˜¯éšè—ç»´æ•°ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—ä¸Šä¸‹æ–‡åŒ–çš„å‘é‡è¡¨ç¤º $X_l = RetNet_l(X^{lâˆ’1})$, l âˆˆ [1, L] ã€‚

### 2.1 Retention ä¿ç•™
In this section, we introduce the retention mechanism that has a dual form of recurrence and parallelism. So we can train the models in a parallel way while recurrently conducting inference.

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å…·æœ‰å¾ªç¯å’Œå¹¶è¡ŒåŒé‡å½¢å¼çš„ä¿ç•™æœºåˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å¾ªç¯è¿›è¡Œæ¨ç†çš„åŒæ—¶ï¼Œä»¥å¹¶è¡Œçš„æ–¹å¼è®­ç»ƒæ¨¡å‹ã€‚

Given input $X âˆˆ R^{|x|Ã—d_{model}}$ , we project it to one-dimensional function $v(n) = X_n Â· w_V$ . Consider a sequence modeling problem that maps v(n) â†’ o(n) through states $s_n$. Let $v_n$, $o_n$ denote v(n), o(n) for simplicity. We formulate the mapping in a recurrent manner:

ç»™å®šè¾“å…¥ $XâˆˆR^{|X|Ã—d_{model}}$ ï¼Œæˆ‘ä»¬å°†å…¶æŠ•å½±åˆ°ä¸€ç»´å‡½æ•° $v(n)=X_nÂ·w_v$ã€‚è€ƒè™‘æ˜ å°„v(n)â†’ o(n)çš„åºåˆ—å»ºæ¨¡é—®é¢˜é€šè¿‡çŠ¶æ€$s_n$ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œè®¾$v_n$ï¼Œ$o_n$è¡¨ç¤ºv(n)ï¼Œo(n)ã€‚æˆ‘ä»¬ä»¥ä¸€ç§åå¤å‡ºç°çš„æ–¹å¼åˆ¶å®šæ˜ å°„ï¼š

$s_n = As_{nâˆ’1} + K_n^âŠºv_n, A âˆˆ R^{dÃ—d} , K_n âˆˆ R^{1Ã—d}$ 

$o_n = Q_ns_n = \sum^n_{m=1} A^{nâˆ’m}K^âŠº_mv_m, Q_n âˆˆ R^{1Ã—d}$ (1)

where we map $v_n$ to the state vector $s_n$, and then implement a linear transform to encode sequence information recurrently.

å…¶ä¸­æˆ‘ä»¬å°† $v_n$ æ˜ å°„åˆ°çŠ¶æ€å‘é‡ $s_n$ï¼Œç„¶åå®ç°çº¿æ€§å˜æ¢æ¥å¾ªç¯ç¼–ç åºåˆ—ä¿¡æ¯ã€‚

Next, we make the projection $Q_n$, $K_n$ content-aware:

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä½¿æŠ•å½±$Q_n$, $K_n$å†…å®¹çŸ¥æ™“ï¼š

$Q = XW_Q, K = XW_K$ (2)

where $W_Q, W_K âˆˆ R^{dÃ—d}$ are learnable matrices.

å…¶ä¸­ $W_Q, W_K âˆˆ R^{dÃ—d}$ æ˜¯å¯å­¦ä¹ çŸ©é˜µã€‚

We diagonalize the matrix $A = Î›(Î³e^{iÎ¸})Î›^âˆ’1$ , where $Î³, Î¸ âˆˆ R^d$ . Then we obtain $A^{nâˆ’m} = Î›(Î³e^{iÎ¸})^{nâˆ’m}Î›^{âˆ’1}$ . By absorbing Î› into $W_Q$ and $W_K$, we can rewrite Equation (1) as:

æˆ‘ä»¬å¯¹è§’åŒ–çŸ©é˜µ$A = Î›(Î³e^{iÎ¸})Î›^âˆ’1$ï¼Œå…¶ä¸­Î³ï¼ŒÎ¸âˆˆRDã€‚ç„¶åæˆ‘ä»¬å¾—åˆ°äº† $A^{nâˆ’m} = Î›(Î³e^{iÎ¸})^{nâˆ’m}Î›^{âˆ’1}$ ã€‚é€šè¿‡å°†âˆ§å¸æ”¶åˆ° $W_Q$ å’Œ $W_K$ ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ–¹ç¨‹(1)é‡å†™ä¸ºï¼š

$o_n = \sum^n_{m=1} Q_n(Î³e^{iÎ¸})^{nâˆ’m}K^âŠº_mv_m $

$= \sum^n_{m=1} (Q_n(Î³e^{iÎ¸}) n )(Km(Î³e^{iÎ¸})^{âˆ’m})^âŠºv_m$ (3)

where $Q_n(Î³e^{iÎ¸})^n$, $K_m(Î³e^{iÎ¸})^{âˆ’m}$ is known as xPos [SDP+22], i.e., a relative position embedding proposed for Transformer. We further simplify Î³ as a scalar, Equation (3) becomes:

å…¶ä¸­ï¼Œ$Q_n(Î³e^{iÎ¸})^n$, $K_m(Î³e^{iÎ¸})^{âˆ’m}$ è¢«ç§°ä¸ºxPos[SDP+22]ï¼Œå³ä¸ºè½¬æ¢å™¨æå‡ºçš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥å°†Î³ç®€åŒ–ä¸ºæ ‡é‡ï¼Œæ–¹ç¨‹(3)å˜ä¸ºï¼š

$o_n = \sum^n_{m=1} Î³^{nâˆ’m}(Q_ne^{inÎ¸})(K_me^{imÎ¸})^â€  v_m$ (4)

where â€  is the conjugate transpose. The formulation is easily parallelizable within training instances.

å…¶ä¸­â€ æ˜¯å…±è½­è½¬ç½®ã€‚è¯¥å…¬å¼å¾ˆå®¹æ˜“åœ¨è®­ç»ƒå®ä¾‹ä¸­å¹¶è¡ŒåŒ–ã€‚

In summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel formulation in Equation (4). We consider the original mapping v(n) â†’ o(n) as vectors and obtain the retention mechanism as follows.

æ€»ä¹‹ï¼Œæˆ‘ä»¬ä»æ–¹ç¨‹(1)ä¸­æ‰€ç¤ºçš„å¾ªç¯å»ºæ¨¡å¼€å§‹ï¼Œç„¶ååœ¨æ–¹ç¨‹(4)ä¸­å¯¼å‡ºå…¶å¹¶è¡Œå…¬å¼ã€‚æˆ‘ä»¬è€ƒè™‘åŸå§‹æ˜ å°„v(n) â†’ o(n)ä½œä¸ºè½½ä½“ï¼Œå¹¶è·å¾—å¦‚ä¸‹çš„ä¿ç•™æœºåˆ¶ã€‚

The Parallel Representation of Retention As shown in Figure 3a, the retention layer is defined as:

ä¿ç•™çš„å¹¶è¡Œè¡¨ç¤ºå¦‚å›¾3aæ‰€ç¤ºï¼Œä¿ç•™å±‚å®šä¹‰ä¸ºï¼š

$Q = (XW_Q) âŠ™ Î˜, K = (XW_K) âŠ™ \hat{Î˜}, V = XW_V$ 

$Î˜_n = e^{iÎ¸}, D_{nm} =  Î³^{nâˆ’m}, n â‰¥ m 0, n < m$

$Retention(X) = (QK^âŠº âŠ™ D)V$ (5)

where $\hat{Î˜}$ is the complex conjugate of Î˜, and $D âˆˆ R^{|x|Ã—|x|}$ combines causal masking and exponential decay along relative distance as one matrix. Similar to self-attention, the parallel representation enables us to train the models with GPUs efficiently.

å…¶ä¸­ï¼Œ$\hat{Î˜}$æ˜¯Î¸çš„å¤å…±è½­ï¼Œ$D âˆˆ R^{|x|Ã—|x|}$ å°†å› æœæ©ç å’Œæ²¿ç›¸å¯¹è·ç¦»çš„æŒ‡æ•°è¡°å‡ç»„åˆä¸ºä¸€ä¸ªçŸ©é˜µã€‚ä¸è‡ªæ³¨æ„ç±»ä¼¼ï¼Œå¹¶è¡Œè¡¨ç¤ºä½¿æˆ‘ä»¬èƒ½å¤Ÿæœ‰æ•ˆåœ°ä½¿ç”¨GPUè®­ç»ƒæ¨¡å‹ã€‚

![Figure 3](../images/RetNet/fig_3.png)<br/>
Figure 3: Dual form of RetNet. â€œGNâ€ is short for GroupNorm.
å›¾3ï¼šRetNetçš„åŒé‡å½¢å¼ã€‚â€œGNâ€æ˜¯GroupNormçš„ç¼©å†™ã€‚

The Recurrent Representation of Retention As shown in Figure 3b, the proposed mechanism can also be written as recurrent neural networks (RNNs), which is favorable for inference. For the n-th timestep, we recurrently obtain the output as:

ä¿ç•™çš„å¾ªç¯è¡¨ç¤ºå¦‚å›¾3bæ‰€ç¤ºï¼Œæ‰€æå‡ºçš„æœºåˆ¶ä¹Ÿå¯ä»¥å†™æˆå¾ªç¯ç¥ç»ç½‘ç»œ(RNN)ï¼Œè¿™æœ‰åˆ©äºæ¨ç†ã€‚å¯¹äºç¬¬nä¸ªæ—¶é—´æ­¥é•¿ï¼Œæˆ‘ä»¬å¾ªç¯åœ°è·å¾—å¦‚ä¸‹è¾“å‡ºï¼š

$S_n = Î³S_{nâˆ’1} + K_n^âŠºV_n $ 

$Retention(X_n) = Q_nS_n, n = 1, Â· Â· Â· , |x|$ (6) 

where Q, K, V, Î³ are the same as in Equation (5).

å…¶ä¸­Qã€Kã€Vã€Î³ä¸æ–¹ç¨‹(5)ç›¸åŒã€‚

The Chunkwise Recurrent Representation of Retention A hybrid form of parallel representation and recurrent representation is available to accelerate training, especially for long sequences. We divide the input sequences into chunks. Within each chunk, we follow the parallel representation (Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the recurrent representation (Equation (6)). Specifically, let B denote the chunk length. We compute the retention output of the i-th chunk via:

ä¿ç•™çš„Chunkwiseå¾ªç¯è¡¨ç¤ºå¹¶è¡Œè¡¨ç¤ºå’Œå¾ªç¯è¡¨ç¤ºçš„æ··åˆå½¢å¼å¯ç”¨äºåŠ é€Ÿè®­ç»ƒï¼Œå°¤å…¶æ˜¯å¯¹äºé•¿åºåˆ—ã€‚æˆ‘ä»¬æŠŠè¾“å…¥åºåˆ—åˆ†æˆå—ã€‚åœ¨æ¯ä¸ªå—ä¸­ï¼Œæˆ‘ä»¬éµå¾ªå¹¶è¡Œè¡¨ç¤º(ç­‰å¼(5))æ¥è¿›è¡Œè®¡ç®—ã€‚ç›¸åï¼Œè·¨å—ä¿¡æ¯æ˜¯åœ¨å¾ªç¯è¡¨ç¤ºä¹‹åä¼ é€’çš„(ç­‰å¼(6))ã€‚å…·ä½“åœ°ï¼Œè®¾Bè¡¨ç¤ºå—é•¿åº¦ã€‚æˆ‘ä»¬é€šè¿‡ä»¥ä¸‹æ–¹å¼è®¡ç®—ç¬¬iä¸ªå—çš„ä¿ç•™è¾“å‡ºï¼š

$Q_{[i]} = Q_{B_i:B_{(i+1)}, K_{[i]} = KB_i:B_{(i+1)}, V_{[i]} = VB_i:B_{(i+1)} $

$R_i = K^âŠº_{[i]}V_{[i]} + Î³^BR_{iâˆ’1}$ 

$Retention(X_{[i]}) = (Q[i]K[ âŠº i] âŠ™ D)V[i] | {z } Inner-Chunk + (Q[i]Ri) âŠ™ Î¾ | {z } Cross-Chunk , Î¾ij = Î³ i+1 (7)

where [i] indicates the i-th chunk, i.e., $x[i] = [x_{(iâˆ’1)}B+1, Â· Â· Â· , x_iB]$.

å…¶ä¸­[i]è¡¨ç¤ºç¬¬iä¸ªå—ï¼Œå³$x[i] = [x_{(iâˆ’1)}B+1, Â· Â· Â· , x_iB]$ã€‚

### 2.2 Gated Multi-Scale Retention é—¨æ§å¤šå°ºåº¦ä¿ç•™
We use h = dmodel/d retention heads in each layer, where d is the head dimension. The heads use different parameter matrices WQ, WK, WV âˆˆ R dÃ—d . Moreover, multi-scale retention (MSR) assigns different Î³ for each head. For simplicity, we set Î³ identical among different layers and keep them fixed. In addition, we add a swish gate [RZL17] to increase the non-linearity of retention layers.

æˆ‘ä»¬åœ¨æ¯å±‚ä¸­ä½¿ç”¨h=dmodel/dä¿æŒå¤´ï¼Œå…¶ä¸­dæ˜¯å¤´å°ºå¯¸ã€‚å¤´ä½¿ç”¨ä¸åŒçš„å‚æ•°çŸ©é˜µWQï¼ŒWKï¼ŒWVâˆˆRdÃ—dã€‚æ­¤å¤–ï¼Œå¤šå°ºåº¦ä¿ç•™(MSR)ä¸ºæ¯ä¸ªå¤´éƒ¨åˆ†é…ä¸åŒçš„Î³ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„å±‚ä¹‹é—´è®¾ç½®Î³ç›¸åŒï¼Œå¹¶ä¿æŒå®ƒä»¬ä¸å˜ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªæ‘†åŠ¨é—¨[RZL17]æ¥å¢åŠ ä¿æŒå±‚çš„éçº¿æ€§ã€‚

Formally, given input X, we define the layer as:

å½¢å¼ä¸Šï¼Œç»™å®šè¾“å…¥Xï¼Œæˆ‘ä»¬å°†å±‚å®šä¹‰ä¸ºï¼š

Î³ = 1 âˆ’ 2 âˆ’5âˆ’arange(0,h) âˆˆ R h headi = Retention(X, Î³i) Y = GroupNormh (Concat(head1, Â· Â· Â· , headh)) MSR(X) = (swish(XWG) âŠ™ Y )WO (8)

where WG, WO âˆˆ R dmodelÃ—dmodel are learnable parameters, and GroupNorm [WH18] normalizes the output of each head, following SubLN proposed in [SPP+19]. Notice that the heads use multiple Î³ scales, which results in different variance statistics. So we normalize the head outputs separately.

å…¶ä¸­WGï¼ŒWOâˆˆR dmodelÃ—dmodelæ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼ŒGroupNorm[WH18]æ ¹æ®[SSP+19]ä¸­æå‡ºçš„SubLNå¯¹æ¯ä¸ªå¤´éƒ¨çš„è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ã€‚è¯·æ³¨æ„ï¼Œå¤´éƒ¨ä½¿ç”¨å¤šä¸ªÎ³æ ‡åº¦ï¼Œè¿™ä¼šå¯¼è‡´ä¸åŒçš„æ–¹å·®ç»Ÿè®¡ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹å¤´éƒ¨è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ã€‚

The pseudocode of retention is summarized in Figure 4.

ä¿ç•™çš„ä¼ªä»£ç å¦‚å›¾4æ‰€ç¤ºã€‚

Figure 4: Pseudocode for the three computation paradigms of retention.
å›¾4ï¼šä¿ç•™çš„ä¸‰ç§è®¡ç®—èŒƒå¼çš„ä¼ªä»£ç ã€‚

Retention Score Normalization We utilize the scale-invariant nature of GroupNorm to improve the numerical precision of retention layers. Specifically, multiplying a scalar value within GroupNorm does not affect outputs and backward gradients, i.e.,  GroupNorm(Î± âˆ— headi) = GroupNorm(headi). We implement three normalization factors in Equation (5). First, we normalize QKâŠº as QKâŠº / âˆš d. Second, we replace D with ËœDnm = Dnm/ âˆšP n i=1 Dni. Third, let R denote the retention scores R = QKâŠº âŠ™ D, we normalize it as ËœRnm = Rnm/max(| P n i=1 Rni|,1). Then the retention output becomes Retention(X) = ËœRV . The above tricks do not affect the final results while stabilizing the numerical flow of both forward and backward passes, because of the scale-invariant property.

ä¿ç•™åˆ†æ•°å½’ä¸€åŒ–æˆ‘ä»¬åˆ©ç”¨GroupNormçš„å°ºåº¦ä¸å˜æ€§è´¨æ¥æé«˜ä¿ç•™å±‚çš„æ•°å€¼ç²¾åº¦ã€‚å…·ä½“åœ°è¯´ï¼Œåœ¨GroupNormå†…ä¹˜ä»¥æ ‡é‡å€¼ä¸ä¼šå½±å“è¾“å‡ºå’Œåå‘æ¢¯åº¦ï¼Œå³GroupNorm(Î±*headi)=GroupNorm(headi)ã€‚æˆ‘ä»¬åœ¨ç­‰å¼(5)ä¸­å®ç°äº†ä¸‰ä¸ªå½’ä¸€åŒ–å› å­ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†QKâŠºè§„èŒƒåŒ–ä¸ºQKâŠ¦/âˆšdã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†dæ›¿æ¢ä¸º~Dnm=Dnm/âˆšP n i=1Dniã€‚ç¬¬ä¸‰ï¼Œè®¾Rè¡¨ç¤ºä¿ç•™åˆ†æ•°R=QKâŠºâŠ™Dï¼Œæˆ‘ä»¬å°†å…¶å½’ä¸€åŒ–ä¸º~Rnm=Rnm/max(|PNi=1Rni|ï¼Œ1)ã€‚ç„¶åä¿æŒè¾“å‡ºå˜ä¸ºretention(X)=~RVã€‚ç”±äºå°ºåº¦ä¸å˜çš„ç‰¹æ€§ï¼Œåœ¨ç¨³å®šå‰å‘å’Œåå‘é€šé“çš„æ•°å€¼æµçš„åŒæ—¶ï¼Œä¸Šè¿°æŠ€å·§ä¸ä¼šå½±å“æœ€ç»ˆç»“æœã€‚

### 2.3 Overall Architecture of Retention Networks ä¿ç•™ç½‘ç»œçš„æ€»ä½“æ¶æ„
For an L-layer retention network, we stack multi-scale retention (MSR) and feed-forward network (FFN) to build the model. Formally, the input sequence {xi} |x| i=1 is transformed to vectors by a word embedding layer. We use the packed embeddings X0 = [x1, Â· Â· Â· , x|x| ] âˆˆ R |x|Ã—dmodel as the input and compute the model output XL:

å¯¹äºLå±‚ä¿ç•™ç½‘ç»œï¼Œæˆ‘ä»¬å †å å¤šå°ºåº¦ä¿ç•™(MSR)å’Œå‰é¦ˆç½‘ç»œ(FFN)æ¥æ„å»ºæ¨¡å‹ã€‚å½¢å¼ä¸Šï¼Œè¾“å…¥åºåˆ—{xi}|x|i=1é€šè¿‡å­—åµŒå…¥å±‚è½¬æ¢ä¸ºå‘é‡ã€‚æˆ‘ä»¬ä½¿ç”¨å‹ç¼©åµŒå…¥X0=[x1ï¼ŒÂ·Â·Â·ï¼Œx|x|]âˆˆR|x|Ã—dmodelä½œä¸ºè¾“å…¥ï¼Œå¹¶è®¡ç®—æ¨¡å‹è¾“å‡ºXLï¼š

Y l = MSR(LN(Xl )) + Xl Xl+1 = FFN(LN(Y l )) + Y l (9)

where LN(Â·) is LayerNorm [BKH16]. The FFN part is computed as FFN(X) = gelu(XW1)W2, where W1, W2 are parameter matrices.

å¼ä¸­ï¼ŒLN(Â·)ä¸ºå±‚æ ‡å‡†[BKH16]ã€‚FFNéƒ¨åˆ†è®¡ç®—ä¸ºFFN(X)=gelu(XW1)W2ï¼Œå…¶ä¸­W1ã€W2æ˜¯å‚æ•°çŸ©é˜µã€‚

Training We use the parallel (Equation (5)) and chunkwise recurrent (Equation (7)) representations during the training process. The parallelization within sequences or chunks efficiently utilizes GPUs to accelerate computation. More favorably, chunkwise recurrence is especially useful for long-sequence training, which is efficient in terms of both FLOPs and memory consumption.

è®­ç»ƒæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å¹¶è¡Œ(æ–¹ç¨‹(5))å’Œåˆ†å—å¾ªç¯(æ–¹ç¨‹(7))è¡¨ç¤ºã€‚åºåˆ—æˆ–å—å†…çš„å¹¶è¡ŒåŒ–æœ‰æ•ˆåœ°åˆ©ç”¨GPUæ¥åŠ é€Ÿè®¡ç®—ã€‚æ›´æœ‰åˆ©çš„æ˜¯ï¼Œåˆ†å—å¾ªç¯å¯¹äºé•¿åºåˆ—è®­ç»ƒç‰¹åˆ«æœ‰ç”¨ï¼Œè¿™åœ¨FLOPå’Œå†…å­˜æ¶ˆè€—æ–¹é¢éƒ½æ˜¯æœ‰æ•ˆçš„ã€‚

Table 1: Model comparison from various perspectives. RetNet achieves training parallelization, constant inference cost, linear long-sequence memory complexity, and good performance.
è¡¨1ï¼šä»ä¸åŒè§’åº¦è¿›è¡Œçš„æ¨¡å‹æ¯”è¾ƒã€‚RetNetå®ç°äº†è®­ç»ƒå¹¶è¡ŒåŒ–ã€æ’å®šçš„æ¨ç†æˆæœ¬ã€çº¿æ€§é•¿åºåˆ—å†…å­˜å¤æ‚æ€§å’Œè‰¯å¥½çš„æ€§èƒ½ã€‚

Inference The recurrent representation (Equation (6)) is employed during the inference, which nicely fits autoregressive decoding. The O(1) complexity reduces memory and inference latency while achieving equivalent results.

æ¨ç†åœ¨æ¨ç†è¿‡ç¨‹ä¸­ä½¿ç”¨äº†å¾ªç¯è¡¨ç¤º(æ–¹ç¨‹(6))ï¼Œå®ƒå¾ˆå¥½åœ°é€‚åº”äº†è‡ªå›å½’è§£ç ã€‚O(1)å¤æ‚æ€§é™ä½äº†å†…å­˜å’Œæ¨ç†å»¶è¿Ÿï¼ŒåŒæ—¶å®ç°äº†ç­‰æ•ˆçš„ç»“æœã€‚

### 2.4 Relation to and Differences from Previous Methods ä¸ä»¥å¾€æ–¹æ³•çš„å…³ç³»å’ŒåŒºåˆ«
Table 1 compares RetNet with previous methods from various perspectives. The comparison results echo the â€œimpossible triangleâ€ presented in Figure 2. Moreover, RetNet has linear memory complexity for long sequences due to the chunkwise recurrent representation. We also summarize the comparisons with specific methods as follows.

è¡¨1ä»ä¸åŒçš„è§’åº¦å°†RetNetä¸ä»¥å‰çš„æ–¹æ³•è¿›è¡Œäº†æ¯”è¾ƒã€‚æ¯”è¾ƒç»“æœä¸å›¾2ä¸­æ‰€ç¤ºçš„â€œä¸å¯èƒ½ä¸‰è§’å½¢â€ç›¸å‘¼åº”ã€‚æ­¤å¤–ï¼Œç”±äºåˆ†å—å¾ªç¯è¡¨ç¤ºï¼ŒRetNetå¯¹é•¿åºåˆ—å…·æœ‰çº¿æ€§å†…å­˜å¤æ‚æ€§ã€‚æˆ‘ä»¬è¿˜å°†ä¸å…·ä½“æ–¹æ³•çš„æ¯”è¾ƒæ€»ç»“å¦‚ä¸‹ã€‚

Transformer The parallel representation of retention shares similar spirits as Transformers [VSP+17]. The most related Transformer variant is Lex Transformer [SDP+22] which implements xPos as position embeddings. As described in Equation (3), the derivation of retention aligns with xPos. In comparison with attention, retention removes softmax and enables recurrent formulation, which significantly benefits inference.

è½¬æ¢å™¨ä¿ç•™çš„å¹¶è¡Œè¡¨ç¤ºä¸è½¬æ¢å™¨[VSP+17]å…·æœ‰ç›¸ä¼¼çš„ç²¾ç¥ã€‚æœ€ç›¸å…³çš„Transformerå˜ä½“æ˜¯Lex Transformer[SDP+22]ï¼Œå®ƒå°†xPoså®ç°ä¸ºä½ç½®åµŒå…¥ã€‚å¦‚ç­‰å¼(3)æ‰€è¿°ï¼Œä¿ç•™ç‡çš„æ¨å¯¼ä¸xPosä¸€è‡´ã€‚ä¸æ³¨æ„åŠ›ç›¸æ¯”ï¼Œä¿ç•™æ¶ˆé™¤äº†softmaxï¼Œå¹¶ä½¿é…ç½®èƒ½å¤Ÿé‡å¤ä½¿ç”¨ï¼Œè¿™æ˜¾è‘—æœ‰åˆ©äºæ¨ç†ã€‚

S4 Unlike Equation (2), if Qn and Kn are content-unaware, the formulation can be degenerated to S4 [GGR21], where O = (QKâŠº , QAKâŠº , .., QA|x|âˆ’1KâŠº ) âˆ— V .

S4ä¸æ–¹ç¨‹(2)ä¸åŒï¼Œå¦‚æœQnå’ŒKnæ˜¯å†…å®¹æœªçŸ¥çš„ï¼Œåˆ™å…¬å¼å¯ä»¥é€€åŒ–ä¸ºS4[GGR21]ï¼Œå…¶ä¸­O=(QKâŠºï¼ŒQAKğ•­„ï¼Œ..ï¼ŒQA|x|âˆ’1K \8890;)*Vã€‚

Linear Attention The variants typically use various kernels Ï•(qi)Ï•(kj )/ P |x| n=1 Ï•(qi)Ï•(kn) to replace the softmax function. However, linear attention struggles to effectively encode position information, rendering the models less performant. Besides, we reexamine sequence modeling from scratch, rather than aiming at approximating softmax.

çº¿æ€§æ³¨æ„å˜ä½“é€šå¸¸ä½¿ç”¨ä¸åŒçš„æ ¸Î“(qi)Î“(kj)/P|x|n=1Î“(qi)Î¾(kn)æ¥ä»£æ›¿softmaxå‡½æ•°ã€‚ç„¶è€Œï¼Œçº¿æ€§æ³¨æ„åŠ›éš¾ä»¥æœ‰æ•ˆåœ°å¯¹ä½ç½®ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œè¿™ä½¿å¾—æ¨¡å‹çš„æ€§èƒ½è¾ƒå·®ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹é‡æ–°å®¡è§†åºåˆ—å»ºæ¨¡ï¼Œè€Œä¸æ˜¯ä»¥é€¼è¿‘softmaxä¸ºç›®æ ‡ã€‚

AFT/RWKV Attention Free Transformer (AFT) simplifies dot-product attention to element-wise operations and moves softmax to key vectors. RWKV replaces AFTâ€™s position embeddings with exponential decay and runs the models recurrently for training and inference. In comparison, retention preserves high-dimensional states to encode sequence information, which contributes to expressive ability and better performance. xPos/RoPE Compared with relative position embedding methods proposed for Transformers,Equation (3) presents a similar formulation as xPos [SDP+22] and RoPE [SLP+21].

AFT/RWKVæ— æ³¨æ„å˜æ¢å™¨(AFT)å°†ç‚¹ç§¯æ³¨æ„åŠ›ç®€åŒ–ä¸ºå…ƒç´ æ“ä½œï¼Œå¹¶å°†softmaxç§»åŠ¨åˆ°å…³é”®å‘é‡ã€‚RWKVç”¨æŒ‡æ•°è¡°å‡ä»£æ›¿AFTçš„ä½ç½®åµŒå…¥ï¼Œå¹¶å¾ªç¯è¿è¡Œæ¨¡å‹è¿›è¡Œè®­ç»ƒå’Œæ¨ç†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¿ç•™ä¿ç•™ä¿ç•™äº†é«˜ç»´çŠ¶æ€æ¥ç¼–ç åºåˆ—ä¿¡æ¯ï¼Œè¿™æœ‰åŠ©äºæé«˜è¡¨è¾¾èƒ½åŠ›å’Œæ›´å¥½çš„æ€§èƒ½ã€‚xPos/RoPEä¸ä¸ºè½¬æ¢å™¨æå‡ºçš„ç›¸å¯¹ä½ç½®åµŒå…¥æ–¹æ³•ç›¸æ¯”ï¼Œæ–¹ç¨‹(3)ç»™å‡ºäº†ä¸xPos[SDP+22]å’ŒRoPE[SLP+21]ç±»ä¼¼çš„å…¬å¼ã€‚

Sub-LayerNorm As shown in Equation (8), the retention layer uses Sub-LayerNorm [WMH+22] to normalize outputs. Because the multi-scale modeling leads to different variances for the heads, we replace the original LayerNorm with GroupNorm.

å¦‚ç­‰å¼(8)æ‰€ç¤ºï¼Œä¿ç•™å±‚ä½¿ç”¨Sub LayerNorm[WMH+22]å¯¹è¾“å‡ºè¿›è¡Œå½’ä¸€åŒ–ã€‚ç”±äºå¤šå°ºåº¦å»ºæ¨¡ä¼šå¯¼è‡´å¤´éƒ¨çš„å·®å¼‚ä¸åŒï¼Œå› æ­¤æˆ‘ä»¬å°†åŸå§‹LayerNormæ›¿æ¢ä¸ºGroupNormã€‚

## 3 Experiments
We conduct experiments on language modeling to evaluate RetNet. We evaluate the proposed architecture with various benchmarks, i.e., language modeling performance, and zero-/few-shot learning on downstream tasks. Moreover, for training and inference, we compare speed, memory consumption, and latency.

æˆ‘ä»¬è¿›è¡Œäº†è¯­è¨€å»ºæ¨¡å®éªŒæ¥è¯„ä¼°RetNetã€‚æˆ‘ä»¬ä½¿ç”¨å„ç§åŸºå‡†æ¥è¯„ä¼°æ‰€æå‡ºçš„æ¶æ„ï¼Œå³è¯­è¨€å»ºæ¨¡æ€§èƒ½å’Œä¸‹æ¸¸ä»»åŠ¡çš„é›¶/å°‘æ ·æœ¬å­¦ä¹ ã€‚æ­¤å¤–ï¼Œå¯¹äºè®­ç»ƒå’Œæ¨ç†ï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†é€Ÿåº¦ã€å†…å­˜æ¶ˆè€—å’Œå»¶è¿Ÿã€‚

Table 2: Sizes, and learning hyper-parameters of the models in language modeling experiments.
è¡¨2ï¼šè¯­è¨€å»ºæ¨¡å®éªŒä¸­æ¨¡å‹çš„å¤§å°å’Œå­¦ä¹ è¶…å‚æ•°ã€‚

Figure 5: Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B.
å›¾5ï¼šå›°æƒ‘éšç€æ¨¡å‹å°ºå¯¸çš„å¢å¤§è€Œå‡å°ã€‚æˆ‘ä»¬ä»ç»éªŒä¸Šè§‚å¯Ÿåˆ°ï¼Œå½“æ¨¡å‹å¤§å°å¤§äº2Bæ—¶ï¼ŒRetNetå¾€å¾€ä¼˜äºTransformerã€‚

### 3.1 Setup
Parameter Allocation We re-allocate the parameters in MSR and FFN for fair comparisons. Let d denote dmodel for simplicity here. In Transformers, there are about 4d 2 parameters in self-attention where WQ, WK, WV , WO âˆˆ R dÃ—d , and 8d 2 parameters in FFN where the intermediate dimension is 4d. In comparison, RetNet has 8d 2 parameters in retention, where WQ, WK âˆˆ R dÃ—d , WG, WV âˆˆ R dÃ—2d , WO âˆˆ R 2dÃ—d . Notice that the head dimension of V is twice Q, K. The widened dimension is projected back to d by WO. In order to keep the parameter number the same as Transformer, the FFN intermediate dimension in RetNet is 2d. Meanwhile, we set the head dimension to 256 in our experiments, i.e., 256 for queries and keys, and 512 for values. For fair comparison, we keep Î³ identical among different model sizes, where Î³ = 1 âˆ’ e linspace(log 1/32,log 1/512,h) âˆˆ R h instead of the default value in Equation (8).

å‚æ•°åˆ†é…ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬é‡æ–°åˆ†é…äº†MSRå’ŒFFNä¸­çš„å‚æ•°ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œè®©dè¡¨ç¤ºdmodelã€‚åœ¨Transformersä¸­ï¼Œè‡ªæ³¨æ„çš„å‚æ•°å¤§çº¦æœ‰4d2ä¸ªï¼Œå…¶ä¸­WQï¼ŒWKï¼ŒWVï¼ŒWOâˆˆRdÃ—dï¼Œä»¥åŠFFNä¸­çš„8d2ä¸ªå‚æ•°ï¼Œå…¶ä¸­ä¸­é—´ç»´æ•°ä¸º4dã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRetNetåœ¨ä¿ç•™ä¸­æœ‰8d2ä¸ªå‚æ•°ï¼Œå…¶ä¸­WQï¼ŒWKâˆˆRDÃ—dï¼ŒWGï¼ŒWVâˆˆRDï¼Ÿdï¼ŒWOâˆˆR 2dÃ—dã€‚è¯·æ³¨æ„ï¼ŒVçš„å¤´éƒ¨å°ºå¯¸æ˜¯Qï¼ŒKçš„ä¸¤å€ã€‚WOå°†åŠ å®½çš„å°ºå¯¸æŠ•å½±å›dã€‚ä¸ºäº†ä¿æŒå‚æ•°ç¼–å·ä¸Transformerç›¸åŒï¼ŒRetNetä¸­çš„FFNä¸­é—´ç»´åº¦ä¸º2dã€‚åŒæ—¶ï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼Œæˆ‘ä»¬å°†å¤´éƒ¨ç»´åº¦è®¾ç½®ä¸º256ï¼Œå³æŸ¥è¯¢å’Œå¯†é’¥ä¸º256ï¼Œå€¼ä¸º512ã€‚ä¸ºäº†å…¬å¹³æ¯”è¾ƒï¼Œæˆ‘ä»¬åœ¨ä¸åŒçš„æ¨¡å‹å¤§å°ä¹‹é—´ä¿æŒÎ³ç›¸åŒï¼Œå…¶ä¸­Î³=1âˆ’eæ—ç©ºé—´(log 1/32ï¼Œlog 1/512ï¼Œh)âˆˆR hï¼Œè€Œä¸æ˜¯æ–¹ç¨‹(8)ä¸­çš„é»˜è®¤å€¼ã€‚

Language Model Training As shown in Table 2, we train language models with various sizes (i.e., 1.3B, 2.7B, and 6.7B) from scratch. The training corpus is a curated compilation of The Pile [GBB+20], C4 [DMI+21], and The Stack [KLBA+22]. We append the <bos> token to indicate the start of a sequence2 . The training batch size is 4M tokens with 2048 maximal length. We train the models with 100B tokens, i.e., 25k steps. We use the AdamW [LH19] optimizer with Î²1 = 0.9, Î²2 = 0.98, and weight decay is set to 0.05. The number of warmup steps is 375 with linear learning rate decay. The parameters are initialized following DeepNet [WMD+22] to guarantee training stability. The implementation is based on TorchScale [MWH+22]. We train the models with 512 AMD MI200 GPUs.

è¯­è¨€æ¨¡å‹è®­ç»ƒå¦‚è¡¨2æ‰€ç¤ºï¼Œæˆ‘ä»¬ä»å¤´å¼€å§‹è®­ç»ƒå„ç§å¤§å°(å³1.3Bã€2.7Bå’Œ6.7B)çš„è¯­è¨€æ¨¡å‹ã€‚è®­ç»ƒè¯­æ–™åº“æ˜¯The Pile[GBB+20]ã€C4[DMI+21]å’ŒThe Stack[KLBA+22]çš„ç­–åˆ’æ±‡ç¼–ã€‚æˆ‘ä»¬é™„åŠ <bos>ä»¤ç‰Œæ¥æŒ‡ç¤ºåºåˆ—2çš„å¼€å§‹ã€‚è®­ç»ƒæ‰¹é‡å¤§å°ä¸º4Mä¸ªä»¤ç‰Œï¼Œæœ€å¤§é•¿åº¦ä¸º2048ã€‚æˆ‘ä»¬ç”¨100Bä»¤ç‰Œè®­ç»ƒæ¨¡å‹ï¼Œå³25kæ­¥ã€‚æˆ‘ä»¬ä½¿ç”¨AdamW[LH19]ä¼˜åŒ–å™¨ï¼Œå…¶ä¸­Î²1=0.9ï¼ŒÎ²2=0.98ï¼Œå¹¶ä¸”æƒé‡è¡°å‡è®¾ç½®ä¸º0.05ã€‚åœ¨çº¿æ€§å­¦ä¹ ç‡è¡°å‡çš„æƒ…å†µä¸‹ï¼Œé¢„çƒ­æ­¥éª¤çš„æ•°é‡ä¸º375ã€‚å‚æ•°æŒ‰ç…§DeepNet[MWMD+22]è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥ä¿è¯è®­ç»ƒçš„ç¨³å®šæ€§ã€‚è¯¥å®ç°åŸºäºTorchScale[MWH+22]ã€‚æˆ‘ä»¬ç”¨512ä¸ªAMD MI200 GPUè®­ç»ƒæ¨¡å‹ã€‚

### 3.2 Comparisons with Transformer ä¸è½¬æ¢å™¨çš„æ¯”è¾ƒ
Language Modeling As shown in Figure 5, we report perplexity on the validation set for the language models based on Transformer and RetNet. We present the scaling curves with three model sizes, i.e., 1.3B, 2.7B, and 6.7B. RetNet achieves comparable results with Transformers. More importantly, the results indicate that RetNet is favorable regarding size scaling. Besides performance, the RetNet training is quite stable in our experiments. Experimental results show that RetNet is a strong competitor to Transformer for large language models. Empirically, we find that RetNet starts to outperform Transformer when the model size is larger than 2B. We also summarize the language modeling results with different context lengths in Appendix B. 2We find that appending the <bos> token at the beginning benefits training stability and performance.

è¯­è¨€å»ºæ¨¡å¦‚å›¾5æ‰€ç¤ºï¼Œæˆ‘ä»¬æŠ¥å‘Šäº†åŸºäºTransformerå’ŒRetNetçš„è¯­è¨€æ¨¡å‹çš„éªŒè¯é›†çš„å›°æƒ‘ã€‚æˆ‘ä»¬ç»™å‡ºäº†ä¸‰ç§æ¨¡å‹å°ºå¯¸çš„ç¼©æ”¾æ›²çº¿ï¼Œå³1.3Bã€2.7Bå’Œ6.7Bã€‚RetNetä¸Transformersçš„ç»“æœç›¸å½“ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç»“æœè¡¨æ˜RetNetåœ¨è§„æ¨¡ç¼©æ”¾æ–¹é¢æ˜¯æœ‰åˆ©çš„ã€‚é™¤äº†æ€§èƒ½ï¼ŒRetNetè®­ç»ƒåœ¨æˆ‘ä»¬çš„å®éªŒä¸­æ˜¯ç›¸å½“ç¨³å®šçš„ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹æ–¹é¢ï¼ŒRetNetæ˜¯Transformerçš„æœ‰åŠ›ç«äº‰å¯¹æ‰‹ã€‚æ ¹æ®ç»éªŒï¼Œæˆ‘ä»¬å‘ç°å½“æ¨¡å‹å¤§å°å¤§äº2Bæ—¶ï¼ŒRetNetå¼€å§‹ä¼˜äºTransformerã€‚æˆ‘ä»¬è¿˜åœ¨é™„å½•Bä¸­æ€»ç»“äº†ä¸åŒä¸Šä¸‹æ–‡é•¿åº¦çš„è¯­è¨€å»ºæ¨¡ç»“æœã€‚2æˆ‘ä»¬å‘ç°ï¼Œåœ¨å¼€å¤´æ·»åŠ <bos>ä»¤ç‰Œæœ‰åˆ©äºè®­ç»ƒçš„ç¨³å®šæ€§å’Œæ€§èƒ½ã€‚

Table 3: Zero-shot and few-shot learning with Transformer and RetNet. The model size is 6.7B.
è¡¨3ï¼šTransformerå’ŒRetNetçš„é›¶æ ·æœ¬å’Œå°‘å¿«ç…§å­¦ä¹ ã€‚æ¨¡å‹å°ºå¯¸ä¸º6.7Bã€‚

Table 4: Training cost of Transformer (Trm), Transformer with FlashAttention (Trm+FlashAttn), and RetNet. We report memory consumption and training throughput (word per second; wps).
è¡¨4ï¼šè½¬æ¢å™¨(Trm)ã€å¸¦FlashAttentionçš„è½¬æ¢å™¨(Trm+FlashAttn)å’ŒRetNetçš„è®­ç»ƒæˆæœ¬ã€‚æˆ‘ä»¬æŠ¥å‘Šäº†å†…å­˜æ¶ˆè€—å’Œè®­ç»ƒååé‡(æ¯ç§’å­—;wps)ã€‚

Zero-Shot and Few-Shot Evaluation on Downstream Tasks We also compare the language models on a wide range of downstream tasks. We evaluate zero-shot and 4-shot learning with the 6.7B models. As shown in Table 3, the datasets include HellaSwag (HS) [ZHB+19], BoolQ [CLC+19], COPA [WPN+19], PIQA [BZB+20], Winograd, Winogrande [LDM12], and StoryCloze (SC) [MRL+17]. The accuracy numbers are consistent with language modeling perplexity presented in Figure 5. RetNet achieves comparable performance with Transformer on zero-shot and in-context learning settings.

ä¸‹æ¸¸ä»»åŠ¡çš„é›¶æ ·æœ¬å’Œå°‘å¿«ç…§è¯„ä¼°æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†å„ç§ä¸‹æ¸¸ä»»åŠ¡çš„è¯­è¨€æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨6.7Bæ¨¡å‹è¯„ä¼°é›¶æ ·æœ¬å’Œå››å°„å­¦ä¹ ã€‚å¦‚è¡¨3æ‰€ç¤ºï¼Œæ•°æ®é›†åŒ…æ‹¬HellaSwag(HS)[ZHB+19]ã€BoolQ[CLC+19]ã€COPA[WPN+19]ã€PIQA[BZB+20]ã€Winogradã€Winogrande[LDM12]å’ŒStoryCloze(SC)[MRL+17]ã€‚å‡†ç¡®åº¦æ•°å­—ä¸å›¾5æ‰€ç¤ºçš„è¯­è¨€å»ºæ¨¡å›°æƒ‘ä¸€è‡´ã€‚RetNetåœ¨é›¶æ ·æœ¬å’Œä¸Šä¸‹æ–‡å­¦ä¹ è®¾ç½®ä¸Šçš„æ€§èƒ½ä¸Transformerç›¸å½“ã€‚

### 3.3 Training Cost è®­ç»ƒè´¹ç”¨
As shown in Table 4, we compare the training speed and memory consumption of Transformer and RetNet, where the training sequence length is 8192. We also compare with FlashAttention [DFE+22], which improves speed and reduces GPU memory IO by recomputation and kernel fusion. In comparison, we implement RetNet using vanilla PyTorch code, and leave kernel fusion or FlashAttention-like acceleration for future work. We use chunkwise recurrent representation of retention as described in Equation (7). The chunk size is set to 512. We evaluate the results with eight Nvidia A100-80GB GPUs, because FlashAttention is highly optimized for A100. Tensor parallelism is enabled for 6.7B and 13B models.

å¦‚è¡¨4æ‰€ç¤ºï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†Transformerå’ŒRetNetçš„è®­ç»ƒé€Ÿåº¦å’Œå†…å­˜æ¶ˆè€—ï¼Œå…¶ä¸­è®­ç»ƒåºåˆ—é•¿åº¦ä¸º8192ã€‚æˆ‘ä»¬è¿˜ä¸FlashAttention[DFE+22]è¿›è¡Œäº†æ¯”è¾ƒï¼Œåè€…é€šè¿‡é‡æ–°è®¡ç®—å’Œå†…æ ¸èåˆæé«˜äº†é€Ÿåº¦å¹¶å‡å°‘äº†GPUå†…å­˜IOã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨æ™®é€šPyTorchä»£ç å®ç°RetNetï¼Œå¹¶å°†å†…æ ¸èåˆæˆ–ç±»ä¼¼FlashAttentionçš„åŠ é€Ÿç•™ç»™æœªæ¥çš„å·¥ä½œã€‚æˆ‘ä»¬ä½¿ç”¨ä¿ç•™çš„åˆ†å—å¾ªç¯è¡¨ç¤ºï¼Œå¦‚æ–¹ç¨‹(7)æ‰€è¿°ã€‚å—å¤§å°è¢«è®¾ç½®ä¸º512ã€‚æˆ‘ä»¬ä½¿ç”¨å…«ä¸ªNvidia A100-80GB GPUè¯„ä¼°ç»“æœï¼Œå› ä¸ºFlashAttentioné’ˆå¯¹A100è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–ã€‚6.7Bå’Œ13Bæ¨¡å‹å¯ç”¨å¼ é‡å¹¶è¡Œã€‚

Experimental results show that RetNet is more memory-efficient and has higher throughput than Transformers during training. Even compared with FlashAttention, RetNet is still competitive in terms of speed and memory cost. Moreover, without relying on specific kernels, it is easy to train RetNet on other platforms efficiently. For example, we train the RetNet models on an AMD MI200 cluster with decent throughput. It is notable that RetNet has the potential to further reduce cost via advanced implementation, such as kernel fusion.

å®éªŒç»“æœè¡¨æ˜ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒRetNetæ¯”Transformerså…·æœ‰æ›´é«˜çš„å†…å­˜æ•ˆç‡å’Œååé‡ã€‚å³ä½¿ä¸FlashAttentionç›¸æ¯”ï¼ŒRetNetåœ¨é€Ÿåº¦å’Œå†…å­˜æˆæœ¬æ–¹é¢ä»ç„¶å…·æœ‰ç«äº‰åŠ›ã€‚æ­¤å¤–ï¼Œåœ¨ä¸ä¾èµ–ç‰¹å®šå†…æ ¸çš„æƒ…å†µä¸‹ï¼Œå¾ˆå®¹æ˜“åœ¨å…¶ä»–å¹³å°ä¸Šé«˜æ•ˆåœ°è®­ç»ƒRetNetã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬åœ¨å…·æœ‰ä¸é”™ååé‡çš„AMD MI200é›†ç¾¤ä¸Šè®­ç»ƒRetNetæ¨¡å‹ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒRetNetæœ‰å¯èƒ½é€šè¿‡é«˜çº§å®ç°(å¦‚å†…æ ¸èåˆ)è¿›ä¸€æ­¥é™ä½æˆæœ¬ã€‚

### 3.4 Inference Cost æ¨ç†æˆæœ¬
As shown in Figure 6, we compare memory cost, throughput, and latency of Transformer and RetNet during inference. Transformers reuse KV caches of previously decoded tokens. RetNet uses the recurrent representation as described in Equation (6). We evaluate the 6.7B model on the A100-80GB GPU in our experiments. Figure 6 shows that RetNet outperforms Transformer in terms of inference cost.

å¦‚å›¾6æ‰€ç¤ºï¼Œæˆ‘ä»¬æ¯”è¾ƒäº†Transformerå’ŒRetNetåœ¨æ¨ç†è¿‡ç¨‹ä¸­çš„å†…å­˜æˆæœ¬ã€ååé‡å’Œå»¶è¿Ÿã€‚è½¬æ¢å™¨é‡ç”¨å…ˆå‰è§£ç çš„ä»¤ç‰Œçš„KVç¼“å­˜ã€‚RetNetä½¿ç”¨å…¬å¼(6)ä¸­æè¿°çš„å¾ªç¯è¡¨ç¤ºã€‚æˆ‘ä»¬åœ¨å®éªŒä¸­è¯„ä¼°äº†A100-80GB GPUä¸Šçš„6.7Bæ¨¡å‹ã€‚å›¾6æ˜¾ç¤ºï¼ŒRetNetåœ¨æ¨ç†æˆæœ¬æ–¹é¢ä¼˜äºTransformerã€‚

Memory As shown in Figure 6a, the memory cost of Transformer increases linearly due to KV caches. In contrast, the memory consumption of RetNet remains consistent even for long sequences, requiring much less GPU memory to host RetNet. The additional memory consumption of RetNet is almost negligible (i.e., about 3%) while the model weights occupy 97%.

å†…å­˜å¦‚å›¾6aæ‰€ç¤ºï¼Œç”±äºKVç¼“å­˜ï¼ŒTransformerçš„å†…å­˜æˆæœ¬çº¿æ€§å¢åŠ ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå³ä½¿å¯¹äºé•¿åºåˆ—ï¼ŒRetNetçš„å†…å­˜æ¶ˆè€—ä¹Ÿä¿æŒä¸€è‡´ï¼Œå› æ­¤æ‰˜ç®¡RetNetæ‰€éœ€çš„GPUå†…å­˜è¦å°‘å¾—å¤šã€‚RetNetçš„é¢å¤–å†…å­˜æ¶ˆè€—å‡ ä¹å¯ä»¥å¿½ç•¥ä¸è®¡(å³çº¦3%)ï¼Œè€Œæ¨¡å‹æƒé‡å 97%ã€‚

Figure 6: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms Transformers in terms of memory consumption, throughput, and latency.
å›¾6ï¼šæ¨¡å‹å¤§å°ä¸º6.7Bçš„Transformerå’ŒRetNetçš„æ¨ç†æˆæœ¬ã€‚RetNetåœ¨å†…å­˜æ¶ˆè€—ã€ååé‡å’Œå»¶è¿Ÿæ–¹é¢ä¼˜äºTransformersã€‚

Throughput As presented in Figure 6b, the throughput of Transformer drops along with the decoding length increases. In comparison, RetNet has higher and length-invariant throughput during decoding, by utilizing the recurrent representation of retention.

ååé‡å¦‚å›¾6bæ‰€ç¤ºï¼ŒTransformerçš„ååé‡éšç€è§£ç é•¿åº¦çš„å¢åŠ è€Œä¸‹é™ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRetNeté€šè¿‡åˆ©ç”¨ä¿ç•™çš„å¾ªç¯è¡¨ç¤ºï¼Œåœ¨è§£ç è¿‡ç¨‹ä¸­å…·æœ‰æ›´é«˜çš„é•¿åº¦ä¸å˜ååé‡ã€‚

Latency Latency is an important metric in deployment, which greatly affects user experience. We report decoding latency in Figure 6c. Experimental results show that increasing batch size renders Transformerâ€™s latency larger. Moreover, the latency of Transformers grows faster with longer input. In order to make latency acceptable, we have to restrict the batch size, which harms the overall inference throughput of Transformers. By contrast, RetNetâ€™s decoding latency outperforms Transformers and keeps almost the same across different batch sizes and input lengths.

å»¶è¿Ÿå»¶è¿Ÿæ˜¯éƒ¨ç½²ä¸­çš„ä¸€ä¸ªé‡è¦æŒ‡æ ‡ï¼Œå®ƒä¼šæå¤§åœ°å½±å“ç”¨æˆ·ä½“éªŒã€‚æˆ‘ä»¬åœ¨å›¾6cä¸­æŠ¥å‘Šäº†è§£ç å»¶è¿Ÿã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œæ‰¹é‡å¤§å°çš„å¢åŠ ä¼šä½¿Transformerçš„å»¶è¿Ÿå˜å¤§ã€‚æ­¤å¤–ï¼ŒTransformersçš„å»¶è¿Ÿéšç€è¾“å…¥æ—¶é—´çš„å»¶é•¿è€Œå¢é•¿å¾—æ›´å¿«ã€‚ä¸ºäº†ä½¿å»¶è¿Ÿå¯ä»¥æ¥å—ï¼Œæˆ‘ä»¬å¿…é¡»é™åˆ¶æ‰¹é‡å¤§å°ï¼Œè¿™ä¼šæŸå®³Transformersçš„æ•´ä½“æ¨ç†ååé‡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒRetNetçš„è§£ç å»¶è¿Ÿä¼˜äºTransformersï¼Œå¹¶ä¸”åœ¨ä¸åŒçš„æ‰¹é‡å¤§å°å’Œè¾“å…¥é•¿åº¦ä¸‹ä¿æŒå‡ ä¹ç›¸åŒã€‚

### 3.5 Comparison with Transformer Variants ä¸è½¬æ¢å™¨å˜ä½“çš„æ¯”è¾ƒ
Apart from Transformer, we compare RetNet with various efficient Transformer variants, including Linear Transformer [KVPF20], RWKV [PAA+23], H3 [DFS+22], and Hyena [PMN+23]. All models have 200M parameters with 16 layers and a hidden dimension of 1024. For H3, we set the head dimension as 8. For RWKV, we use the TimeMix module to substitute self-attention layers while keeping FFN layers consistent with other models for fair comparisons. We train the models with 10k steps with a batch size of 0.5M tokens. Most hyperparameters and training corpora are kept the same as in Section 3.1.

é™¤äº†è½¬æ¢å™¨ï¼Œæˆ‘ä»¬è¿˜å°†RetNetä¸å„ç§é«˜æ•ˆè½¬æ¢å™¨å˜ä½“è¿›è¡Œäº†æ¯”è¾ƒï¼ŒåŒ…æ‹¬çº¿æ€§è½¬æ¢å™¨[KVPF20]ã€RWKV[PAA+23]ã€H3[DFS+22]å’ŒHyena[PMN+23]ã€‚æ‰€æœ‰æ¨¡å‹éƒ½æœ‰200Mä¸ªå‚æ•°ï¼Œæœ‰16å±‚ï¼Œéšè—å°ºå¯¸ä¸º1024ã€‚å¯¹äºH3ï¼Œæˆ‘ä»¬å°†å¤´éƒ¨å°ºå¯¸è®¾ç½®ä¸º8ã€‚å¯¹äºRWKVï¼Œæˆ‘ä»¬ä½¿ç”¨TimeMixæ¨¡å—æ¥æ›¿ä»£è‡ªæˆ‘å…³æ³¨å±‚ï¼ŒåŒæ—¶ä¿æŒFFNå±‚ä¸å…¶ä»–æ¨¡å‹çš„ä¸€è‡´æ€§ï¼Œä»¥è¿›è¡Œå…¬å¹³çš„æ¯”è¾ƒã€‚æˆ‘ä»¬ç”¨10kä¸ªæ­¥éª¤è®­ç»ƒæ¨¡å‹ï¼Œæ‰¹é‡å¤§å°ä¸º0.5Mä¸ªä»¤ç‰Œã€‚å¤§å¤šæ•°è¶…å‚æ•°å’Œè®­ç»ƒè¯­æ–™åº“ä¸ç¬¬3.1èŠ‚ä¸­çš„ä¿æŒç›¸åŒã€‚

Table 5 reports the perplexity numbers on the in-domain validation set and other out-of-domain corpora, e.g., Project Gutenberg 2019-2022 (PG22) [SDP+22], QMSum [ZYY+21], GovReport [HCP+21], SummScreen [CCWG21, SSI+22]. Overall, RetNet outperforms previous methods across different datasets. RetNet not only achieves better evaluation results on the in-domain corpus but also obtains lower perplexity on several out-of-domain datasets. The favorable performance makes RetNet a strong successor to Transformer, besides the benefits of significant cost reduction (Sections 3.3 and 3.4).

è¡¨5æŠ¥å‘Šäº†åŸŸå†…éªŒè¯é›†å’Œå…¶ä»–åŸŸå¤–è¯­æ–™åº“çš„å›°æƒ‘æ•°å­—ï¼Œä¾‹å¦‚ï¼ŒProject Gutenberg 2019-2022(PG22)[SDP+22]ã€QMSum[ZYY+21]ã€GovReport[HCP+21]ã€SummScreen[CCWG21ã€SSI+22]ã€‚æ€»çš„æ¥è¯´ï¼ŒRetNetåœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šéƒ½ä¼˜äºä»¥å‰çš„æ–¹æ³•ã€‚RetNetä¸ä»…åœ¨åŸŸå†…è¯­æ–™åº“ä¸Šè·å¾—äº†æ›´å¥½çš„è¯„ä¼°ç»“æœï¼Œè€Œä¸”åœ¨å‡ ä¸ªåŸŸå¤–æ•°æ®é›†ä¸Šè·å¾—äº†æ›´ä½çš„å›°æƒ‘ã€‚é™¤äº†æ˜¾è‘—é™ä½æˆæœ¬çš„å¥½å¤„(ç¬¬3.3èŠ‚å’Œç¬¬3.4èŠ‚)å¤–ï¼Œè‰¯å¥½çš„æ€§èƒ½ä½¿RetNetæˆä¸ºTransformerçš„æœ‰åŠ›ç»§ä»»è€…ã€‚

Table 5: Perplexity results on language modeling. RetNet outperforms other architectures on both the in-domain evaluation set and various out-of-domain corpora.
è¡¨5ï¼šè¯­è¨€å»ºæ¨¡çš„å›°æƒ‘ç»“æœã€‚RetNetåœ¨åŸŸå†…è¯„ä¼°é›†å’Œå„ç§åŸŸå¤–è¯­æ–™åº“ä¸Šéƒ½ä¼˜äºå…¶ä»–æ¶æ„ã€‚

Table 6: Ablation results on in-domain and out-of-domain corpora.
è¡¨6ï¼šåŸŸå†…å’ŒåŸŸå¤–è¯­æ–™åº“çš„æ¶ˆèç»“æœã€‚

In addition, we discuss the training and inference efficiency of the compared methods. Let d denote the hidden dimension, and n the sequence length. For training, RWKVâ€™s token-mixing complexity is O(dn) while Hyenaâ€™s is O(dn log n) with Fast Fourier Transform acceleration. The above two methods reduce training FLOPS via employing element-wise operators to trade-off modeling capacity. In comparison with retention, the chunk-wise recurrent representation is O(dn(b + h)), where b is the chunk size, h is the head dimension, and we usually set b = 512, h = 256. For either large model size (i.e., larger d) or sequence length, the additional b + h has negligible effects. So the RetNet training is quite efficient without sacrificing the modeling performance. For inference, among the compared efficient architectures, Hyena has the same complexity (i.e., O(n) per step) as Transformer while the others can perform O(1) decoding.

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†æ¯”è¾ƒæ–¹æ³•çš„è®­ç»ƒå’Œæ¨ç†æ•ˆç‡ã€‚è®¾dè¡¨ç¤ºéšè—ç»´åº¦ï¼Œnè¡¨ç¤ºåºåˆ—é•¿åº¦ã€‚å¯¹äºè®­ç»ƒï¼ŒRWKVçš„ä»¤ç‰Œæ··åˆå¤æ‚æ€§ä¸ºO(dn)ï¼Œè€ŒHyenaçš„ä»¤ç‰Œæ··åˆå¤æ‚åº¦ä¸ºO(dn-logn)ï¼Œå…·æœ‰å¿«é€Ÿå‚…ç«‹å¶å˜æ¢åŠ é€Ÿã€‚ä¸Šè¿°ä¸¤ç§æ–¹æ³•é€šè¿‡ä½¿ç”¨å…ƒç´ è¿ç®—ç¬¦æ¥æƒè¡¡å»ºæ¨¡èƒ½åŠ›æ¥å‡å°‘è®­ç»ƒFLOPSã€‚ä¸ä¿ç•™ç›¸æ¯”ï¼ŒæŒ‰å—çš„å¾ªç¯è¡¨ç¤ºæ˜¯O(dn(b+h))ï¼Œå…¶ä¸­bæ˜¯å—å¤§å°ï¼Œhæ˜¯å¤´éƒ¨å°ºå¯¸ï¼Œå¹¶ä¸”æˆ‘ä»¬é€šå¸¸è®¾ç½®b=512ï¼Œh=256ã€‚å¯¹äºå¤§çš„æ¨¡å‹å¤§å°(å³ï¼Œè¾ƒå¤§çš„d)æˆ–åºåˆ—é•¿åº¦ï¼Œé¢å¤–çš„b+hå…·æœ‰å¯å¿½ç•¥çš„å½±å“ã€‚å› æ­¤ï¼Œåœ¨ä¸ç‰ºç‰²å»ºæ¨¡æ€§èƒ½çš„æƒ…å†µä¸‹ï¼ŒRetNetè®­ç»ƒæ˜¯éå¸¸æœ‰æ•ˆçš„ã€‚ä¸ºäº†æ¨ç†ï¼Œåœ¨æ¯”è¾ƒçš„é«˜æ•ˆæ¶æ„ä¸­ï¼ŒHyenaå…·æœ‰ä¸Transformerç›¸åŒçš„å¤æ‚æ€§(å³ï¼Œæ¯æ­¥O(n))ï¼Œè€Œå…¶ä»–æ¶æ„å¯ä»¥æ‰§è¡ŒO(1)è§£ç ã€‚

### 3.6 Ablation Studies æ¶ˆèç ”ç©¶
We ablate various design choices of RetNet and report the language modeling results in Table 6. The evaluation settings and metrics are the same as in Section 3.5.

æˆ‘ä»¬åˆ—ä¸¾äº†RetNetçš„å„ç§è®¾è®¡é€‰æ‹©ï¼Œå¹¶åœ¨è¡¨6ä¸­æŠ¥å‘Šäº†è¯­è¨€å»ºæ¨¡ç»“æœã€‚è¯„ä¼°è®¾ç½®å’ŒæŒ‡æ ‡ä¸ç¬¬3.5èŠ‚ä¸­çš„ç›¸åŒã€‚

Architecture We ablate the swish gate and GroupNorm as described in Equation (8). Table 6 shows that the above two components improve the final performance. Firstly, the gating module is essential for enhancing non-linearity and improving model capability. Notice that we use the same parameter allocation as Transformers after removing the gate. Secondly, group normalization in retention balances the variances of multi-head outputs, which improves training stability and language modeling results.

æ¶æ„æˆ‘ä»¬æŒ‰ç…§ç­‰å¼(8)ä¸­çš„æè¿°ï¼Œæ¶ˆèswish gateå’ŒGroupNormã€‚è¡¨6æ˜¾ç¤ºï¼Œä¸Šè¿°ä¸¤ä¸ªç»„ä»¶æé«˜äº†æœ€ç»ˆæ€§èƒ½ã€‚é¦–å…ˆï¼Œé—¨æ§æ¨¡å—å¯¹äºå¢å¹¿éçº¿æ€§å’Œæé«˜æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚è¯·æ³¨æ„ï¼Œåœ¨ç§»é™¤é—¨ä¹‹åï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸Transformersç›¸åŒçš„å‚æ•°åˆ†é…ã€‚å…¶æ¬¡ï¼Œä¿ç•™ä¸­çš„ç»„å½’ä¸€åŒ–å¹³è¡¡äº†å¤šå¤´è¾“å‡ºçš„æ–¹å·®ï¼Œæé«˜äº†è®­ç»ƒç¨³å®šæ€§å’Œè¯­è¨€å»ºæ¨¡ç»“æœã€‚

Multi-Scale Decay Equation (8) shows that we use different Î³ as the decay rates for the retention heads. In the ablation studies, we examine removing Î³ decay (i.e., â€œâˆ’ Î³ decayâ€) and applying the same decay rate across heads (i.e., â€œâˆ’ multi-scale decayâ€). Specifically, ablating Î³ decay is equivalent to Î³ = 1. In the second setting, we set Î³ = 127/128 for all heads. Table 6 indicates that both the decay mechanism and using multiple decay rates can improve the language modeling performance.

å¤šå°ºåº¦è¡°å˜æ–¹ç¨‹(8)è¡¨æ˜ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„Î³ä½œä¸ºä¿ç•™å¤´çš„è¡°å˜ç‡ã€‚åœ¨æ¶ˆèç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬æ£€æŸ¥äº†å»é™¤Î³è¡°å˜(å³â€œâˆ’Î³è¡°å˜â€)å’Œåœ¨å¤´éƒ¨åº”ç”¨ç›¸åŒçš„è¡°å˜ç‡(å³â€œâ€“å¤šå°ºåº¦è¡°å˜â€)ã€‚å…·ä½“æ¥è¯´ï¼Œæ¶ˆèÎ³è¡°å˜ç›¸å½“äºÎ³=1ã€‚åœ¨ç¬¬äºŒä¸ªè®¾ç½®ä¸­ï¼Œæˆ‘ä»¬ä¸ºæ‰€æœ‰ç£å¤´è®¾ç½®Î³=127/128ã€‚è¡¨6è¡¨æ˜ï¼Œè¡°å‡æœºåˆ¶å’Œä½¿ç”¨å¤šé‡è¡°å‡ç‡éƒ½å¯ä»¥æé«˜è¯­è¨€å»ºæ¨¡æ€§èƒ½ã€‚

Head Dimension From the recurrent perspective of Equation (1), the head dimension implies the memory capacity of hidden states. In the ablation study, we reduce the default head dimension from 256 to 64, i.e., 64 for queries and keys, and 128 for values. We keep the hidden dimension dmodel the same so the number of heads increases. Experimental results in Table 6 show that the larger head dimension achieves better performance.

å¤´éƒ¨ç»´åº¦ä»æ–¹ç¨‹(1)çš„å¾ªç¯è§’åº¦æ¥çœ‹ï¼Œå¤´éƒ¨ç»´åº¦æ„å‘³ç€éšè—çŠ¶æ€çš„è®°å¿†èƒ½åŠ›ã€‚åœ¨æ¶ˆèç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å°†é»˜è®¤å¤´éƒ¨å°ºå¯¸ä»256å‡å°åˆ°64ï¼Œå³æŸ¥è¯¢å’Œé”®ä¸º64ï¼Œå€¼ä¸º128ã€‚æˆ‘ä»¬ä¿æŒéšè—ç»´åº¦dmodelç›¸åŒï¼Œå› æ­¤å¤´çš„æ•°é‡ä¼šå¢åŠ ã€‚è¡¨6ä¸­çš„å®éªŒç»“æœè¡¨æ˜ï¼Œå¤´éƒ¨å°ºå¯¸è¶Šå¤§ï¼Œæ€§èƒ½è¶Šå¥½ã€‚

## 4 Conclusion
In this work, we propose retentive networks (RetNet) for sequence modeling, which enables various representations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers. The above advantages make RetNet an ideal successor to Transformers for large language models, especially considering the deployment benefits brought by the O(1) inference complexity. In the future, we would like to scale up RetNet in terms of model size [CDH+22] and training steps. Moreover, retention can efficiently work with structured prompting [HSD+22b] by compressing long-term memory. We will also use RetNet as the backbone architecture to train multimodal large language models [HSD+22a, HDW+23, PWD+23].

åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºåºåˆ—å»ºæ¨¡çš„ä¿ç•™ç½‘ç»œ(RetNet)ï¼Œå®ƒå®ç°äº†å„ç§è¡¨ç¤ºï¼Œå³å¹¶è¡Œã€å¾ªç¯å’Œåˆ†å—å¾ªç¯ã€‚ä¸Transformersç›¸æ¯”ï¼ŒRetNetå®ç°äº†æ˜¾è‘—æ›´å¥½çš„æ¨ç†æ•ˆç‡(åœ¨å†…å­˜ã€é€Ÿåº¦å’Œå»¶è¿Ÿæ–¹é¢)ã€è‰¯å¥½çš„è®­ç»ƒå¹¶è¡Œæ€§å’Œæœ‰ç«äº‰åŠ›çš„æ€§èƒ½ã€‚ä¸Šè¿°ä¼˜åŠ¿ä½¿RetNetæˆä¸ºå¤§å‹è¯­è¨€æ¨¡å‹Transformersçš„ç†æƒ³ç»§ä»»è€…ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ°O(1)æ¨ç†å¤æ‚æ€§å¸¦æ¥çš„éƒ¨ç½²ä¼˜åŠ¿ã€‚æœªæ¥ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨æ¨¡å‹å¤§å°[CDH+22]å’Œè®­ç»ƒæ­¥éª¤æ–¹é¢æ‰©å¤§RetNetã€‚æ­¤å¤–ï¼Œé€šè¿‡å‹ç¼©é•¿æœŸè®°å¿†ï¼Œä¿ç•™å¯ä»¥æœ‰æ•ˆåœ°ä¸ç»“æ„åŒ–æç¤º[HSD+22b]é…åˆä½¿ç”¨ã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨RetNetä½œä¸ºéª¨å¹²æ¶æ„æ¥è®­ç»ƒå¤šæ¨¡å‹å¤§å‹è¯­è¨€æ¨¡å‹[HSD+22aï¼ŒHDW+23ï¼ŒPWD+23]ã€‚

In addition, we are interested in deploying RetNet models on various edge devices, such as mobile phones.

æ­¤å¤–ï¼Œæˆ‘ä»¬æœ‰å…´è¶£åœ¨å„ç§è¾¹ç¼˜è®¾å¤‡(å¦‚æ‰‹æœº)ä¸Šéƒ¨ç½²RetNetæ¨¡å‹ã€‚

## Acknowledgement
We would like to acknowledge Jiayu Ding, Songlin Yang, and colleagues from MSRA System Group for the helpful discussions.

æˆ‘ä»¬æ„Ÿè°¢ä¸ä½³å®‡ã€æ¨æ¾æ—å’ŒMSRAç³»ç»Ÿé›†å›¢çš„åŒäº‹ä»¬çš„æœ‰ç›Šè®¨è®ºã€‚

## References
* [BKH16] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXivpreprint arXiv:1607.06450, 2016.
* [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, PrafullaDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandï¿¾hini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, ChristopherBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners. In Advances in Neural Information Processing Systems,volume 33, pages 1877â€“1901. Curran Associates, Inc., 2020.
* [BZB+20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa:Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAIConference on Artificial Intelligence, 2020.
* [CCWG21] Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. Summscreen: A datasetfor abstractive screenplay summarization. arXiv preprint arXiv:2104.07091, 2021.
* [CDH+22] Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, SakshamSinghal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. On therepresentation collapse of sparse mixture of experts. In Alice H. Oh, Alekh Agarwal,Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural InformationProcessing Systems, 2022.
* [CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins,and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/noquestions. In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, Volume1 (Long and Short Papers), pages 2924â€“2936, Minneapolis, Minnesota, June 2019.Association for Computational Linguistics.
* [DFE+22] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. Flashattention:Fast and memory-efficient exact attention with io-awareness. Advances in NeuralInformation Processing Systems, 35:16344â€“16359, 2022.
* [DFS+22] Tri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and ChristopherRÃ©. Hungry hungry hippos: Towards language modeling with state space models. arXivpreprint arXiv:2212.14052, 2022.
* [DMI+21] Jesse Dodge, Ana MarasoviÂ´c, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, andMatt Gardner. Documenting large webtext corpora: A case study on the colossal cleancrawled corpus. In Conference on Empirical Methods in Natural Language Processing,2021.
* [GBB+20] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster,Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GBdataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.
* [GGR21] Albert Gu, Karan Goel, and Christopher RÃ©. Efficiently modeling long sequences withstructured state spaces. arXiv preprint arXiv:2111.00396, 2021.
* [HCP+21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficientattentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021.
* [HDW+23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi,Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Languageis not all you need: Aligning perception with language models. ArXiv, abs/2302.14045,2023.
* [HS97] Sepp Hochreiter and JÃ¼rgen Schmidhuber. Long short-term memory. Neural Computaï¿¾tion, 9:1735â€“1780, November 1997.
* [HSD+22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shumï¿¾ing Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv,abs/2206.06336, 2022.
* [HSD+22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structuredprompting: Scaling in-context learning to 1,000 examples. ArXiv, abs/2212.06713,2022.
* [KLBA+22] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, CarlosMuÃ±oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf,Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The Stack: 3 tb ofpermissively licensed source code. Preprint, 2022.
* [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. Transï¿¾formers are rnns: Fast autoregressive transformers with linear attention. In InternationalConference on Machine Learning, pages 5156â€“5165. PMLR, 2020.
* [LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema chalï¿¾lenge. In Thirteenth International Conference on the Principles of Knowledge Repreï¿¾sentation and Reasoning, 2012.
* [LH19] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Internaï¿¾tional Conference on Learning Representations, 2019.
* [MRL+17] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and JamesAllen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of the 2ndWorkshop on Linking Models of Lexical, Sentential and Discourse-level Semantics,pages 46â€“51, 2017.
* [MWH+22] Shuming Ma, Hongyu Wang, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong,Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, and Furu Wei. TorchScale:Transformers at scale. CoRR, abs/2211.13184, 2022.
* [OSG+23] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for longsequences. ArXiv, abs/2303.06349, 2023.
* [PAA+23] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, HuanqiCao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He,Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra,Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang,Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang,Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for thetransformer era, 2023.
* [PMN+23] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus,Yoshua Bengio, Stefano Ermon, and Christopher RÃ©. Hyena hierarchy: Towards largerconvolutional language models. arXiv preprint arXiv:2302.10866, 2023.
* [PWD+23] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, andFuru Wei. Kosmos-2: Grounding multimodal large language models to the world.ArXiv, abs/2306.14824, 2023.
* [RZL17] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Swish: a self-gated activationfunction. arXiv: Neural and Evolutionary Computing, 2017.
* [SDP+22] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim,Vishrav Chaudhary, Xia Song, and Furu Wei. A length-extrapolatable transformer.arXiv preprint arXiv:2212.10554, 2022.
* [Sha19] Noam M. Shazeer. Fast transformer decoding: One write-head is all you need. ArXiv,abs/1911.02150, 2019.
* [SLP+21] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhancedtransformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.
* [SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language modelsusing model parallelism. arXiv preprint arXiv:1909.08053, 2019.
* [SSI+22] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta,Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparisonover long language sequences. arXiv preprint arXiv:2201.03533, 2022.
* [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances inNeural Information Processing Systems 30: Annual Conference on Neural InformationProcessing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 6000â€“6010, 2017.
* [WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the Europeanconference on computer vision (ECCV), pages 3â€“19, 2018.
* [WMD+22] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and FuruWei. DeepNet: Scaling Transformers to 1,000 layers. ArXiv, abs/2203.00555, 2022.
* [WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers.arXiv preprint arXiv:2210.06423, 2022.
* [WPN+19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,Felix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark forgeneral-purpose language understanding systems. arXiv preprint arXiv:1905.00537,2019.
* [ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag:Can a machine really finish your sentence? In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics, 2019.
* [ZYY+21] Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hasï¿¾san Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A newbenchmark for query-based multi-domain meeting summarization. arXiv preprintarXiv:2104.05938, 2021.

## A Hyperparameters
Hyperparameters 1.3B 2.7B 6.7B
Layers 24 32 32
Hidden size 2048 2560 4096
FFN size 4096 5120 8192
Heads 8 10 16
Learning rate 6 Ã— 10âˆ’4 3 Ã— 10âˆ’4 3 Ã— 10âˆ’4
LR scheduler Polynomial decay
Warm-up steps 375
Tokens per batch 4M
Adam Î² (0.9, 0.98)
Training steps 25,000
Gradient clipping 2.0
Dropout 0.1
Weight decay 0.01

Table 7: Hyperparamters used for the models in Section 3.

## B Grouped Results of Different Context Lengths
As shown in Table 8, we report language modeling results with different context lengths. In order to make the numbers comparable, we use 2048 text chunks as evaluation data and only compute perplexity for the last 128 tokens. Experimental results show that RetNet outperforms Transformer across different context lengths. Besides, RetNet can utilize longer context for better results.

Model 512 1024 2048
Transformer 13.55 12.56 12.35
RetNet 13.09 12.14 11.98

Table 8: Language modeling perplexity of RetNet and Transformer with different context length. The results show that RetNet has a consistent advantage across sequence length.