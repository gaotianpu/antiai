# MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices
https://arxiv.org/abs/2004.02984

Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUEscore o 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).

自然语言处理(NLP)最近通过使用具有数亿个参数的巨大预训练模型取得了巨大的成功。然而，这些模型模型尺寸大、延迟高，因此无法部署到资源有限的移动设备上。在本文中，我们提出了MobileBERT来压缩和加速流行的BERT模型。与最初的BERT一样，MobileBERT是任务无关的，即,它可以通过简单的微调一般地应用于各种下游NLP任务。基本上，MobileBERT是BERT_LARGE的精简版，同时配备了瓶颈结构以及精心设计的自注意和前馈网络之间的平衡。为了训练MobileBERT，我们首先训练了一个专门设计的教师模型，一个倒置的瓶颈合并的BERT_LARGE模型。然后，我们将知识从这位老师迁移到MobileBERT。实证研究表明，MobileBERT比BERT_BASE小4.3倍，快5.5倍，同时在众所周知的基准测试中取得了竞争性的结果。在GLUE的自然语言推理任务中，MobileBERT在Pixel 4手机上的GLUEscore为77.7(比BERT_BASE低0.6)，延迟为62毫秒。在SQuAD v1.1/v2.0问答任务中，MobileBERT的dev F1得分为90.0/79.2(比BERT_BASE高1.5/2.1)。