# RoBERTa-wwm-ext Fine-Tuning for Chinese Text Classification
RoBERTa-wwm-ext 中文文本分类微调 https://arxiv.org/abs/2103.00492

Bidirectional Encoder Representations from Transformers (BERT) have shown to be a promising way to dramatically improve the performance across various Natural Language Processing tasks [Devlin et al., 2019]. Meanwhile, progress made over the past few years by various Neural Net-work has also proved the effectiveness of Neural Network in the field of Natural Language Processing. In this project, RoBERTa-wwm-ext [Cui et al., 2019] pre-train language model was adopted and fine-tuned for Chinese text classification. The models were able to classify Chinese texts into two categories, containing descriptions of legal behavior and descriptions of illegal behavior. Four different models are also proposed in the paper. Those models will use RoBERTa-wwm-extas their embedding layer and feed the embedding into different neural networks. The motivation be-hind proposing these models is straightforward. By introducing complex output layer architecture, the overall performance of the models could be improved. All the models were trained on a data set derived from Chinese public court records, and the performance of different models were compared.The experiment shows that the performance of pro-posed models failed to beat the original RoBERTa-wwm-ext model in terms of accuracy and training efficiency.

来自变形金刚的双向编码器表示(BERT)已被证明是一种有希望的方法，可以显著提高各种自然语言处理任务的性能[Devlinet al., 2019]。同时，过去几年各种神经网络工作取得的进展也证明了神经网络在自然语言处理领域的有效性。在这个项目中，采用了RoBERTa wwm ext[Cui et al.，2019]预训练语言模型，并针对中文文本分类进行了微调。这些模型能够将中文文本分为两类，包括对法律行为的描述和对违法行为的描述。本文还提出了四种不同的模型。这些模型将使用RoBERTa wwm ext作为嵌入层，并将嵌入信息输入到不同的神经网络中。提出这些模型的动机很简单。通过引入复杂的输出层架构，可以提高模型的整体性能。所有模型都是在中国公共法庭记录的数据集上训练的，并对不同模型的性能进行了比较。实验表明，在准确性和训练效率方面，提出的模型的性能未能击败原始的RoBERTa wwm-ext模型。