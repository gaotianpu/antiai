# ERNIE 2.0: A Continual Pre-training Framework for Language Understanding
2019-7-29 原文：https://arxiv.org/abs/1907.12412

## Abstract
Recently, pre-trained models have achieved state-of-the-art results in various language understanding tasks, which indicates that pre-training on large-scale corpora may play a crucial role in natural language processing. Current pre-training procedures usually focus on training the model with several simple tasks to grasp the co-occurrence of words or sentences. However, besides co-occurring, there exists other valuable lexical, syntactic and semantic information in training corpora, such as named entity, semantic closeness and discourse relations. In order to extract to the fullest extent, the lexical, syntactic and semantic information from training corpora, we propose a continual pre-training framework named ERNIE 2.0 which builds and learns incrementally pre-training tasks through constant multi-task learning. Experimental results demonstrate that ERNIE 2.0 outperforms BERT and XLNet on 16 tasks including English tasks on GLUE benchmarks and several common tasks in Chinese. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE.

最近，预训练模型在各种语言理解任务中取得了最先进的结果，这表明大规模语料库的预训练可能在自然语言处理中起着至关重要的作用。当前的预训练程序通常侧重于用几个简单的任务训练模型，以掌握单词或句子的共现。然而，除了共现外，训练语料库中还存在其他有价值的词汇、句法和语义信息，如命名实体、语义贴近度和语篇关系。为了最大限度地从训练语料库中提取词汇、句法和语义信息，我们提出了一个名为ERNIE 2.0的持续预训练框架，该框架通过不断的多任务学习，逐步构建和学习预训练任务。实验结果表明，ERNIE 2.0在16项任务上优于BERT和XLNet，其中包括GLUE基准上的英语任务和汉语中的几个常见任务。源代码和预先训练的模型已在https://github.com/PaddlePaddle/ERNIE.