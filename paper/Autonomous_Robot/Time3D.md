# Time3D: End-to-End Joint Monocular 3D Object Detection and Tracking for Autonomous Driving

While separately leveraging monocular 3D object detection and 2D multi-object tracking can be straightforwardly applied to sequence images in a frame-by-frame fashion, stand-alone tracker cuts off the transmission of the uncertainty from the 3D detector to tracking while cannot pass tracking error differentials back to the 3D detector. In this work, we propose jointly training 3D detection and 3D tracking from only monocular videos in an end-to-end manner. The key component is a novel spatial-temporal information flow module that aggregates geometric and appearance features to predict robust similarity scores across all objects in current and past frames. Specifically, we leverage the attention mechanism of the transformer, in which self-attention aggregates the spatial information in a specific frame, and cross-attention exploits relation and affinities of all objects in the temporal domain of sequence frames. The affinities are then supervised to estimate the trajectory and guide the flow of information between corresponding 3D objects. In addition, we propose a temporal consistency loss that explicitly involves 3D target motion modeling into the learning, making the 3D trajectory smooth in the world coordinate system. Time3D achieves 21.4\% AMOTA, 13.6\% AMOTP on the nuScenes 3D tracking benchmark, surpassing all published competitors, and running at 38 FPS, while Time3D achieves 31.2\% mAP, 39.4\% NDS on the nuScenes 3D detection benchmark.

虽然单独利用单眼3D对象检测和2D多对象跟踪可以直接应用于逐帧方式的序列图像，但独立跟踪器切断了从3D检测器到跟踪的不确定性传输，同时不能将跟踪误差差传递回3D检测器。在这项工作中，我们建议以端到端的方式从单目视频中联合训练3D检测和3D跟踪。关键组件是一个新的时空信息流模块，它聚合几何和外观特征，以预测当前和过去帧中所有对象的稳健相似性分数。具体来说，我们利用了变换器的注意力机制，其中自我注意力聚集了特定帧中的空间信息，而交叉注意力利用了序列帧的时域中所有对象的关系和相似性。然后监督相关性以估计轨迹并引导相应3D对象之间的信息流。此外，我们提出了一种时间一致性损失，该损失明确地将3D目标运动建模纳入学习，使3D轨迹在世界坐标系中平滑。Time3D在nuScenes 3D跟踪基准上实现21.4%的AMOTA和13.6%的AMOTP，超过所有已发布的竞争对手，以38 FPS的速度运行，而Time3D在nuScenes三维检测基准上实现31.2%的mAP和39.4%的NDS。