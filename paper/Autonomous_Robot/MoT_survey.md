# DEEP LEARNING IN VIDEO MULTI-OBJECT TRACKING: A SURVEY
视频中多目标跟踪深度学习的一项调查 2019.7.18 https://arxiv.org/abs/1907.12740

## 阅读笔记
* 目标追踪和目标检测的区别和联系？

## Abstract
The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions.

多目标跟踪(MOT) 的问题在于跟踪序列中不同目标的轨迹，通常是视频。 近年来，随着深度学习的兴起，为该问题提供解决方案的算法受益于深度模型的表征能力。 本文对采用深度学习模型解决单摄像头视频 MOT 任务的工作进行了全面调查。 确定了 MOT算法 的四个主要步骤，并深入回顾了深度学习在每个阶段中的应用方式。 还提供了三个 MOTChallenge 数据集上所呈现工作的完整实验比较，确定了表现最好的方法之间的许多相似之处，并提出了一些可能的未来研究方向。

Keywords Multiple Object Tracking · Deep Learning · Video Tracking · Computer Vision · Convolutional Neural Networks · LSTM · Reinforcement Learning  

## 1 Introduction
Multiple Object Tracking (MOT), also called Multi-Target Tracking (MTT), is a computer vision task that aims to analyze videos in order to identify and track objects belonging to one or more categories, such as pedestrians, cars, animals and inanimate objects, without any prior knowledge about the appearance and number of targets. Differently from object detection algorithms, whose output is a collection of rectangular bounding boxes identified by their coordinates, height and width, MOT algorithms also associate a target ID to each box (known as a detection), in order to distinguish among intra-class objects. An example of the output of a MOT algorithm is illustrated in figure 1. The MOT task plays an important role in computer vision: from video surveillance to autonomous cars, from action recognition to crowd behaviour analysis, many of these problems would benefit from a high-quality tracking algorithm. 

多目标跟踪 (MOT)，也称为多目标跟踪 (MTT)，是一项计算机视觉任务，旨在分析视频以识别和跟踪属于一个或多个类别的目标，例如行人、汽车、动物和无生命的物体 目标，没有任何关于目标的外观和数量的先验知识。 不同于目标检测算法，其输出是一组由坐标、高度和宽度标识的矩形边框，MOT算法 还将目标 ID 关联到每个框(称为检测)，以便区分类内目标 . 图 1 展示了 MOT算法 的输出样本。MOT 任务在计算机视觉中发挥着重要作用：从视频监控到自动驾驶汽车，从动作识别到人群行为分析，其中许多问题都将受益于高质量跟踪算法。

Figure 1: An illustration of the output of a MOT algorithm. Each output bounding box has a number that identifies a specific person in the video.
图 1：MOT算法 输出的图示。 每个输出边框都有一个数字，用于标识视频中的特定人物。

While in Single Object Tracking (SOT) the appearance of the target is known a priori, in MOT a detection step is necessary to identify the targets, that can leave or enter the scene. The main difficulty in tracking multiple targets simultaneously stems from the various occlusions and interactions between objects, that can sometimes also have similar appearance. Thus, simply applying SOT models directly to solve MOT leads to poor results, often incurring in target drift and numerous ID switch errors, as such models usually struggle in distinguishing between similar looking intra-class objects. A series of algorithms specifically tuned to multi-target tracking have then been developed in recent years to address these issues, together with a number of benchmark datasets and competitions to ease the comparisons between the different methods.

在单目标跟踪 (SOT) 中，目标的外观是先验已知的，而在 MOT 中，需要一个检测步骤来识别可以离开或进入场景的目标。 同时跟踪多个目标的主要困难源于目标之间的各种遮挡和相互作用，有时它们也可能具有相似的外观。 因此，简单地直接应用 SOT 模型来解决 MOT 会导致糟糕的结果，通常会导致目标漂移和大量 ID 切换错误，因为此类模型通常难以区分外观相似的类内目标。 近年来开发了一系列专门针对多目标跟踪的算法来解决这些问题，同时还开发了许多基准数据集和竞赛，以简化不同方法之间的比较。

Recently, more and more of such algorithms have started exploiting the representational power of deep learning (DL). The strength of Deep Neural Networks (DNN) resides in their ability to learn rich representations and to extract complex and abstract features from their input. Convolutional neural networks (CNN) currently constitute the state-of-the-art in spatial pattern extraction, and are employed in tasks such as image classification [1, 2, 3] or object detection [4, 5, 6], while recurrent neural networks (RNN) like the Long Short-Term Memory (LSTM) are used to process sequential data, like audio signals, temporal series and text [7, 8, 9, 10]. Since DL methods have been able to reach top performance in many of those tasks, we are now progressively seeing them used in most of the top performing MOT algorithms, aiding to solve some of the subtasks in which the problem is divided. 

最近，越来越多的此类算法开始利用深度学习 (DL) 的表征能力。 深度神经网络 (DNN) 的优势在于它们能够学习丰富的表征并从输入中提取复杂和抽象的特征。 卷积神经网络 (CNN) 目前构成空间模式提取的最新技术，并用于图像分类 [1、2、3] 或目标检测 [4、5、6] 等任务，而循环网络(RNN)，像长短期记忆 (LSTM) 这样的神经网络用于处理顺序数据，例如音频信号、时间序列和文本 [7、8、9、10]。 由于 DL 方法已经能够在其中许多任务中达到最佳性能，我们现在逐渐看到它们被用于大多数性能最佳的 MOT算法 中，帮助解决一些将问题分解的子任务。

This work presents a survey of algorithms that make use of the capabilities of deep learning models to perform Multiple Object Tracking, focusing on the different approaches used for the various components of a MOT algorithm and putting them in the context of each of the proposed methods. While the MOT task can be applied to both 2D and 3D data, and to both single-camera and multi-camera scenarios, in this survey we focus on 2D data extracted from videos recorded by a single camera.

这项工作对利用深度学习模型的能力执行多目标跟踪的算法进行了调查，重点关注用于 MOT算法 的各个组件的不同方法，并将它们放在每个建议方法的上下文中。 虽然 MOT 任务可以应用于 2D 和 3D 数据，以及单摄像头和多摄像头场景，但在本次调查中，我们重点关注从单摄像头录制的视频中提取的 2D 数据。

Some reviews and surveys have been published on the subject of MOT. Their main contributions and limitations are the following: 
* Luo et al. [11] presented the first comprehensive review to focus specifically on MOT, in particular on pedestrian tracking. They provided a unified formulation of the MOT problem and described the main techniques used in the key steps of a MOT system. They presented deep learning as one of the future research directions, since at the time it had only been employed by very few algorithms. 
* Camplani et al. [12] presented a survey on Multiple Pedestrian Tracking, but they focused on RGB-D data, while our focus is on 2D RGB images, without additional inputs. Moreover, their review does not cover deep learning based algorithms. 
* Emami et al. [13] proposed a formulation of single and multi-sensor tracking tasks as a Multidimensional Assignment Problem (MDAP). They also presented a few approaches that employed deep learning in tracking problems, but it wasn’t the focus of their paper and they didn’t provide any experimental comparison among such methods. 
* Leal-Taixé et al. [14] presented an analysis of the results obtained by algorithms on the MOT15 [15] and MOT16 [16] datasets, providing a summary of the trending lines of research and statistics about the results. They found that after 2015, methods have been shifting from trying to find better optimization algorithms for the association problem to focusing on improving the affinity models, and they predict that many more approaches would tackle this issue by using deep learning. However, this work also did not focus on deep learning, and it does not cover more recent MOT algorithms, published in the last years.

一些关于 MOT 主题的评论和调查已经发表。 它们的主要贡献和局限性如下：
* Luo et al. [11] 提出了第一个综合调查，专门关注 MOT，特别是行人跟踪。 他们提供了 MOT 问题的统一表述，并描述了 MOT 系统关键步骤中使用的主要技术。 他们将深度学习作为未来的研究方向之一，因为当时只有极少数算法采用它。
* Camplani et al. [12] 提出了一项关于多行人跟踪调查，但他们专注于 RGB-D 数据，而我们的重点是 2D RGB 图像，没有额外的输入。 此外，他们的评论不包括基于深度学习的算法。
* Emami et al. [13] 提出了一个单一和多传感器跟踪任务的公式作为多维分配问题(MDAP)。 他们还提出了一些在跟踪问题中采用深度学习的方法，但这不是他们论文的重点，他们也没有提供这些方法之间的任何实验比较。
* Leal-Taixé et al. [14] 对算法在 MOT15 [15] 和 MOT16 [16] 数据集上获得的结果进行了分析，提供了研究趋势和结果统计的总结。 他们发现，在 2015之后，方法已经从试图为关联问题寻找更好的优化算法转变为专注于改进亲和力模型，他们预测更多的方法将通过使用深度学习来解决这个问题。 然而，这项工作也没有关注深度学习，也没有涵盖最近几年发布的最近的 MOT算法 。 <!-- affinity 亲和力 ?-->

In this paper, based on the discussed limitations, our aim is to provide a survey with the following main contributions: 
* We provide the first comprehensive survey on the use of Deep Learning in Multiple Object Tracking, focusing on 2D data extracted from single-camera videos, including recent works that have not been covered by past surveys and reviews. The use of DL in MOT is in fact recent, and many approaches have been published in the last three years. 
* We identify four common steps in MOT algorithms and describe the different DL models and approaches employed in each of those steps, including the algorithmic context in which they are used. The techniques utilized by each analyzed work are also summarized in a table, together with links to the available source code, to serve as a quick reference for future research. 
* We collect experimental results on the most commonly used MOT datasets to perform a numerical comparison among them, also identifying the main trends in the best performing algorithms. 
* As final point, we discuss the possible future directions of research.

在本文中，基于所讨论的局限性，我们的目标是提供一项具有以下主要贡献的调查：
* 提供了关于在多目标跟踪中使用深度学习的首次综合调查，重点关注从单摄像头视频中提取的 2D 数据，包括过去调查和评论未涵盖的近期工作。 在 MOT 中使用 DL 实际上是最近才出现的，并且在过去三年中已经发布了许多方法。
* 确定了 MOT算法 中的四个常见步骤，并描述了每个步骤中采用的不同 DL 模型和方法，包括使用它们的算法上下文。 每个分析工作使用的技术也总结在一个表中，连同可用源代码的链接，作为未来研究的快速参考。
* 在最常用的 MOT 数据集上收集实验结果，对它们进行数值比较，同时确定性能最佳算法的主要趋势。
* 作为最后一点，我们讨论了未来可能的研究方向。

The survey is further organized in this manner. We first describe the general structure of MOT algorithms and the most commonly used metrics and datasets in section 2. Section 3 explores the various DL-based models and algorithms in each of the four identified steps of a MOT algorithm. Section 4 presents a numerical comparison among the presented algorithms and identifies common trends and patterns in current approaches, as well as some limitations and possible future research directions. Finally, section 5 summarizes the findings of the previous sections and presents some final remarks. 

以这种方式进一步组织调查。 我们首先在第 2 节中描述了 MOT算法 的一般结构以及最常用的指标和数据集。第 3 节探讨了 MOT算法 四个确定步骤中每个步骤中的各种基于 DL 的模型和算法。 第 4 节对所提出的算法进行了数值比较，并确定了当前方法中的常见趋势和模式，以及一些局限性和未来可能的研究方向。 最后，第 5 节总结了前面几节的发现并提出了一些最后的评论。

## 2 MOT: algorithms, metrics and datasets
In this section, a general description about the problem of MOT is provided. The main characteristics and common steps of MOT algorithms are identified and described in section 2.1. The metrics that are usually employed to evaluate the performance of the models are discussed in section 2.2, while the most important benchmark datasets are presented in section 2.3. 

在本节中，提供了有关 MOT 问题的一般描述。 MOT算法 的主要特征和通用步骤在 2.1 节中进行了识别和描述。 通常用于评估模型性能的指标在 2.2 节中讨论，而最重要的基准数据集在 2.3 节中介绍。

### 2.1 Introduction to MOT algorithms
The standard approach employed in MOT algorithms is tracking-by-detection: a set of detections (i.e. bounding boxes identifying the targets in the image) are extracted from the video frames and are used to guide the tracking process, usually by associating them together in order to assign the same ID to bounding boxes that contain the same target. For this reason, many MOT algorithms formulate the task as an assignment problem. Modern detection frameworks [4, 17, 18, 5, 6] ensure a good detection quality, and the majority of MOT methods (with some exceptions, as we will see) have been focusing on improving the association; indeed, many MOT datasets provide a standard set of detections that can be used by the algorithms (that can thus skip the detection stage) in order to exclusively compare their performances on the quality of the association algorithm, since the detector performance can heavily affect the tracking results.

MOT算法 中采用的标准方法是检测跟踪：从视频帧中提取一组检测(即识别图像中目标的边框)并用于指导跟踪过程，通常通过将相同的 ID 分配给包含相同目标的边框,  把它们关联在一起。 出于这个原因，许多 MOT算法 将任务制定为分配问题。 现代检测框架 [4、17、18、5、6] 确保了良好的检测质量，大多数 MOT 方法(除了一些例外，正如我们将看到的)一直专注于提高关联;  事实上，许多 MOT 数据集提供了一组标准的检测，算法可以使用这些检测(因此可以跳过检测阶段)，以便专门比较它们在关联算法质量方面的性能，因为检测器性能会严重影响 跟踪结果。<!-- 关联算法 -->

MOT algorithms can also be divided into batch and online methods. Batch tracking algorithms are allowed to use future information (i.e. from future frames) when trying to determine the object identities in a certain frame. They often exploit global information and thus result in better tracking quality. Online tracking algorithms, on the contrary, can only use present and past information to make predictions about the current frame. This is a requirement in some scenarios, like autonomous driving and robot navigation. Compared to batch methods, online methods tend to perform worse, since they cannot fix past errors using future information. It is important to note that while a real-time algorithm is required to run in an online fashion, not every online method necessarily runs in real-time; quite often, in fact, with very few exceptions, online algorithms are still too slow to be employed in a real-time environment, especially when exploiting deep learning algorithms, that are often computationally intensive.

MOT算法也可以分为批处理和在线方法。 当尝试确定特定帧中的目标身份时，允许批跟踪算法使用未来信息(即来自未来帧的信息)。 他们经常利用全局信息，从而提高跟踪质量。 相反，在线跟踪算法只能使用现在和过去的信息来预测当前帧。 这是某些场景的要求，例如自动驾驶和机器人导航。 与批处理方法相比，在线方法往往表现更差，因为它们无法使用未来的信息修复过去的错误。 重要的是要注意，虽然需要实时算法以在线方式运行，但并非每个在线方法都必须实时运行;  事实上，除了极少数例外，在线算法在实时环境中的应用仍然太慢，尤其是在利用深度学习算法时，这些算法通常是计算密集型的。

Despite the huge variety of approaches presented in the literature, the vast majority of MOT algorithms share part or all of the following steps (summarized in figure 2): 
* Detection stage: an object detection algorithm analyzes each input frame to identify objects belonging to the target class(es) using bounding boxes, also known as ‘detections’ in the context of MOT; 
* Feature extraction/motion prediction stage: one or more feature extraction algorithms analyze the detections and/or the tracklets to extract appearance, motion and/or interaction features. Optionally, a motion predictor predicts the next position of each tracked target; 
* Affinity stage: features and motion predictions are used to compute a similarity/distance score between pairs of detections and/or tracklets; 
* Association stage: the similarity/distance measures are used to associate detections and tracklets belonging to the same target by assigning the same ID to detections that identify the same target.  

尽管文献中提出了各种各样的方法，但绝大多数 MOT算法 都共享以下部分或全部步骤(总结在图 2 中)：
* 检测阶段：目标检测算法分析每个输入帧以使用边框识别属于目标类的目标，在 MOT 的上下文中也称为“检测”; 
* 特征提取/运动预测阶段：一种或多种特征提取算法分析检测和/或轨迹以提取外观、运动和/或交互特征。 可选地，运动预测器预测每个跟踪目标的下一个位置; 
* 亲和力阶段：特征和运动预测用于计算检测对和/或轨迹之间的相似性/距离分数; 
* 关联阶段：相似性/距离度量用于通过将相同的ID分配给识别相同目标的检测来关联属于同一目标的检测和轨迹。

Figure 2: Usual workflow of a MOT algorithm: given the raw frames of a video (1), an object detector is run to obtain the bounding boxes of the objects (2). Then, for every detected object, different features are computed, usually visual and motion ones (3). After that, an affinity computation step calculates the probability of two objects belonging to the same target (4), and finally an association step assigns a numerical ID to each object (5).
图 2：MOT算法 的常规工作流程：给定视频的原始帧 (1)，运行目标检测器以获得目标的边框 (2)。 然后，对于每个检测到的目标，计算不同的特征，通常是视觉和运动特征 (3)。 之后，亲和力计算步骤计算两个目标属于同一目标的概率 (4)，最后关联步骤为每个目标分配一个数字 ID (5)。

While these stages can be performed sequentially in the order presented here (often once per frame for online methods and once for the whole video for batch methods), there are many algorithms that merge some of these steps together, or intertwine them, or even perform them multiple times using different techniques (e.g. in algorithms that work in two phases). Moreover, some methods do not directly associate detections together, but use them to refine trajectory predictions and to manage initialization and termination of new tracks; nonetheless, many of the presented steps can often still be identified even in such cases, as we will see.

虽然这些阶段可以按此处显示的顺序顺序执行(在线方法通常每帧一次，批处理方法整个视频一次)，但有许多算法将这些步骤中的一些合并在一起，或将它们交织在一起，甚至执行 他们多次使用不同的技术(例如，在分两个阶段工作的算法中)。 此外，一些方法不直接将检测关联在一起，而是使用它们来改进轨迹预测并管理新轨迹的初始化和终止;  尽管如此，正如我们将要看到的那样，即使在这种情况下，许多提出的步骤仍然可以经常被识别出来。 <!--目标追踪有什么用，如何指导无人驾驶的决策？-->

### 2.2 Metrics
In order to provide a common experimental setup where algorithms can be fairly tested and compared, a group of metrics have been de facto established as standard, and they are used in almost every work. The most relevant ones are metrics defined by Wu and Nevatia [19], the so-called CLEAR MOT metrics [20], and recently the ID metrics [21]. These sets of metrics aim to reflect the overall performance of the tested models, and point out the possible drawbacks of each one. Therefore, those metrics are defined as follows:

为了提供可以公平测试和比较算法的通用实验设置，实际上已经将一组指标确立为标准，并且它们几乎用于每项工作。 最相关的是 Wu 和 Nevatia [19] 定义的指标，即所谓的 CLEAR MOT 指标 [20]，以及最近的 ID 指标 [21]。 这些指标集旨在反映被测模型的整体性能，并指出每个模型可能存在的缺点。 因此，这些指标定义如下：

#### Classical metrics 经典指标
These metrics, defined by Wu and Nevatia [19], highlight the different types of errors a MOT algorithm can make. In order to show those problems, the following values are computed: 
* Mostly Tracked (MT) trajectories: number of ground-truth trajectories that are correctly tracked in at least 80% of the frames. 
* Fragments: trajectory hypotheses which cover at most 80% of a ground truth trajectory. Observe that a true trajectory can be covered by more than one fragment. 
* Mostly Lost (ML) trajectories: number of ground-truth trajectories that are correctly tracked in less than 20% of the frames. 
* False trajectories: predicted trajectories which do not correspond to a real object (i.e. to a ground truth trajectory). 
* ID switches: number of times when the object is correctly tracked, but the associated ID for the object is mistakenly changed.

这些指标由 Wu 和 Nevatia [19] 定义，突出了 MOT算法 可能产生的不同类型的错误。 为了显示这些问题，计算了以下值：
* 大多数跟踪 (MT) 轨迹：在至少 80% 的帧中正确跟踪的基准实况轨迹的数量。
* 片段：轨迹假设最多覆盖 80% 的真实轨迹。 观察到一个真实的轨迹可以被多个片段覆盖。
* 大部分丢失 (ML) 轨迹：在不到 20% 的帧中被正确跟踪的基准实况轨迹的数量。
* 错误轨迹：与真实物体不对应的预测轨迹(即地面真实轨迹)。
* ID switches：目标被正确跟踪，但目标的关联ID被错误更改的次数。

#### CLEAR MOT metrics
The CLEAR MOT metrics were developed for the Classification of Events, Activities and Relationships (CLEAR) workshops held in 2006 [22] and 2007 [23]. The workshops were jointly organized by the the European CHIL project, the U.S. VACE project, and the National Institute of Standards and Technology (NIST). Those metrics are MOTA (Multiple Object Tracking Accuracy) and MOTP (Multiple Object Tracking Precision). They serve as a summary of other simpler metrics which compose them. We will explain the simpler metrics at first and build the complex ones over them. A detailed description on how to match the real objects (ground truth) with the tracker hypothesis can be found in [20], as it is not trivial how to consider when a hypothesis is related to an object, and it depends on the precise tracking task to be evaluated. In our case, as we are focusing on 2D tracking with single camera, the most used metric to decide whether an object and a prediction are related or not is Intersection over Union (IoU) of bounding boxes, as it was the measure established in the presentation paper of MOT15 dataset [15]. Specifically, the mapping between ground truth and hypotheses is established as follows: if the ground truth object $o_i$ and the hypothesis $h_j$ are matched in frame t − 1, and in frame t the $IoU(o_i , h_j ) ≥ 0.5$, then $o_i$ and $h_j$ are matched in that frame, even if there exists another hypothesis $h_k$ such that $IoU(o_i , h_j ) < IoU(o_i , h_k)$, considering the continuity constraint. After the matching from previous frames has been performed, the remaining objects are tried to be matched with the remaining hypotheses, still using a 0.5 IoU threshold. The ground truth bounding boxes that cannot be associated with a hypothesis are counted as false negatives (FN), and the hypotheses that cannot be associated with a real bounding box are marked as false positives (FP). Also, every time a ground truth object tracking is interrupted and later resumed is counted as a fragmentation, while every time a tracked ground truth object ID is incorrectly changed during the tracking duration is counted as an ID switch. Then, the simple metrics computed are the following: 
* FP: the number of false positives in the whole video; 
* FN: the number of false negatives in the whole video; 
* Fragm: the total number of fragmentations; 
* IDSW: the total number of ID switches.

CLEAR MOT 指标是为 2006 [22] 和 2007 [23] 举办的事件、活动和关系分类 (CLEAR) 研讨会制定的。 这些研讨会由欧洲 CHIL 项目、美国 VACE 项目和美国国家标准与技术研究院 (NIST) 联合举办。 这些指标是 MOTA(多目标跟踪精度)和 MOTP(多目标跟踪精度)。 它们充当构成它们的其他更简单指标的摘要。 我们将首先解释更简单的指标，然后在它们之上构建复杂的指标。 关于如何将真实物体(基准实况)与跟踪器假设相匹配的详细描述可以在 [20] 中找到，因为当假设与物体相关时如何考虑并不是微不足道的，它取决于精确的跟踪 要评估的任务。 在我们的例子中，由于我们专注于使用单个摄像头进行 2D 跟踪，因此最常用来确定目标和预测是否相关的指标是边框的并集交集(IoU)，因为它是在 MOT15 数据集的演示文稿 [15]。 具体来说，基准实况 和假设之间的映射建立如下：如果 基准实况目标 $o_i$ 和假设 $h_j$ 在帧 t − 1 中匹配，并且在帧 t 中 $IoU(o_i , h_j ) ≥ 0.5 $，则 $o_i$ 和 $h_j$ 在该框架中匹配，即使存在另一个假设 $h_k$ 使得 $IoU(o_i, h_j) < IoU(o_i, h_k)$，考虑到连续性约束。 在执行了先前帧的匹配之后，尝试将剩余的目标与剩余的假设进行匹配，仍然使用 0.5 IoU 阈值。 无法与假设相关联的真实边框被计为假阴性 (FN)，而无法与真实边框相关联的假设被标记为假阳性 (FP)。 此外，每次 基准实况 目标跟踪被中断并稍后恢复都被计为一次碎片，而每次被跟踪的 基准实况 目标 ID 在跟踪持续时间内被错误更改都被计为 ID 切换。 然后，计算出的简单指标如下：
* FP：整个视频中误报的数量; 
* FN：整个视频中漏报的数量; 
* Fragm：分片总数; 
* IDSW：ID开关总数。

The MOTA score is then defined as follows: 

MOTA分数定义如下：

$MOTA = 1 − \frac{(FN + FP + IDSW )}{GT}\ ∈ (−∞, 1] $

where GT is the number of ground truth boxes. It is important to note that the score can be negative, as the algorithm can commit a number of errors greater than the number of ground truth boxes. Usually, instead of reporting MOTA, it is common to report the percentage MOTA, which is just the previous expression expressed as a percentage. On the other hand, MOTP is computed as:

其中 GT 是基准实况框的数量。 重要的是要注意分数可能是负数，因为算法可能犯下的错误数量大于基准实况框的数量。 通常，不报告 MOTA，而是报告 MOTA 百分比，这只是前面用百分比表示的表达式。 另一方面，MOTP 计算如下：

$MOTP = \frac{ \sum_{t,i} d_{t,i} }{\sum_{t} c_t }$

where $c_t$ denotes the number of matches in frame t, and  $d_{t,i}$  is the bounding box overlap between the hypothesis i with its assigned ground truth object. It is important to note that this metric takes few information about tracking into account, and rather focuses on the quality of the detections.

其中 $c_t$ 表示帧 t 中的匹配数，$d_{t,i}$ 是假设 i 与其分配的基准实况目标之间的边框重叠。 重要的是要注意，该指标很少考虑有关跟踪的信息，而是侧重于检测的质量。

#### ID scores
The main problem of MOTA score is that it takes into account the number of times a tracker makes an incorrect decision, such as an ID switch, but in some scenarios (e.g. airport security) one could be more interested in rewarding a tracker that can follow an object for the longest time possible, in order to not lose its position. Because of that, in [21] a couple of alternative new metrics are defined, that are supposed to complement the information given by the CLEAR MOT metrics. Instead of matching ground truth and detections frame by frame, the mapping is performed globally, and the trajectory hypothesis assigned to a given ground truth trajectory is the one that maximizes the number of frames correctly classified for the ground truth. In order to solve that problem, a bipartite graph is constructed, and the minimum cost solution for that problem is taken as the problem solution. For the bipartite graph, the sets of vertices are defined as follows: the first set of vertices, $V_T$ , has a so-called regular node for each true trajectory, and a false positive node for each computed trajectory. The second set, $V_C$ , has a regular node for each computed trajectory and a false negative for each true one. The costs of the edges are set in order to count the number of false negative and false positive frames in case that edge were chosen (more information can be found in [21]). After the association is performed, there are four different possible pairs, attending to the nature of the involved nodes. If a regular node from $V_T$ is matched with a regular node of $V_C$ (i.e. a true trajectory is matched with a computed trajectory), a true positive ID is counted. Every false positive from $V_T$ matched with a regular node from $V_C$ counts as a false positive ID. Every regular node from $V_T$ matched with a false negative from $V_C$ counts as a false negative ID, and finally, every false positive matched with a false negative counts as a true negative ID. Afterwards, three scores are calculated. IDTP is the sum of the weights of the edges selected as true positive ID matches (it can be seen as the percentage of detections correctly assigned in the whole video). IDFN is the sum of weights from the selected false negative ID edges, and IDFP is the sum of weights from the selected false positive ID edges. 

MOTA评分的主要问题是，它考虑了追踪器做出错误决定的次数，例如ID切换，但在某些情况下(例如机场安全)，人们可能更感兴趣的是奖励跟踪目标最长时间的追踪器，以便不丢失其位置。因此，在[21]中定义了一些替代的新度量，这些度量被认为是对CLEAR MOT度量给出的信息的补充。不是逐帧匹配基准实况和检测，而是全局执行映射，并且分配给给定基准实况轨迹的轨迹假设是最大化针对基准实况正确分类的帧数的假设。为了解决该问题，构造了一个二分图，并将该问题的最小代价解作为问题解。对于二分图，顶点集定义如下：第一组顶点$V_T$对于每个真实轨迹都有一个所谓的规则节点，对于每个计算轨迹都有假正节点。第二个集合$V_C$为每个计算出的轨迹都有一个规则节点，为每个真轨迹都有假阴性。设置边缘的成本，以便在选择边缘的情况下计算假阴性和假阳性帧的数量(更多信息见[21])。在执行关联之后，有四个不同的可能对，关注所涉及节点的性质。如果来自$V_T$的规则节点与$V_C$的常规节点相匹配(即，真实轨迹与计算轨迹相匹配)，则对真实正ID进行计数。来自$V_T$的每个假阳性与来自$V_C$的常规节点匹配都算作假阳性ID。来自$V_T$的每个常规节点与来自$V-C$的假阴性匹配都算作伪阴性ID。最后，每个与假阴性匹配的假阳性都算作真阴性ID。然后，计算三个分数。IDTP是被选为真正ID匹配的边缘的权重之和(可以看作是整个视频中正确分配的检测百分比)。IDFN是来自所选假负ID边缘的权重之和，而IDFP是来自所选择假正ID边缘的加权之和。

With these three basic measures, another three measures are computed: 
* Identification precision: IDP = $\frac{IDTP}{IDTP + IDFP } $
* Identification recall: IDR = $\frac{IDTP}{IDTP + IDFN} $ 
* Identification F1: $IDF1 = \frac{2}{\frac{1}{IDP} + \frac{1}{IDR}} = \frac{2IDTP}{2IDTP+IDFP+IDFN} $

Usually, the reported metrics in almost every piece of work are the CLEAR MOT metrics, mostly tracked trajectories (MT), mostly lost trajectories (ML) and IDF1, since this metrics are the ones shown in MOTChallenge leaderboards (see section 2.3 for details). Additionally, the number of frames per second (FPS) the tracker can process is often reported, and is also included in the leaderboards. However, we find this metric difficult to compare among different algorithms, since some of the methods include the detection phase while others skip that computation. Also, the dependency on the hardware employed is relevant in terms of speed.

通常，几乎每项工作中报告的指标都是CLEAR MOT指标，主要是跟踪轨迹(MT)、丢失轨迹(ML)和IDF1，因为该指标显示在MOTChallenge排行榜中(详见第2.3节)。此外，追踪器可以处理的每秒帧数(FPS)经常被报告，并且也被包括在排行榜中。然而，我们发现这种度量很难在不同的算法之间进行比较，因为一些方法包括检测阶段，而其他方法则跳过了计算。此外，对所用硬件的依赖性在速度方面也是相关的。

### 2.3 Benchmark datasets
In the past few years, a number of datasets for MOT have been published. In this section we are going to describe the most important ones, starting from a general description of the MOTChallenge benchmark, then focusing on its datasets, and finally describing KITTI and other less commonly used MOT datasets.

在过去的几年中，已经发布了许多 MOT 数据集。 在本节中，我们将描述最重要的部分，从 MOTChallenge 基准的一般描述开始，然后重点介绍其数据集，最后描述 KITTI 和其他不太常用的 MOT 数据集。

#### MOTChallenge. 
MOTChallenge(1 https://motchallenge.net/) is the most commonly used benchmark for multiple object tracking. It provides, among others, some of largest datasets for pedestrian tracking that are currently publicly available. For each dataset, the ground truth for the training split, and detections for both training and test splits are provided. The reason why MOTChallenge datasets frequently provide detections (often referred to as public detections, as opposed to the private detections, that are obtained by the algorithm authors by using a detector of their own) is that the detection quality has a big impact on the final performance of the tracker, but the detection part of the algorithms is often independent from the tracking part and usually uses already existing models; providing public detections that every model can use makes the comparison of the tracking algorithms easier, since the detection quality is factored out from the performance computation and trackers start on a common ground. The evaluation of an algorithm on the test dataset is done by submitting the results to a test server. The MOTChallenge website contains a leaderboard for each of the datasets, showing in separate pages models using the publicly provided detections and the ones using private detections. Online methods are also marked as so. MOTA is the primary evaluation score for the MOTChallenge, but many other metrics are shown, including all the ones presented in section 2.2. As we will see, since the vast majority of MOT algorithms that use deep learning focus on pedestrians, the MOTChallenge datasets are the most widely used, as they are the most comprehensive ones currently available, providing more data to train deep models.

MOT挑战。 MOTChallenge(1 https://motchallenge.net/) 是多目标跟踪最常用的基准。 它提供了目前公开可用的一些最大的行人跟踪数据集。 对于每个数据集，提供了训练拆分的基准实况以及训练和测试拆分的检测。 MOTChallenge 数据集之所以频繁提供检测(通常称为公共检测，相对于私有检测，算法作者使用自己的检测器获得)是因为检测质量对最终结果有很大影响 跟踪器的性能，但算法的检测部分通常独立于跟踪部分并且通常使用已经存在的模型;  提供每个模型都可以使用的公共检测使得跟踪算法的比较更容易，因为检测质量是从性能计算中提取出来的，并且跟踪器从一个共同的基础开始。 通过将结果提交给测试服务器来对测试数据集上的算法进行评估。 MOTChallenge 网站包含每个数据集的排行榜，在单独的页面模型中显示使用公开提供的检测和使用私人检测的模型。 在线方法也被标记为如此。 MOTA 是 MOTChallenge 的主要评估分数，但还显示了许多其他指标，包括第 2.2 节中介绍的所有指标。 正如我们将看到的，由于绝大多数使用深度学习的 MOT算法 都专注于行人，因此 MOTChallenge 数据集使用最广泛，因为它们是目前可用的最全面的数据集，可以提供更多数据来训练深度模型。

#### MOT15. 
The first MOTChallenge dataset is 2D MOT 20152 [15] (often just called MOT15). It contains a series of 22 videos (11 for training and 11 for testing), collected from older datasets, with a variety of characteristics (fixed and moving cameras, different environments and lighting conditions, and so on) so that the models would need to generalize better in order to obtain good results on it. In total, it contains 11283 frames at various resolutions, with 1221 different identities and 101345 boxes. The provided detections were obtained using the ACF detector [24].

MOT15。 第一个 MOTChallenge 数据集是 2D MOT 20152 [15](通常简称为 MOT15)。 它包含一系列 22 个视频(11 个用于训练，11 个用于测试)，这些视频是从较旧的数据集中收集的，具有各种特征(固定和移动相机、不同的环境和光照条件等)，因此模型需要 更好地泛化以获得好的结果。 总共包含 11283 个不同分辨率的帧，1221 个不同的身份和 101345 个框。 提供的检测是使用 ACF 检测器获得的 [24]。

#### MOT16/17. 
A new version of the dataset was presented in 2016, called MOT163 [16]. This time, the ground truth was made from scratch, so that it was consistent throughout the dataset. The videos are also more challenging, since they have a higher pedestrian density. A total of 14 videos are included in the set (7 for training and 7 for testing), with public detections obtained using the Deformable Part-based Model (DPM) v5 [25, 26], that they found to obtain better performance in detecting pedestrians on the dataset when compared to other models. This time the dataset includes 11235 frames with 1342 identities and 292733 boxes in total. The MOT17 dataset4 includes the same videos as MOT16, but with more accurate ground truth and with three sets of detections for each video: one from Faster R-CNN [4], one from DPM and one from the Scale-Dependent Pooling detector (SDP) [27]. The trackers would then have to prove to be versatile and robust enough to get a good performance using different detection qualities.

MOT16/17。 2016推出了一个新版本的数据集，称为 MOT163 [16]。 这一次，基准实况是从头开始的，因此它在整个数据集中是一致的。 这些视频也更具挑战性，因为它们的行人密度更高。 该集合中总共包含 14 个视频(7 个用于训练，7 个用于测试)，使用基于可变形部件的模型 (DPM) v5 [25、26] 获得公共检测，他们发现在检测方面获得更好的性能 与其他模型相比，数据集上的行人。 这次数据集包括 11235 帧，1342 个身份和总共 292733 个框。 MOT17 数据集 4 包含与 MOT16 相同的视频，但具有更准确的地面真实性和每个视频的三组检测：一组来自 Faster R-CNN [4]，一组来自 DPM，一组来自尺度相关池化检测器(SDP) ) [27]。 然后必须证明跟踪器具有足够的通用性和稳健性，以便使用不同的检测质量获得良好的性能。

#### MOT19. 
Very recently, a new version of the dataset for the CVPR 2019 Tracking Challenge5 has been released, containing 8 videos (4 for training, 4 for testing) with extremely high pedestrian density, reaching up to 245 pedestrians per frame on average in the most crowded video. The dataset contains 13410 frames with 6869 tracks and a total of 2259143 boxes, much more than the previous datasets. While submissions for this dataset have only been allowed for a limited amount of time, this data will be the basis for the release of MOT19 in late 2019 [28].

MOT19。 最近，CVPR 2019 Tracking Challenge5 的新版数据集已经发布，包含 8 个行人密度极高的视频(4 个用于训练，4 个用于测试)，在最拥挤的地方平均每帧达到 245 名行人 视频。 该数据集包含 13410 帧和 6869 条轨道，总共 2259143 个框，比之前的数据集多得多。 虽然只允许在有限的时间内提交此数据集，但此数据将成为 2019底发布 MOT19 的基础 [28]。

#### KITTI. 
While the MOTChallenge datasets focus on pedestrian tracking, the KITTI tracking benchmark6 [29, 30] allows for tracking of both people and vehicles. The dataset was collected by driving a car around a city and it was released in 2012. It consists of 21 training videos and 29 test ones, with a total of about 19000 frames (32 minutes). It includes detections obtained using the DPM7 and RegionLets8 [31] detectors, as well as stereo and laser information; however, as explained, in this survey we are only going to focus on models using 2D images. The CLEAR MOT metrics, MT, ML, ID switches and fragmentations are used to evaluate the methods. It is possible to submit results only for pedestrians or only for cars, and two different leaderboards are maintained for the two classes.

KITTI。 虽然 MOTChallenge 数据集侧重于行人跟踪，但 KITTI 跟踪基准 6 [29、30] 允许跟踪人和车辆。 该数据集是在2012年发布的，通过驾驶一辆汽车绕着一个城市收集。它由21个训练视频和29个测试视频组成，总共约19000帧(32分钟)。 它包括使用 DPM7 和 RegionLets8 [31] 检测器获得的检测，以及立体声和激光信息;  然而，如前所述，在本次调查中，我们将只关注使用 2D 图像的模型。 CLEAR MOT 指标、MT、ML、ID 开关和碎片用于评估这些方法。 可以只为行人或汽车提交结果，并且为这两个类别维护两个不同的排行榜。

#### Other datasets. 
Besides the previously described datasets, there is a number of older, and now less frequently used, ones. Among those we can find the UA-DETRAC tracking benchmark9 [32], that focuses on vehicles tracked from traffic cameras, and the TUD10 [33] and PETS200911 [34] datasets, that both focus on pedestrians. Many of their videos are now part of the MOTChallenge datasets. 

其他数据集。 除了前面描述的数据集外，还有许多较旧且现在不常使用的数据集。 其中我们可以找到 UA-DETRAC 跟踪基准9 [32]，它侧重于交通摄像头跟踪的车辆，以及 TUD10 [33] 和 PETS200911 [34] 数据集，它们都侧重于行人。 他们的许多视频现在都是 MOTChallenge 数据集的一部分。

2 Dataset: https://motchallenge.net/data/2D_MOT_2015/, leaderboard: https://motchallenge.net/results/2D_MOT_2015/. 
3 Dataset: https://motchallenge.net/data/MOT16/, leaderboard: https://motchallenge.net/results/MOT16/. 
4 Dataset: https://motchallenge.net/data/MOT17/, leaderboard: https://motchallenge.net/results/MOT17/. 
5 https://motchallenge.net/workshops/bmtt2019/tracking.html 
6 http://www.cvlibs.net/datasets/kitti/eval_tracking.php 
7 The website says the detections were obtained using a model based on a latent SVM, or L-SVM. That model is now known as Deformable Parts Model (DPM). 
8 http://www.xiaoyumu.com/project/detection 
9 https://detrac-db.rit.albany.edu/Tracking 
10 https://www.d2.mpi-inf.mpg.de/node/428 
11 http://www.cvg.reading.ac.uk/PETS2009/a.html

## 3 Deep learning in MOT
As this survey focuses on the use of deep learning in the MOT task, we organize this section into five subsections. Each of the first four subsections provides a review on how deep learning is exploited in each one of the four MOT stages defined previously12. Subsection 3.4, besides presenting the use of deep learning in the association process, will also include its use in the overall track management process (e.g. initialization/termination of tracks), since it is strictly linked to the association step. Subsection 3.5 will finally describe uses of deep learning in MOT that do not fit into the four-step scheme.

由于本次调查的重点是深度学习在 MOT 任务中的使用，因此我们将本节分为五个小节。 前四个小节中的每个小节都回顾了深度学习是如何在前面定义的四个 MOT 阶段中的每个阶段中被利用的。 3.4 小节除了介绍深度学习在关联过程中的使用外，还将包括它在整个轨道管理过程中的使用(例如轨道的初始化/终止)，因为它与关联步骤严格相关。 3.5 小节将最终描述不适合四步方案的深度学习在 MOT 中的使用。

We have included a summary table in A that shows the main techniques used in each of the four steps in each paper presented in this survey. The mode of operation (batch vs. online) is indicated and a link to the source code or to other provided material is also included (when available).

我们在 A 中包含了一个汇总表，显示了本次调查中每篇论文的四个步骤中的每一个步骤中使用的主要技术。 指出了操作模式(批处理与在线)，并且还包括指向源代码或其他提供材料的链接(如果可用)。

### 3.1 DL in detection step
While many works have used as input to their algorithms dataset-provided detections generated by various detectors (for example Aggregated Channel Features [24] for MOT15 [15] or Deformable Parts Model [25] for MOT16 [16]), there have also been algorithms that integrated a custom detection step, that often contributed to improve the overall tracking performance by enhancing the detection quality.

虽然许多工作已将各种检测器生成的由数据集提供的检测作为其算法的输入(例如 MOT15 [15] 的聚合通道特征 [24] 或 MOT16 [16] 的可变形部件模型 [25])，但也有 集成自定义检测步骤的算法，通常有助于通过提高检测质量来提高整体跟踪性能。

As we will see, most of the algorithms that employed custom detections made use of Faster R-CNN and its variants (section 3.1.1) or SSD (section 3.1.2), but approaches that used different models also exist (section 3.1.3). Despite the vast majority of algorithms utilized deep learning models to extract rectangular bounding boxes, a few works made a different use of deep networks in the detection step: these works are the focus of section 3.1.4.

正如我们将看到的，大多数采用自定义检测的算法都使用了 Faster R-CNN 及其变体(第 3.1.1 节)或 SSD(第 3.1.2 节)，但也存在使用不同模型的方法(第 3.1.1 节)。 尽管绝大多数算法使用深度学习模型来提取矩形边框，但一些工作在检测步骤中对深度网络进行了不同的使用：这些工作是第 3.1.4 节的重点。

#### 3.1.1 Faster R-CNN
Figure 3: Example of a deep learning based detector (Faster R-CNN architecture [4])
图 3：基于深度学习的检测器样本(Faster R-CNN 架构 [4])

The Simple Online and Realtime Tracking (SORT) algorithm [35] has been one of the first MOT pipelines to leverage convolutional neural networks for the detection of pedestrians. Bewley et al. showed that replacing detections obtained using Aggregated Channel Features (ACF) [24] with detections computed by Faster R-CNN [4] (illustrated in figure 3) could improve the MOTA score by 18.9% (absolute change) on the MOT15 dataset [15]. They used a relatively simple approach that consisted in predicting object motion using the Kalman filter [36] and then associating the detections together with the help of the Hungarian algorithm [37], using intersection-over-union (IoU) distances to compute the cost matrix. At the time of publishing, SORT was ranked as the best-performing open source algorithm on the MOT15 dataset. 

简单在线和实时跟踪 (SORT) 算法 [35] 是首批利用卷积神经网络检测行人的 MOT 管道之一。 Bewley et al. 表明，将使用聚合通道特征 (ACF) [24] 获得的检测替换为 Faster R-CNN [4] 计算的检测(如图 3 所示)可以将 MOT15 数据集上的 MOTA 分数提高 18.9%(绝对变化)[15]. 他们使用了一种相对简单的方法，包括使用卡尔曼滤波器 [36] 预测物体运动，然后在匈牙利算法 [37] 的帮助下将检测结果关联起来，使用联合交集(IoU)距离来计算成本 矩阵。 在发布时，SORT 被评为 MOT15 数据集上性能最好的开源算法。<!-- 卡尔曼滤波器, 匈牙利算法 ?-->

12Note that the classification of the models should not be considered as a strict categorization, since it’s not rare that one of them has been used for multiple purposes and drawing a line is sometimes difficult. For example, some deep learning models, Siamese networks in particular, are often trained to output an affinity score, but at inference time they are only used to extract ‘association features’, and a simple hardcoded distance measure is then used instead to compute the affinities. In those cases, we decided to consider the network as performing feature extraction, since the similarity measure is not directly learned. However, those models could have also been considered to use deep learning for affinity computation. 

12 请注意，模型的分类不应被视为严格的分类，因为其中一个被用于多种用途的情况并不少见，有时很难划清界限。 例如，一些深度学习模型，特别是孪生网络，通常被训练为输出亲和力分数，但在推理时它们仅用于提取“关联特征”，然后使用简单的硬编码距离度量来计算 亲和力。 在这些情况下，我们决定将网络视为执行特征提取，因为相似性度量不是直接学习的。 然而，这些模型也可以考虑使用深度学习进行亲和力计算。

Yu et al. reached the same conclusions in [38] using a modified Faster R-CNN, that included skip-pooling [39] and multi-region features [40] and that was fine-tuned on multiple pedestrian detection datasets. With this architecture they were able to improve the performance of the algorithm they proposed (see section 3.2.2) by more than 30% (absolute change, measured in MOTA), reaching state-of-the-art performance on the MOT16 dataset [16]. They also showed that having higher-quality detections reduces the need of complex tracking algorithms while still obtaining similar results: this is because the MOTA score is heavily influenced by the amount of false positives and false negatives, and using accurate detections is an effective way of reducing both. The detections computed by [38] on the MOT16 dataset have also been made available to the public(13) and many MOT algorithms have since exploited them [41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51].

Yu et al. [38] 中使用改进的 Faster R-CNN 得出了相同的结论，其中包括跳跃池 [39] 和多区域特征 [40]，并且在多个行人检测数据集上进行了微调。 通过这种架构，他们能够将他们提出的算法的性能(参见第 3.2.2 节)提高 30% 以上(绝对变化，以 MOTA 衡量)，在 MOT16 数据集上达到最先进的性能 [ 16]。 他们还表明，拥有更高质量的检测可以减少对复杂跟踪算法的需求，同时仍能获得相似的结果：这是因为 MOTA 分数受误报和漏报数量的影响很大，使用准确的检测是一种有效的方法 减少两者。 [38] 在 MOT16 数据集上计算的检测结果也已向公众公开(13)，此后许多 MOT算法 都利用了它们 [41、42、43、44、45、46、47、48、49、50、51] .

In the following years, other works have taken advantage of the detection accuracy of Faster R-CNN, that has since been applied as part of MOT algorithms to detect athletes [52], cells [53] and pigs [54]. Moreover, an adaptation of Faster R-CNN that adds a segmentation branch, Mask R-CNN [17], has been used for example by Zhou et al. [55] both to detect and to track pedestrians.

在接下来的几年中，其他工作利用了 Faster R-CNN 的检测准确性，此后已作为 MOT算法 的一部分应用于检测运动员 [52]、单元 [53] 和猪 [54]。 此外，Zhou et al. 已经使用了一种 Faster R-CNN 的改编版，它添加了一个分割分支 Mask R-CNN [17]。 [55] 检测和跟踪行人.

#### 3.1.2 SSD
The SSD [5] detector is another commonly used network in the detection step. In particular, Zhang et al. [54] compared it with Faster R-CNN and R-FCN [18] in their pig tracking pipeline, showing that it worked better on their dataset. They employed a Discriminative Correlation Filters (DCF) based online tracking method [56] with the use of HOG [57] and Colour Names [58] features to predict the position of so-called tag-boxes, small regions around the center of each animal. The Hungarian algorithm was used for the association between tracked tag-boxes and detections, and in the case of tracking failure the output of the DCF tracker was used to refine the bounding boxes. Lu et al. [59] also used SSD, but in this case to detect a variety of object classes to track (people, animals, cars, etc., see section 3.2.4).

SSD [5]检测器是检测步骤中另一个常用的网络。 特别是，Zhang et al. [54] 在他们的猪跟踪管道中将它与 Faster R-CNN 和 R-FCN [18] 进行了比较，表明它在他们的数据集上效果更好。 他们采用基于判别相关滤波器 (DCF) 的在线跟踪方法 [56]，使用 HOG [57] 和颜色名称 [58] 特征来预测所谓的标签框的位置，即每个标签框中心周围的小区域 动物。 匈牙利算法用于跟踪标记框和检测之间的关联，在跟踪失败的情况下，DCF 跟踪器的输出用于细化边框。 Lu et al. [59] 也使用了 SSD，但在这种情况下检测各种目标类进行跟踪(人、动物、汽车等，见 3.2.4 节)。

Some works have tried to refine the detections obtained with SSD by taking into account the information obtained in other steps of the tracking algorithm. Kieritz et al. [60], in their joint detection and tracking framework, used the affinity scores computed between tracks and detections to replace the standard Non-Maximum Suppression (NMS) step included in the SSD network with a version that refines detection confidence scores based on their correspondence to tracked targets.

一些工作试图通过考虑在跟踪算法的其他步骤中获得的信息来改进使用 SSD 获得的检测。 Kieritz et al. [60]，在他们的联合检测和跟踪框架中，使用跟踪和检测之间计算的亲和力分数来替换 SSD 网络中包含的标准非最大抑制(NMS)步骤，该步骤根据它们的对应关系改进检测置信度分数 跟踪目标。

Zhao et al. [61] instead employed the SSD detector to search for pedestrians and vehicles in a scene, but they used a CNN-based Correlation Filter (CCF) to allow SSD to generate more accurate bounding boxes. The CCF exploited PCA-compressed [62] CNN features to predict the position of a target in the subsequent frame; the predicted position was then used to crop a ROI (Region Of Interest) around it, that was given as input to SSD. In that way, the network was able to compute small detections using deeper layers, that extract more valuable semantic information and that are thus known to produce more accurate bounding boxes and less false negatives. The algorithm then combined these detections with the ones obtained on the full image with a NMS step and then association between tracks and detections was performed using the Hungarian algorithm, with a cost matrix that took into account geometric (IoU) and appearance (Average Peak-to-Correlation Energy - APCE [63]) cues. APCE was also used for an object re-identification (ReID) step, to recover from occlusions. The authors showed that training a detector with multi-scale augmentation could lead to much better performance in tracking and the algorithm reached accuracy comparable to state-of-the-art online algorithms on KITTI and MOT15.

Zhao et al. [61] 改为使用 SSD 检测器来搜索场景中的行人和车辆，但他们使用基于 CNN 的相关滤波器 (CCF) 来允许 SSD 生成更准确的边框。 CCF 利用 PCA 压缩的 [62] CNN 特征来预测目标在后续帧中的位置;  然后使用预测的位置在其周围裁剪 ROI(感兴趣区域)，并将其作为 SSD 的输入。 通过这种方式，网络能够使用更深层来计算小的检测，从而提取更有价值的语义信息，从而产生更准确的边框和更少的漏报。 然后该算法将这些检测与通过 NMS 步骤在完整图像上获得的检测相结合，然后使用匈牙利算法执行轨迹和检测之间的关联，成本矩阵考虑了几何(IoU)和外观(平均峰值- to-Correlation Energy - APCE [63]) 线索。 APCE 还用于目标重新识别 (ReID) 步骤，以从遮挡中恢复。 作者表明，训练具有多尺度增强的检测器可以带来更好的跟踪性能，并且该算法达到的精度可与 KITTI 和 MOT15 上最先进的在线算法相媲美。

#### 3.1.3 Other detectors
Among the other CNN models used as detectors in MOT, we can mention the YOLO series of detectors [64, 6, 65]; in particular, YOLOv2 has been used by Kim et al. [66] also to detect pedestrians. Sharma et al. [67] used instead a Recurrent Rolling Convolution (RRC) CNN [68] and a SubCNN [69] to detect vehicles in videos recorded on a moving camera in the context of autonomous driving (see section 3.2.4). Pernici et al. [70] used the Tiny CNN detector [71] in their face tracking algorithm, obtaining a better performance when compared to the Deformable Parts Model detector (DPM) [25], that does not use deep learning techniques.

在 MOT 中用作检测器的其他 CNN 模型中，我们可以提到 YOLO 系列检测器 [64、6、65];  特别是，YOLOv2 已被 Kim et al. [66] 使用来检测行人。 Sharma et al. [67] 改为使用循环滚动卷积 (RRC) CNN [68] 和 SubCNN [69] 来检测在自动驾驶的情况下记录在移动摄像机上的视频中的车辆(参见第 3.2.4 节)。 Pernici et al. [70] 在他们的面部跟踪算法中使用了 Tiny CNN 检测器 [71]，与不使用深度学习技术的可变形部件模型检测器 (DPM) [25] 相比，获得了更好的性能。

#### 3.1.4 Other uses of CNNs in the detection step
Sometimes CNNs have been employed in the MOT detection step for uses other than directly computing object bounding boxes.

有时 CNN 已被用于 MOT 检测步骤，而不是直接计算目标边框。

For example, CNNs have been exploited to reduce false positives in [72], where vehicle detections were obtained with a modified version of the ViBe algorithm [73] that performed background subtraction on the input. These detections were first given as input to a SVM [74] and, in case the SVM was not confident enough to either discard or confirm them, a Faster-CNN based network [75] would then be used to decide whether to keep or discard each of them. In this way, only a few objects would have to be analyzed by the CNN, making the detection step faster.

例如，CNN 已被用来减少 [72] 中的误报，其中车辆检测是通过对输入执行背景减法的 ViBe 算法 [73] 的修改版本获得的。 这些检测首先作为 SVM [74] 的输入提供，如果 SVM 没有足够的信心丢弃或确认它们，则将使用基于 Faster-CNN 的网络 [75] 来决定是保留还是丢弃 他们每个人。 这样，CNN 只需分析少数目标，从而使检测步骤更快。

13https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view 


Bullinger et al. explored a different approach in [76], where instead of computing classical bounding boxes in the detection step, a Multi-task Network Cascade [77] was instead employed to obtain instance-aware semantic segmentation maps. The authors argue that since the 2D shape of instances, differently from rectangular bounding boxes, do not contain background structures or parts of other objects, optical flow based tracking algorithms would perform better, especially when the target position in the image is also subject to camera motion in addition to the object’s own motion. After obtaining the segmentation maps for the various instances present in the current frame, an optical flow method ([78, 79, 80]) was applied to predict the position and shape of each instance in the next frame. An affinity matrix between predicted and detected instances was then computed and given as input to the Hungarian algorithm for association. While the method obtained slightly lower MOTA score on the whole MOT15 dataset when compared to SORT, the authors showed that it performed better on videos with moving camera.

Bullinger et al. [76] 中探索了一种不同的方法，其中不是在检测步骤中计算经典边框，而是采用多任务网络级联 [77] 来获得实例感知语义分割图。 作者认为，由于实例的 2D 形状与矩形边框不同，不包含背景结构或其他物体的一部分，因此基于光流的跟踪算法会表现更好，尤其是当图像中的目标位置也受相机影响时 除了目标自身的运动之外的运动。 在获得当前帧中存在的各种实例的分割图后，应用光流法([78、79、80])来预测下一帧中每个实例的位置和形状。 然后计算预测实例和检测实例之间的亲和力矩阵，并将其作为匈牙利算法的输入以进行关联。 虽然与 SORT 相比，该方法在整个 MOT15 数据集上获得的 MOTA 分数略低，但作者表明它在移动摄像机的视频上表现更好。 <!--基于光流(optical flow)的跟踪-->

### 3.2 DL in feature extraction and motion prediction  特征提取与运动预测
The feature extraction phase is the preferred one for the employment of deep learning models, due to their strong representational power that makes them good at extracting meaningful high-level features. The most typical approach in this area is the use of CNNs to extract visual features, as it is commented in section 3.2.2. Instead of using classical CNN models, another recurrent idea consists in training them as Siamese CNNs, using contrastive loss functions, in order to find the set of features that best distinguish between subjects. Those approaches are explained in section 3.2.3. Furthermore, some authors explored the capabilities of CNNs to predict object motion inside correlation filter based algorithms: these are commented in section 3.2.5. Finally, other types of deep learning models have been employed, usually including them in more complex systems, combining deep features with classical ones. They are explained in sections 3.2.4 (specifically for visual features) and 3.2.6 (for approaches that don’t fit in the other categories).

特征提取阶段是使用深度学习模型的首选阶段，因为它们具有强大的表示能力，这使得它们擅长提取有意义的高级特征。 该领域最典型的方法是使用 CNN 提取视觉特征，如 3.2.2 节所述。 另一个反复出现的想法是使用对比损失函数将它们训练为 孪生CNN，而不是使用经典的 CNN 模型，以便找到最能区分主体的特征集。 这些方法在 3.2.3 节中进行了解释。 此外，一些作者探索了 CNN 在基于相关滤波器的算法中预测目标运动的能力：这些在第 3.2.5 节中有评论。 最后，还采用了其他类型的深度学习模型，通常将它们包含在更复杂的系统中，将深度特征与经典特征相结合。 它们在第 3.2.4 节(专门针对视觉特征)和第 3.2.6 节(针对不属于其他类别的方法)中进行了解释。

#### 3.2.1 Autoencoders: first usage of DL in a MOT pipeline
To the best of our knowledge, the first approach using deep learning in MOT was presented by Wang et al. [81] in 2014. They proposed a network of autoencoders stacked in two layers that were used to refine visual features extracted from natural scenes [82]. After the extraction step, affinity computation was performed using a SVM, and the association task was formulated as a minimum spanning tree problem. They showed that feature refinement greatly improved the model performance. However, the dataset on which the algorithm was tested is not commonly used and results are hardly comparable to other methods.

据我们所知，Wang et al.[81] 在 2014提出了第一种在 MOT 中使用深度学习的方法。他们提出了一个自动编码器网络，堆叠成两层，用于改进从自然场景中提取的视觉特征 [82]。 在提取步骤之后，使用 SVM 执行亲和力计算，并将关联任务表述为最小生成树问题。 他们表明，特征细化极大地提高了模型性能。 然而，测试该算法的数据集并不常用，结果很难与其他方法进行比较。

#### 3.2.2 CNNs as visual feature extractors
The most widely used methods for feature extraction are based on subtle modifications of convolutional neural networks. One of the first uses of these models can be found in [83]. Here, Kim et al. incorporated visual features into a classical algorithm, called Multiple Hypothesis Tracking, using a pretrained CNN that extracted 4096 visual features from the detections, that were later reduced to 256 using PCA. This modification improved the MOTA score on MOT15 by more than 3 points. By the time that paper was submitted, it was the top ranked algorithm on that dataset. Yu el al. [38] used a modified version of GoogLeNet [2], pretrained on a custom re-identification dataset, built by combining classical person identification datasets (PRW [84], Market-1501 [85], VIPeR [86], CUHK03 [87]). Visual features were combined with spatial ones, extracted with a Kalman filter, and then an affinity matrix was computed.

最广泛使用的特征提取方法是基于对卷积神经网络的细微修改。 这些模型的第一个用途之一可以在 [83] 中找到。 在这里，Kim et al. 将视觉特征合并到称为多假设跟踪的经典算法中，使用预训练的 CNN 从检测中提取 4096 个视觉特征，后来使用 PCA 将其减少到 256 个。 此修改将 MOT15 上的 MOTA 分数提高了 3 分以上。 到那篇论文提交时，它是该数据集上排名最高的算法。 Yu el al. [38] 使用了 GoogLeNet [2] 的修改版本，在自定义重新识别数据集上进行了预训练，通过结合经典人物识别数据集(PRW [84]、Market-1501 [85]、VIPeR [86]、CUHK03 [87]). 视觉特征与空间特征相结合，用卡尔曼滤波器提取，然后计算亲和矩阵。

Other examples of the use of CNNs for feature extraction can be found in [88], where a custom CNN was used to extract appearance features in a Multiple Hypothesis Tracking framework, in [89], whose tracker employed a pretrained region-based CNN [90], or in [91], where a CNN extracted visual features from fish heads, later combined with motion prediction from a Kalman Filter.

在 [88] 中可以找到使用 CNN 进行特征提取的其他样本，其中使用自定义 CNN 在多假设跟踪框架中提取外观特征，在 [89] 中，其跟踪器采用了预训练的基于区域的 CNN [90] 或 [91]，其中 CNN 从鱼头?中提取视觉特征，然后结合卡尔曼滤波器的运动预测。

The SORT algorithm [35], presented in section 3.1.1, was later refined with deep features, and this new version was called DeepSORT [41]. This model incorporated visual information extracted by a custom residual CNN [92]. The CNN provided a normalized vector with 128 features as output, and the cosine distance between those vectors was added to the affinity scores used in SORT. A diagram of the network structure can be found in figure 4. The experiments showed that this modification overcame the main drawback of the SORT algorithm, which was a high number of ID switches.

SORT[35] 算法，在第 3.1.1 节中介绍，后来用深度特征进行了改进，这个新版本被称为 DeepSORT [41]。 该模型结合了由自定义残差 CNN [92] 提取的视觉信息。 CNN 提供了一个具有 128 个特征的归一化向量作为输出，这些向量之间的余弦距离被添加到 SORT 中使用的亲和力分数中。 网络结构图如图 4 所示。实验表明，这种修改克服了 SORT 算法的主要缺点，即大量的 ID 切换。

Mahmoudi et al. [42] also incorporated CNN extracted visual features along with dynamic and position features, and then solved the association problem via Hungarian algorithm. In [93], a ResNet-50 [3] pretrained on ImageNet was used as visual feature extractor. An extensive explanation of how a CNN can be used to distinguish pedestrians can be found in [94]. In their model, Bae et al. combined the output of the CNN with shape and motion models, and computed an aggregated affinity score for each pair of detections; the association problem was then solved by the Hungarian algorithm. Again, Ullah et al. [95] applied an off-the-shelf version of GoogLeNet [2] for feature extraction. Fang et al. [96] selected as visual features the output of a hidden convolutional layer of an Inception CNN [97]. Fu et al. [98] employed the DeepSORT feature extractor, and measured the correlation of features using a discriminative correlation filter. Afterwards, the matching score was combined with a spatio-temporal relation score, and the final score was used as a likelihood in a Gaussian Mixture Probability Hypothesis Density filter [99]. The authors in [100] used a fine-tuned GoogLeNet on the ILSVRC CLS-LOC [101] dataset for pedestrians recognition. In [70], the authors reused the visual features extracted by the CNN-based detector, and the association was performed using a Reverse Nearest Neighbor technique [102]. Sheng et al. [103] employed the convolutional part of GoogLeNet to extract appearance features, using the cosine distance between them to compute an affinity score between pairs of detections, and merging that information with motion prediction in order to compute an overall affinity which serves as edge cost in a graph problem. Chen et al. [104] utilized the convolutional part of ResNet to build a custom model, stacking a LSTM cell on top of the convolutions, in order to compute simultaneously a similarity score and a bounding box regression.

Mahmoudi et al. [42] 还结合了 CNN 提取的视觉特征以及动态和位置特征，然后通过匈牙利算法解决了关联问题。 在 [93] 中，使用在 ImageNet 上预训练的 ResNet-50 [3] 作为视觉特征提取器。 在 [94] 中可以找到关于如何使用 CNN 来区分行人的广泛解释。 在他们的模型中，Bae et al. 将 CNN 的输出与形状和运动模型相结合，并计算每对检测的聚合亲和力分数;  然后通过匈牙利算法解决了关联问题。 同样，Ullah et al. [95] 应用现成版本的 GoogLeNet [2] 进行特征提取。 Fang et al. [96] 选择 Inception CNN [97] 的隐藏卷积层的输出作为视觉特征。 Fu et al. [98] 采用 DeepSORT 特征提取器，并使用判别相关过滤器测量特征的相关性。 之后，匹配分数与时空关系分数相结合，最终分数被用作高斯混合概率假设密度过滤器中的可能性 [99]。 [100] 中的作者在 ILSVRC CLS-LOC [101] 数据集上使用微调的 GoogLeNet 进行行人识别。 在 [70] 中，作者重新使用了基于 CNN 的检测器提取的视觉特征，并使用反向最近邻技术 [102] 进行关联。 Sheng et al. [103] 使用 GoogLeNet 的卷积部分来提取外观特征，使用它们之间的余弦距离来计算检测对之间的亲和力得分，并将该信息与运动预测合并以计算整体亲和力，作为边缘成本 图形问题。 Chen et al. [104] 利用 ResNet 的卷积部分构建自定义模型，将 LSTM 单元堆叠在卷积之上，以便同时计算相似性分数和边框回归。<!--反向最近邻技术-->

Figure 4: Diagram of DeepSORT [41] CNN-based feature extractor. The red blocks are simple convolutional layers, the yellow block is a max pooling layer, and the blue blocks are residual blocks, that are composed of three convolutional layers each [3]. The final green block represents a fully-connected layer with batch normalization and L2 normalization. The output size of each block is indicated in parentheses. 
图 4：DeepSORT [41] 基于 CNN 的特征提取器图。 红色块是简单的卷积层，黄色块是最大池化层，蓝色块是残差块，每个块由三个卷积层组成 [3]。 最后的绿色块表示具有批量归一化和 L2 归一化的全连接层。 每个块的输出大小在括号中指示。

In [53], the model learned to distinguish fast moving cells from slow moving cells. After the classification was computed, slow cells were associated using only motion features, since they were almost still, while fast cells were associated using both motion features and visual features extracted by a Fast R-CNN based on VGG-16 [1], specifically fine-tuned for the cell classification task. Moreover, the proposed model included a tracking optimization step, where false negatives and false positives were reduced by combining possible tracklets that were mistakenly interrupted.

在 [53] 中，该模型学会了区分快速移动单元和慢速移动单元。 在计算分类后，慢单元仅使用运动特征关联，因为它们几乎是静止的，而快单元则使用基于 VGG-16 [1] 的 Fast R-CNN 提取的运动特征和视觉特征关联，特别是 针对单元分类任务进行了微调。 此外，所提出的模型包括一个跟踪优化步骤，其中通过组合可能被错误中断的轨迹来减少误报和误报。

Ran et al. [52] proposed a combination of a classical CNN for visual features extraction and AlphaPose CNN for pose estimation. The output of these two networks was then fed into a LSTM model together with the tracklet information history to compute a similarity, as it is explained in section 3.3.1.

Ran et al. [52] 提出了用于视觉特征提取的经典 CNN 和用于姿态估计的 AlphaPose CNN 的组合。 然后将这两个网络的输出与 轨迹 信息历史一起馈入 LSTM 模型以计算相似度，如第 3.3.1 节中所述。

An interesting employment of CNNs in feature extraction can be found in [51]. The authors used a pose detector, called DeepCut [105], that was a modification of Fast R-CNN; its output consisted in score maps predicting the presence of fourteen body parts. These were combined with the cropped images of detected pedestrians and fed into a CNN. A more detailed explanation of the algorithm is available in section 3.3.6.

在 [51] 中可以找到 CNN 在特征提取中的一个有趣应用。 作者使用了一种名为 DeepCut [105] 的姿势检测器，它是 Fast R-CNN 的改进版;  它的输出包括预测 14 个身体部位存在的分数图。 这些与检测到的行人的裁剪图像相结合并输入 CNN。 3.3.6 节提供了算法的更详细说明。

#### 3.2.3 Siamese networks
Another recurrent idea is training CNNs with loss functions that combine information from different images, in order to learn the set of features that best differentiates examples of different objects. These networks are usually called Siamese networks (an example of the architecture is shown in figure 5). Kim et al. [106] proposed a Siamese network [107] which was trained using a contrastive loss. The network took two images, their IoU score and their area ratio as input, and produced a contrastive loss as output. After the net was trained, the layer that computed the contrastive loss was removed, and the last layer was used as a feature vector for the input image. The similarity score was later computed by combining the Euclidean distance between feature vectors, the IoU score and the area ratio between bounding boxes. The association step was solved using a custom greedy algorithm. Wang et al. [108] also proposed a Siamese network which took two image patches and computed a similarity score between them. The score at test time was computed comparing the visual features extracted by the network for the two images, and including temporally constrained information. The distance employed as similarity score was a Mahalanobis distance with a weight matrix, also learned by the model.

另一个反复出现的想法是使用结合来自不同图像的信息的损失函数来训练 CNN，以学习最能区分不同目标样本的一组特征。 这些网络通常称为孪生网络(架构样本如图 5 所示)。 Kim et al. [106] 提出了一个孪生网络 [107]，它使用对比损失进行训练。 该网络将两幅图像、它们的 IoU 分数和它们的面积比作为输入，并产生对比损失作为输出。 训练网络后，移除计算对比损失的层，最后一层用作输入图像的特征向量。 随后通过结合特征向量之间的欧氏距离、IoU 分数和边框之间的面积比来计算相似性分数。 使用自定义贪心算法解决了关联步骤。 Wang et al. [108] 还提出了一个孪生网络，它采用两个图像块并计算它们之间的相似性得分。 测试时的分数是通过比较网络为两幅图像提取的视觉特征计算得出的，包括时间约束信息。 用作相似性得分的距离是具有权重矩阵的马氏距离，该距离也由模型学习。

Figure 5: Example of a Siamese CNN architecture. For feature extraction, the network is trained as a Siamese CNN, but at inference time the output probability is discarded, and the last fully connected layer is used as feature vector for a single candidate. When the network is used for affinity computation, the whole structure is preserved during inference.
图5: 孪生CNN 架构样本。 对于特征提取，网络被训练为 孪生CNN，但在推理时丢弃输出概率，最后一个全连接层用作单个候选者的特征向量。 当网络用于亲和力计算时，整个结构在推理过程中被保留。

Zhang et al. [109] proposed a loss function called SymTriplet loss. According to their explanation, during the training phase three CNNs with shared weights were used, and the loss function combined the information extracted from two images belonging to the same object (positive pair) and from an image of a different one (two negative pairs). The SymTriplet loss decreased when the distance between the feature vectors of the positive pair was small, and increased when the negative pairs’ features were close. Optimizing that function resulted in very similar feature vectors for images of the same object, while producing different vectors for different objects, with a larger distance between them. The dataset on which the tracking algorithm was tested was made of chapters from TV series and music videos from YouTube. Since the videos included different shots, the problem was divided into two stages. First, data association between frames in the same shot were performed. The affinity score in that case was a combination between the Euclidean distance of the feature vectors from the detections, temporal and kinematic information. Afterwards, tracklets were linked across shots, using a Hierarchical Agglomerative Clustering algorithm working over the appearance features.

Zhang et al. [109] 提出了一种称为 SymTriplet 损失的损失函数。 根据他们的解释，在训练阶段使用了三个具有共享权重的 CNN，损失函数结合了从属于同一目标的两幅图像(正对)和不同图像(两个负对)中提取的信息 . 当正对的特征向量之间的距离较小时，SymTriplet 损失会减少，而当负对的特征相近时，损失会增加。 优化该函数导致同一目标的图像的特征向量非常相似，同时为不同的目标生成不同的向量，它们之间的距离更大。 测试跟踪算法的数据集由来自 YouTube 的电视剧和音乐视频的章节组成。 由于视频包含不同的样本，因此问题分为两个阶段。 首先，执行同一样本中帧之间的数据关联。 在那种情况下，亲和力分数是特征向量与检测、时间和运动学信息的欧几里得距离的组合。 之后，使用处理外观特征的分层凝聚聚类算法将 tracklets 跨样本链接起来。

Leal-Taixé et al. [110] proposed a Siamese CNN which received two stacked images as an input, and output the probability of both images belonging to the same person. They trained the network with this output so that it learned the most representative features to distinguish subjects. Afterwards, the output layer was removed and the features extracted by the last hidden layer were used as input for a Gradient Boosting model, together with contextual information, in order to get an affinity score between detections. Then, the association step was solved using Linear Programming [111].

Leal-Taixé et al. [110] 提出了一种 Siamese CNN，它接收两个堆叠图像作为输入，并输出两个图像属于同一个人的概率。 他们用这个输出训练网络，使其学习到最具代表性的特征来区分主题。 之后，输出层被移除，最后一个隐藏层提取的特征与上下文信息一起用作梯度提升模型的输入，以获得检测之间的亲和力分数。 然后，使用线性规划 [111] 解决关联步骤。

Son et al. [112] proposed a new CNN architecture, called Quad-CNN. This model received as input four image patches, where the first three of them were from the same person, but in increasing time order, and the last one from another person. The network was trained using a custom loss, combining information about temporal distances between detections, extracted visual features, and bounding box positions. During the test phase, the network took two detections, and predicted the probability that both detections belonged to the same person, using the learned embedding.

Son et al. [112] 提出了一种新的 CNN 架构，称为 Quad-CNN。 该模型接收四个图像块作为输入，其中前三个图像块来自同一个人，但时间顺序递增，最后一个图像块来自另一个人。 该网络使用自定义损失进行训练，结合了有关检测之间的时间距离、提取的视觉特征和边框位置的信息。 在测试阶段，网络进行了两次检测，并使用学习到的嵌入预测了两次检测属于同一个人的概率。

In [55] a Siamese network based on Mask R-CNN [17] was built. After the Mask R-CNN had produced the mask for each detection, three examples were fed into the shallow Siamese net, two from the same object (positive pair) and one from another object (negative pair), again, and a triplet loss was used for training. After the training phase, the output layer was removed, and a 128-d vector was extracted from the last hidden layer. The appearance similarity was then computed using the cosine distance. That similarity was further combined with a motion consistency, which consisted on a score based on the predicted position of the object, assuming linear motion, and with a spatial potential, which was a more complex motion model. The association problem was then solved with a power iteration over a 3-d tensor of computed similarities.

在 [55] 中，建立了一个基于 Mask R-CNN [17] 的孪生网络。 在 Mask R-CNN 为每次检测生成掩码后，三个样本被送入浅孪生网络，两个来自同一目标(正对)，一个来自另一个目标(负对)，同样，三元组损失是 用于训练。 在训练阶段之后，输出层被移除，并从最后一个隐藏层中提取出一个 128 维的向量。 然后使用余弦距离计算外观相似度。 这种相似性进一步与运动一致性相结合，运动一致性包括基于物体预测位置的分数，假设线性运动，以及空间潜力，这是一个更复杂的运动模型。 然后通过对计算相似度的 3-d 张量进行幂迭代来解决关联问题。

Maksai et al. [113] directly used the 128-d feature vector extracted by the ReID triplet CNN proposed in [114], and combined it with other appearance-based features (as an alternative to an appearance-less version of the algorithm). Those features were further processed by a bidirectional LSTM. In [115] a similar approach was followed, with a so-called Spatial Attention Network (SAN). The SAN was a Siamese CNN, which used a pretrained ResNet-50 as base model. That net was truncated so that only the convolutional layers were employed. Then, a Spatial Attention Map was extracted from the last convolutional layers of the model: it represented a measure of the importance of different parts in the bounding box, in order to exclude background and other targets from the extracted features. The features were in fact weighted by this map, acting as a mask. The masked features from both detections were then merged into a fully connected layer which computed the similarity between them. During training, the network was also set to output a classification score, because the authors observed that jointly optimizing classification and affinity computation tasks resulted in a better performance in the latter. The affinity information was further fed into a bidirectional LSTM, as in the previous example. Both will be further discussed in section 3.3. Ma et al. [116] also trained a Siamese CNN in order to extract visual features from tracked pedestrians in their model, which is explained in detail in section 3.4.1.

Maksai et al. [113] 直接使用了 [114] 中提出的 ReID 三元组 CNN 提取的 128 维特征向量，并将其与其他基于外观的特征相结合(作为算法的无外观版本的替代方案)。 这些特征由双向 LSTM 进一步处理。 在 [115] 中采用了类似的方法，即所谓的空间注意网络(SAN)。 SAN 是 孪生CNN，它使用预训练的 ResNet-50 作为基础模型。 该网络被截断，因此仅使用了卷积层。 然后，从模型的最后一个卷积层中提取空间注意图：它表示边框中不同部分重要性的度量，以便从提取的特征中排除背景和其他目标。 这些特征实际上由这张地图加权，充当掩码。 然后将来自两个检测的掩码特征合并到一个全连接层中，该层计算它们之间的相似性。 在训练期间，网络还设置为输出分类分数，因为作者观察到联合优化分类和亲和力计算任务会导致后者的性能更好。 亲和力信息被进一步馈送到双向 LSTM 中，如前一个样本所示。 两者都将在 3.3 节中进一步讨论。 马et al. [116] 还训练了 孪生CNN，以便从他们模型中跟踪的行人中提取视觉特征，这在第 3.4.1 节中有详细解释。

In [117], Zhou et al. proposed a visual displacement CNN, which learned to predict the next position of an object depending on previous positions of the objects, and the influence that an object had over other objects in the scene. That CNN was then used to predict the location of objects in the next frame, taking as input their past trajectories. The network was also capable of extracting visual information from the predicted positions and the actual detections, in order to compute a similarity score, as it is explained in section 3.3.6.

在 [117] 中，Zhou et al. 提出了一种视觉位移 CNN，它学会了根据目标的先前位置以及目标对场景中其他目标的影响来预测目标的下一个位置。 然后，该 CNN 被用来预测下一帧中物体的位置，并将它们过去的轨迹作为输入。 该网络还能够从预测位置和实际检测中提取视觉信息，以计算相似度分数，如第 3.3.6 节所述。 <!--预测目标下一个位置！-->

Chen et al. [118] proposed a two-steps algorithm which employed GoogLeNet trained with triplet loss for feature extraction. In the first step, the model used a R-FCN to predict possible detection candidates using information from the existing tracklets. Then, those detections were combined with the actual detections and NMS was performed. Afterwards, using the customly trained GoogLeNet model, they extracted visual features from the detections, and solved the association problem with a hierarchical association algorithm. When their paper was published, the algorithm was ranked on top among online methods in the MOT16 dataset.

Chen et al. [118] 提出了一种两步算法，该算法采用经过三元组损失训练的 GoogLeNet 进行特征提取。 在第一步中，该模型使用 R-FCN 来使用来自现有轨迹的信息来预测可能的检测候选目标。 然后，将这些检测与实际检测相结合，并执行 NMS。 然后，使用自定义训练的 GoogLeNet 模型，他们从检测中提取视觉特征，并使用层次关联算法解决关联问题。 当他们的论文发表时，该算法在 MOT16 数据集的在线方法中名列前茅。

Lee et al. [119] recently explored an interesting approach, combining pyramid and Siamese networks together. Their model, called Feature Pyramid Siamese Network, employed a backbone network (they studied the performance using SqueezeNet [120] and GoogLeNet [2], but the backbone network can be changed), which extracted visual features from two different images using the same parameters. Afterwards, some of the hidden feature maps from the network were extracted and given to the Feature Pyramid Siamese Network. The network then employed an upsampling and merging strategy to create a feature vector for every stage of the pyramid. Deeper layers were merged with shallower ones in order to enrich the simpler features with more complex ones. Afterwards, affinity score computation was performed, as explained in section 3.3.7.

Lee et al. [119] 最近探索了一种有趣的方法，将金字塔和孪生网络结合在一起。 他们的模型称为特征金字塔孪生网络，采用主干网络(他们使用 SqueezeNet [120] 和 GoogLeNet [2] 研究了性能，但主干网络可以更改)，使用相同的参数从两个不同的图像中提取视觉特征 . 之后，从网络中提取了一些隐藏的特征图，并将其提供给特征金字塔孪生网络。 然后网络采用上采样和合并策略为金字塔的每个阶段创建一个特征向量。 较深的层与较浅的层合并，以便用更复杂的特征来丰富更简单的特征。 之后，如第 3.3.7 节所述，执行亲和力分数计算。

#### 3.2.4 More complex approaches for visual feature extraction
More complex approaches have also been proposed. Lu et al. [59] employed the class predicted by the SSD in the detection step as a feature, and combined it with an image descriptor extracted with RoI pooling for each detection. Afterwards, the extracted features were used as input for a LSTM network, which learned to compute association features for the detections. Those features were later used for affinity computation, using the cosine distance between them.

更复杂的方法。 Lu et al. [59] 在检测步骤中采用 SSD 预测的类别作为特征，并将其与每次检测中使用 RoI 池提取的图像描述符相结合。 之后，提取的特征被用作 LSTM 网络的输入，该网络学习计算检测的关联特征。 这些特征后来被用于亲和力计算，使用它们之间的余弦距离。

In [121], the shallower layers of GoogLeNet were employed to learn a dictionary of features of the tracked objects. In order to learn the dictionary, the algorithm randomly selected objects in the first 100 frames of the video. The model extracted the feature maps in the first seven layers of the network. Then a dimensionality reduction was performed using Orthogonal Matching Pursuit (OPM) [122] over the features extracted from the objects, and the learned representation was used as a dictionary. During the test phase, the OPM representation was computed for every detected object in the scene, and compared with the dictionary in order to construct a cost matrix, combining visual and motion information extracted by a Kalman filter. Finally, the association was performed using the Hungarian algorithm.

在 [121] 中，GoogLeNet 的较浅层被用来学习被跟踪目标的特征字典。 为了学习字典，算法随机选择视频前 100 帧中的目标。 该模型提取了网络前七层的特征图。 然后使用正交匹配追踪 (OPM) [122] 对从目标中提取的特征进行降维，并将学习到的表示用作字典。 在测试阶段，为场景中的每个检测到的目标计算 OPM 表示，并与字典进行比较以构建成本矩阵，结合卡尔曼滤波器提取的视觉和运动信息。 最后，使用匈牙利算法进行关联。

LSTMs are sometimes employed for motion prediction, in order to learn more complex, non-linear motion models from the data. In figure 6 a scheme of the typical use of LSTMs for motion prediction is shown. An example of this use of recurrent networks is shown by Sadeghian et al. [123], who proposed a model that employed three different RNNs to compute various types of features, not only motion ones, for each detection. The first RNN was employed to extract appearance features. The input of this RNN was a visual features vector extracted by a VGG CNN [1], pretrained specifically for person re-identification. The second RNN was a LSTM trained to predict the motion model for every tracked object. In this case, the output of the LSTM was the velocity vector of each object. The last RNN was trained to learn the interactions between different objects on the scene, since the position of some objects could be influenced by the behavior of surrounding items. The affinity computation was then performed by another LSTM, taking the information of the other RNNs as input. 

LSTM 有时用于运动预测，以便从数据中学习更复杂的非线性运动模型。 在图 6 中，显示了 LSTM 用于运动预测的典型使用方案。 Sadeghian et al. [123]展示了这种使用循环网络的样本，他提出了一个模型，该模型采用三种不同的 RNN 来为每次检测计算各种类型的特征，而不仅仅是运动特征。 第一个 RNN 用于提取外观特征。 此 RNN 的输入是由 VGG CNN [1] 提取的视觉特征向量，专门为行人重新识别进行了预训练。 第二个 RNN 是一个 LSTM，经过训练可以预测每个跟踪目标的运动模型。 在这种情况下，LSTM 的输出是每个目标的速度向量。 最后一个 RNN 被训练来学习场景中不同目标之间的交互，因为一些目标的位置可能会受到周围项目行为的影响。 然后由另一个 LSTM 执行亲和力计算，将其他 RNN 的信息作为输入。

Figure 6: Typical usage of LSTM for motion prediction. A group of bounding boxes are fed into the network, and the produced output is the predicted bounding box in the next frame 
图 6：LSTM 用于运动预测的典型用法。 一组边框被输入网络，产生的输出是下一帧的预测边框


In [124], a model of stacked CNNs was proposed. The first section of the model consisted of a pretrained shared CNN which extracted common features for every object in the scene. That CNN was not updated online. Then, a RoI pooling was applied and the RoI features for every candidate were extracted. Afterwards, for every tracked candidate a new specific CNN was instantiated and trained online. Those CNNs extracted both the visibility map and the spatial attention map for its candidate. Finally, after the refined features were extracted, the probability of each new image belonging to every already tracked object was computed, and the association step was finally performed using a greedy algorithm.

在 [124] 中，提出了一种堆叠 CNN 模型。 模型的第一部分包含一个预训练的共享 CNN，它为场景中的每个目标提取共同特征。 CNN 没有在线更新。 然后，应用 RoI 池化并提取每个候选目标的 RoI 特征。 之后，对于每个被跟踪的候选人，一个新的特定 CNN 被实例化并在线训练。 这些 CNN 为其候选目标提取了可见性图和空间注意力图。 最后，在提取细化特征后，计算每个新图像属于每个已跟踪目标的概率，最后使用贪心算法执行关联步骤。

Sharma et al. [67] designed a set of cost functions to compute similarity between detections of vehicles. Those costs combined appearance features, extracted by a CNN, with 3D shape and position features assuming an environment with a moving camera. The defined costs were a 3D-2D cost, were the estimated 3D projection of the bounding box on the previous frame was compared with the 2D bounding box on the new frame, a 3D-3D cost, were the 3D projection of the previous bounding box was overlapped with the 3D projection of the current bounding box, an appearance cost, were the euclidean distance of the extracted visual features was computed, and a shape and pose cost, were the rough shape and position of the object in the bounding boxes were computed and compared. Note that while 3D projections were inferred, the input was still 2D images. After every cost was computed, the final pairwise cost between detections in two subsequent frames was a linear combination of the former costs. The final association problem was solved using the Hungarian algorithm.

Sharma et al. [67] 设计了一组成本函数来计算车辆检测之间的相似性。 这些成本将 CNN 提取的外观特征与 3D 形状和位置特征相结合，假设环境中有移动相机。 定义的成本是 3D-2D 成本，是前一帧上边框的估计 3D 投影与新帧上的 2D 边框进行比较，3D-3D 成本是前一帧边框的 3D 投影 与当前边框的 3D 投影重叠，外观成本是计算提取的视觉特征的欧氏距离，形状和姿势成本是计算边框中目标的粗略形状和位置 并进行了比较。 请注意，虽然推断了 3D 投影，但输入仍然是 2D 图像。 在计算完每个成本后，两个后续帧中检测之间的最终成对成本是前一个成本的线性组合。 使用匈牙利算法解决了最终的关联问题。

Kim et al. [66] employed the information extracted by the YOLOv2 CNN object detector to build a random ferns classifier [125]. The algorithm worked in two steps. In the first step, a so-called teacher-RF was trained in order to differentiate pedestrians from non-pedestrians. After the teacher-RF was trained, for every tracked object, a random ferns classifier was constructed. Those classifiers were called student-RF, and they were smaller than the teacher-RF. They were specialized in distinguishing their tracked object from the rest of the objects in the scene. The decision of having a small random ferns classifier for every object was taken in order to reduce the computational complexity of the overall model, so that it could work in real time.

Kim et al. [66] 利用 YOLOv2 CNN 目标检测器提取的信息来构建随机蕨类植物分类器 [125]。 该算法分两步进行。 在第一步中，训练了所谓的 teacher-RF，以区分行人和非行人。 在训练了 teacher-RF 之后，对于每个被跟踪的目标，构建了一个随机蕨类植物分类器。 这些分类器称为学生 RF，它们比教师 RF 小。 他们专门将跟踪的目标与场景中的其他目标区分开来。 为每个目标设置一个小型随机蕨类植物分类器的决定是为了降低整个模型的计算复杂度，以便它可以实时工作。

In [126] the number of affinity computations that the model must compute was reduced by estimating first the position of objects in subsequent frames, using a Hidden Markov Model [127]. Then, the feature extraction was performed using a pretrained CNN. After the visual features were extracted, the affinity computation was only computed between feasible pairs, that is, between detections close enough to the HMM prediction to be considered as the same object. The affinity score was obtained using a mutual information function between visual features. When the affinity scores were computed, a dynamic programming algorithm was used to associate detections. 

在 [126] 中，通过使用隐马尔可夫模型 [127] 首先估计目标在后续帧中的位置，减少了模型必须计算的亲和力计算的数量。 然后，使用预训练的 CNN 进行特征提取。 提取视觉特征后，仅在可行对之间计算亲和度计算，即在足够接近 HMM 预测以被视为同一目标的检测之间。 使用视觉特征之间的互信息函数获得亲和力分数。 在计算亲和力分数时，使用动态编程算法来关联检测。

#### 3.2.5 CNNs for motion prediction: correlation filters
Wang et al. [128] studied the employment of a correlation filter [129], whose output is a response map for the tracked object. That map was an estimation of the new position of the object in the next frame. Such affinity was further combined with an optical flow affinity, computed using the Lucas-Kanade algorithm [130], a motion affinity calculated with a Kalman filter, and a scale affinity, represented by a ratio involving height and width of the bounding boxes. The affinity between two detections was computed as a linear combination of the previous scores. There was also a step of false detections removal, using a SVM classifier, and missing detections handling, using for that task the response map calculated in the previous steps. If an object was mistakenly lost and then re-identified, that step could fix the mistake and reconnect the broken tracklet.

Wang et al. [128] 研究了相关滤波器 [129] 的使用，其输出是被跟踪目标的响应图。 该图是对下一帧中目标新位置的估计。 这种亲和力进一步与使用 Lucas-Kanade 算法 [130] 计算的光流亲和力、使用卡尔曼滤波器计算的运动亲和力和尺度亲和力相结合，由涉及边框的高度和宽度的比率表示。 两个检测之间的亲和力被计算为先前分数的线性组合。 还有一个错误检测删除步骤，使用 SVM 分类器和缺失检测处理，该任务使用在前面步骤中计算的响应映射。 如果一个目标被错误地丢失然后重新识别，该步骤可以修复错误并重新连接损坏的 tracklet。

In [61], a correlation filter was also employed to predict the position of the object in subsequent frames. The filter received as input the appearance features extracted by a CNN, previously reduced using PCA, and produced a response map of the predicted position for the object in the next frame as output. The predicted position was later used to compute a similarity score, combining the IoU between prediction and detections, and the APCE score of the response map. After the cost matrix was constructed, computing said score for every pair of detections between frames, the assignment problem was solved using the Hungarian algorithm.

在[61]中，还采用了相关滤波器来预测目标在后续帧中的位置。 该过滤器接收由 CNN 提取的外观特征作为输入，之前使用 PCA 进行了缩减，并生成下一帧中目标预测位置的响应图作为输出。 预测位置后来用于计算相似性分数，结合预测和检测之间的 IoU，以及响应图的 APCE 分数。 构建成本矩阵后，计算帧间每对检测的得分，使用匈牙利算法解决分配问题。

#### 3.2.6 Other approaches
Rosello et al. [131] explored a completely different approach, using a reinforcement learning framework to train a set of agents that helped in the feature extraction step. The algorithm was based solely on motion features, without any visual information employed. The motion model was learned using a Kalman filter, whose behavior was managed by an agent, using one agent for each tracked object. The agent learned to decide which action should the Kalman filter take, between a set of actions that included ignoring the prediction, ignoring the new measure, using both information pieces, starting or stopping a track. The authors claimed that their algorithm could solve the tracking task even in non-visual scenarios, in contrast with classical algorithms whose performance was deeply influenced by visual features. However, the experimental results on MOT15 are not reliable and cannot be compared with other models because the model was tested on the training set.

Rosello et al. [131] 探索了一种完全不同的方法，使用强化学习框架来训练一组有助于特征提取步骤的智能体。 该算法完全基于运动特征，没有使用任何视觉信息。 运动模型是使用卡尔曼滤波器学习的，其行为由智能体管理，每个跟踪目标使用一个智能体。 智能体学会了决定卡尔曼滤波器应该采取哪些行动，在一组行动之间，包括忽略预测、忽略新措施、使用两个信息片段、开始或停止轨道。 作者声称他们的算法即使在非视觉场景下也可以解决跟踪任务，这与性能受视觉特征影响很深的经典算法形成了鲜明对比。 但是，MOT15 上的实验结果并不可靠，无法与其他模型进行比较，因为该模型是在训练集上进行测试的。

Another algorithm that relied solely on motion features was the one proposed in [132]. Babaee et al. presented a LSTM which learned to predict the new position and size of the bounding box for every object in the scene, using information about position and velocity in previous frames. Using the IoU between the predicted bounding box and the real detection, an affinity measure was computed, and the tracks were associated using a custom greedy algorithm. The pipeline was applied on tracking results obtained by other algorithms, in order to handle occlusions, and the authors showed that their method could effectively reduce the number of ID switches.

另一种完全依赖运动特征的算法是 [132] 中提出的算法。 Babaee et al. 提出了一个 LSTM，它学会了预测场景中每个目标的边框的新位置和大小，使用有关先前帧中的位置和速度的信息。 使用预测边框和实际检测之间的 IoU，计算亲和度度量，并使用自定义贪婪算法关联轨迹。 该管道应用于其他算法获得的跟踪结果，以处理遮挡，作者表明他们的方法可以有效减少 ID 切换的次数。

### 3.3 DL in affinity computation 亲和力计算中的深度学习
While many works compute affinity between tracklets and detections (or tracklets and other tracklets) by using some distance measure over features extracted by a CNN, there are also algorithms that use deep learning models to directly output an affinity score, without having to specify an explicit distance metric between the features. This section focuses on such works.

虽然许多工作通过对 CNN 提取的特征使用某种距离度量来计算 tracklets 和检测(或 tracklets 和其他 tracklets)之间的亲和力，但也有一些算法使用深度学习模型直接输出亲和力分数，而无需指定显式 特征之间的距离度量。 本节重点介绍此类工作。

In particular, we are first going to describe algorithms that used recurrent neural networks, starting from standard LSTMs (section 3.3.1) and then describing uses of Siamese LSTMs (section 3.3.2) and Bidirectional LSTMs (section 3.3.3). A particular use of LSTM-computed affinities in the context of Multiple Hypothesis Tracking (MHT) frameworks is presented in section 3.3.4; finally, a few works that employed different kinds of recurrent network for affinity computations are presented in section 3.3.5.

具体来说，我们将首先描述使用循环神经网络的算法，从标准 LSTM(第 3.3.1 节)开始，然后描述 Siamese LSTM(第 3.3.2 节)和双向 LSTM(第 3.3.3 节)的使用。 3.3.4 节介绍了在多重假设跟踪 (MHT) 框架的上下文中使用 LSTM 计算的亲和力;  最后，第 3.3.5 节介绍了一些采用不同类型的循环网络进行亲和力计算的工作。

In the second part of this section we are going to explore instead the uses of CNNs in affinity computation (section 3.3.6), including the algorithms that used the output of Siamese CNNs directly as an affinity score (section 3.3.7), instead of relying on distance measures over feature vectors like in section 3.2.3.

在本节的第二部分，我们将探索 CNN 在亲和力计算中的使用(第 3.3.6 节)，包括直接使用 Siamese CNN 的输出作为亲和力得分的算法(第 3.3.7 节)，而不是 像第 3.2.3 节那样依赖特征向量的距离度量。

#### 3.3.1 Recurrent neural networks and LSTMs
One of the first works to use a deep network to directly compute an affinity is [133], where Milan et al. proposed an end-to-end learning approach for online MOT, summarized in figure 7. A recurrent neural network (RNN) based model was used as the main tracker, mimicking a Bayesian filter algorithm, consisting of three blocks: the first was a motion prediction block, that learned a motion model that took as input the state of the target in the past frames (i.e. the old bounding box locations and sizes) and predicted the target state in the next frame without accounting for the detections; the second block refined the state prediction using the detections in the new frame and an association vector containing the probability of associating the target with all such detections (it is evident how this can be considered an affinity score); the third block managed the birth and death of tracks, as it used the previous collected information to predict the probability of existence of the track in the new frame14. The association vector was computed using a LSTM-based network, that used the Euclidean distance between the predicted state of the target and the states of the detections in the new frame as input features (besides the hidden state and the cell state, as any standard LSTM). The networks were trained separately using 100K 20-frame long synthetically generated sequences. While the algorithm performed favorably to other techniques, like the combination of a Kalman filter with the Hungarian algorithm, the results on the MOT15 test set did not quite reach top accuracy; however, the algorithm was able to run much faster than other algorithms (∼ 165 FPS) and did not use any kind of appearance features, leaving room for future improvements.

使用深度网络直接计算亲和力的首批工作之一是 [133]，其中 Milan et al. 提出了一种用于在线 MOT 的端到端学习方法，如图 7 所示。基于循环神经网络 (RNN) 的模型用作主要跟踪器，模仿贝叶斯过滤算法，由三个块组成：第一个是运动 预测块，它学习了一个运动模型，该模型将过去帧中的目标状态(即旧的边框位置和大小)作为输入，并在不考虑检测的情况下预测下一帧中的目标状态;  第二个块使用新帧中的检测和包含将目标与所有此类检测相关联的概率的关联向量来改进状态预测(很明显这可以被视为亲和力分数);  第三个块管理轨道的诞生和死亡，因为它使用以前收集的信息来预测轨道在新框架中存在的概率 14。 关联向量是使用基于 LSTM 的网络计算的，该网络使用目标的预测状态与新帧中的检测状态之间的欧几里德距离作为输入特征(除了隐藏状态和单元状态，作为任何标准 长短期记忆)。 这些网络分别使用 100K 20 帧长的合成生成序列进行训练。 虽然该算法的性能优于其他技术，例如卡尔曼滤波器与匈牙利算法的组合，但 MOT15 测试集上的结果并没有达到最高准确度;  然而，该算法能够比其他算法运行得更快(~ 165 FPS)并且没有使用任何类型的外观特征，为未来的改进留下了空间。

14To smooth the existence probability predictions and avoid deleting tracks of temporarily occluded objects, the difference between the new and the old existence probability was also output so that it could be minimized during training. 

14为了平滑存在概率预测并避免删除临时遮挡目标的轨迹，还输出了新旧存在概率之间的差异，以便在训练期间将其最小化。


Figure 7: Diagram of the MOT algorithm proposed by Milan et al. [133] employing a LSTM to predict detection associations. The algorithm used two different RNNs to solve them problem, each one specialized in one subtask. The LSTM (left) learned to associate detections with tracks given predicted positions. It received the pairwise-distance matrix between detections and predictions ($C_{t+1}$), the cell state ($c_i$) and the hidden state ($h_i$) as input, and output the vector Ait+1 representing the probability of associating target i with the detections in the frame. The RNN (right) was trained to predict the position of the targets in the new frame and the possible birth and death of new ones. It received as input the hidden state ($h_t$) and the current position of the target ($x_t$), outputting the predicted position and the new hidden state (blue box). After the associations from the LSTM were computed, using the detections($z_{t+1}$), the positions of targets were updated (green box), and the existence probability ε was computed to predict death and birth of trajectories (red box). 
图 7：Milan et al. 提出的 MOT算法 示意图。 [133] 使用 LSTM 来预测检测关联。 该算法使用两个不同的 RNN 来解决它们的问题，每个 RNN 专门负责一个子任务。 LSTM(左)学会了将检测与给定预测位置的轨迹相关联。 它接收检测和预测之间的成对距离矩阵 ($C_{t+1}$)、单元状态 ($c_i$) 和隐藏状态 ($h_i$) 作为输入，并输出表示目标 i 与检测相关联概率的向量 Ait+1 在框架中。 RNN(右)被训练来预测目标在新框架中的位置以及新目标可能的出生和死亡。 它接收隐藏状态 ($h_t$) 和目标的当前位置 ($x_t$) 作为输入，输出预测位置和新的隐藏状态(蓝色框)。 在计算 LSTM 的关联后，使用检测($z_{t+1}$)，更新目标的位置(绿色框)，并计算存在概率 ε 以预测轨迹的死亡和出生(红色框)。

Among the other works that later used LSTMs there is [123], that used a LSTM with a fully-connected (FC) layer to fuse features extracted by 3 other LSTMs (as already explained in section 3.2.4) and output an affinity score15. The overall algorithm is similar to the Markov Decision Processes (MDP) based framework presented in [134]: a single object tracker (SOT) is used to track targets independently; when a target gets occluded, the SOT is stopped and a bipartite graph is built that uses the affinities computed by the LSTM as edge costs and the association problem is then solved with the help of the Hungarian algorithm. The authors showed that using both a combination of all the 3 feature extractors and an LSTM rather than a plain FC layer led to consistently better performance on a MOT15 validation set. The algorithm also reached state-of-the-art MOTA scores on both MOT15 and MOT16 test sets at the time of publication, confirming the validity of the approach.

在后来使用 LSTM 的其他工作中有 [123]，它使用具有全连接 (FC) 层的 LSTM 来融合由其他 3 个 LSTM 提取的特征(如第 3.2.4 节所述)并输出亲和力分数 15 . 整体算法类似于[134]中提出的基于马尔可夫决策过程(MDP)的框架：单个目标跟踪器(SOT)用于独立跟踪目标;  当目标被遮挡时，SOT 停止并构建一个二分图，该图使用 LSTM 计算的亲和力作为边成本，然后借助匈牙利算法解决关联问题。 作者表明，使用所有 3 个特征提取器和 LSTM 的组合而不是普通的 FC 层可以在 MOT15 验证集上获得更好的性能。 该算法在发布时还在 MOT15 和 MOT16 测试集上达到了最先进的 MOTA 分数，证实了该方法的有效性。

15The paper seems to imply that while the LSTM is trained to predict an affinity score, only the affinity features are extracted and are then used to replace the handcrafted features used in the MDP paper. The algorithm presented in the MDP paper, though, adds on top of those features another FC layer, trained with reinforcement learning to classify the track/detection pair as belonging to the same identity or not. Thus we can consider the overall affinity computation as performed by a deep learning model. 

15该论文似乎暗示，虽然 LSTM 被训练来预测亲和力分数，但仅提取亲和力特征，然后用于替换 MDP 论文中使用的手工特征。 不过，MDP 论文中提出的算法在这些特征之上添加了另一个 FC 层，该层通过强化学习进行训练，以将跟踪/检测对分类为是否属于同一身份。 因此，我们可以考虑由深度学习模型执行的整体亲和力计算。

Another approach using multiple LSTMs is [52], where Ran et al. proposed a Pose-based Triple Stream Network, that computed an affinity combining 3 other affinities output by 3 LSTMs: one for appearance similarity, using CNN features and pose information extracted with AlphaPose [135], one for motion similarity, using pose joints velocity, and one for interaction similarity, using an interaction grid. A custom tracking algorithm is then used to associate the detections. The comparison with other state-of-the-art MOT algorithms on their proprietary Volleyball dataset for athlete tracking was favourable. 

另一种使用多个 LSTM 的方法是 [52]，其中 Ran et al. 提出了一种基于姿势的三流网络，它计算了结合 3 个 LSTM 输出的 3 个其他亲和力的亲和力：一个用于外观相似性，使用 CNN 特征和用 AlphaPose [135] 提取的姿势信息，一个用于运动相似性，使用姿势关节速度， 一个用于交互相似性，使用交互网格。 然后使用自定义跟踪算法来关联检测。 与其他最先进的 MOT算法 在他们用于运动员跟踪的专有排球数据集上的比较是有利的。


#### 3.3.2 Siamese LSTMs
Liang et al. [136] also used multiple LSTMs to model various features, but they proceeded in a different way. Since extracting appearance features with CNNs is computationally expensive, they proceeded with a so-called pre-association step, that used a SVM to predict the association probability between tracklets and detections. The SVM took as input position and velocity similarity scores, computed using two LSTMs for position and velocity prediction. The preassociation step then consisted in discarding the detections with low SVM affinity scores. After this step, an actual association step was performed by using VGG-16 features given as input to a Siamese LSTM, that predicted an affinity score between the tracklet and the detections. Association was performed in a greedy manner, associating the detection with the highest score to the tracklet. Testing was done on the MOT17 datasets, and the results were in line with the top-performing algorithms.

Liang et al. [136] 也使用多个 LSTM 对各种特征进行建模，但它们以不同的方式进行。 由于使用 CNN 提取外观特征的计算成本很高，因此他们进行了所谓的预关联步骤，该步骤使用 SVM 来预测轨迹和检测之间的关联概率。 SVM 将位置和速度相似度分数作为输入，使用两个 LSTM 计算用于位置和速度预测。 然后，预关联步骤包括丢弃具有低 SVM 亲和力分数的检测。 在此步骤之后，通过使用 VGG-16 特征作为 Siamese LSTM 的输入执行实际的关联步骤，该特征预测了 tracklet 和检测之间的亲和力分数。 关联以贪婪的方式进行，将得分最高的检测与轨迹关联起来。 在 MOT17 数据集上进行了测试，结果与性能最佳的算法一致。

Wan et al. [43] also used a Siamese LSTM in their algorithm, that was also composed of two steps. In the first step, short reliable tracklets were built by using Hungarian algorithm with affinity measures computed using the IoU between detections and the predicted target positions (obtained with Kalman filter or Lucas-Kanade optical flow). The second step also used the Hungarian algorithm to join the tracklets, but this time the affinity was computed using a Siamese LSTM framework that used motion features concatenated to appearance features extracted by a CNN (like in [137]), pre-trained on the CUHK03 Re-ID dataset.

Wan et al. [43] 在他们的算法中也使用了 Siamese LSTM，它也由两个步骤组成。 在第一步中，使用匈牙利算法构建了短的可靠轨迹，并使用检测和预测目标位置之间的 IoU 计算亲和力度量(通过卡尔曼滤波器或 Lucas-Kanade 光流获得)。 第二步也使用 Hungarian 算法加入 tracklets，但这次亲和力是使用 Siamese LSTM 框架计算的，该框架使用连接到 CNN 提取的外观特征的运动特征(如 [137] 中)，在 CUHK03 Re-ID 数据集。

#### 3.3.3 Bidirectional LSTMs
A different usage of LSTMs in the affinity computation phase was presented by Zhu et al. [115]. They used a so-called Temporal Attention Network (TAN) to compute attention coefficients to weigh the features extracted by the Spatial Attention Network (SAN) (see section 3.2.3) to give less importance to noisy observations. A bidirectional LSTM was employed to this end. The whole network (called Dual Matching Attention Network) was used to recover from occlusions when a modified version of the Efficient Convolution Operators tracker (ECO) [56], trained exploiting hard example mining, failed to detect a target. The algorithm obtained results comparable to online state-of-the-art methods on MOT16 and MOT17 according to various metrics (MOTA, IDF1, number of ID switches).

Zhu et al. 提出了 LSTM 在亲和力计算阶段的不同用法。 [115]。 他们使用所谓的时间注意力网络 (TAN) 来计算注意力系数，以衡量空间注意力网络 (SAN) 提取的特征(参见第 3.2.3 节)，以降低噪声观察的重要性。 为此采用了双向 LSTM。 当高效卷积运算符跟踪器 (ECO) [56] 的修改版本经过训练，利用困难样本挖掘未能检测到目标时，整个网络(称为双重匹配注意网络)用于从遮挡中恢复。 根据各种指标(MOTA、IDF1、ID 开关数量)，该算法在 MOT16 和 MOT17 上获得的结果可与在线最先进的方法相媲美。

Yoon et al. [138] also used a Bidirectional LSTM to compute affinities, on top of some FC layers that encoded non-appearance features (only bounding box coordinates and detection confidences). The association was solved using the Hungarian algorithm. They trained the network on the Stanford Drone Dataset (SDD) [139] and evaluated it on both SDD and MOT15. They reached comparable results with top algorithms that did not use visual cues, but the performance was still worse than appearance-based methods.

尹 et al. [138] 还使用双向 LSTM 来计算亲和力，在一些编码非外观特征(仅边框坐标和检测置信度)的 FC 层之上。 该关联是使用匈牙利算法解决的。 他们在斯坦福无人机数据集 (SDD) [139] 上训练了网络，并在 SDD 和 MOT15 上对其进行了评估。 他们与不使用视觉线索的顶级算法取得了可比的结果，但性能仍然比基于外观的方法差。

#### 3.3.4 Uses of LSTMs in MHT frameworks
In Multiple Hypothesis Tracking approaches, a tree of potential track hypotheses for each candidate target is first built. Then the likelihood of each track is computed and the combination of tracks that has the highest likelihood is chosen as a solution. Various deep learning algorithms have also been employed to enhance MHT based approaches.

在多假设跟踪方法中，首先为每个候选目标构建一个潜在跟踪假设树。 然后计算每个轨道的可能性，并选择具有最高可能性的轨道组合作为解决方案。 还采用了各种深度学习算法来增强基于 MHT 的方法。

Kim et al. [93] proposed the use of a so-called Bilinear LSTM network as the gating step of the MHT-DAM [83] algorithm, that is, the affinity score computed by the LSTM was used to decide whether to prune or not a certain branch of the hypotheses tree. The LSTM cell had a modified forward pass (inspired by the online recursive least squares estimator proposed in [83]) and took as input the appearance features of a tracklet in the past frames, extracted with a ResNet-50 CNN. The output of the LSTM cell was a feature matrix representing the historical appearance of the tracklet, and such matrix was then multiplied by the vector with the appearance features of the detection that needed to be compared with the tracklet. FC layers on top of that finally computed the affinity score between the tracklet and the detection. The authors claimed that such a modified LSTM is able to store longer-term appearance models than classical LSTMs. They also proposed to add a motion modeling classical LSTM to compute historical motion features (using the normalized bounding box coordinates and sizes), that were then concatenated to the appearance features before proceeding with the FC layers and the final softmax that output the affinity score. The two LSTMs were first trained separately and then fine-tuned jointly. The training data was also augmented including localization errors and missing detections, to resemble real-world data more closely. They used MOT15, MOT17, ETH, KITTI and other minor datasets for training and they evaluated the model on MOT16 and MOT17. They showed that their model is sensitive to the quality of detections, as they improved on the MOTA performance of MHT-DAM when using the public Faster R-CNN and SDP detections, while they performed worse than it on the public DPM detections. Anyway, they seemed to get a higher IDF1 score regardless of the detections used, and their overall results reflected that, since they got the highest IDF1 of all the methods using MHT-based algorithms. However, the tracking quality, as measured both in MOTA and in IDF1, was still lower than other state-of-the-art algorithms. 

Kim et al. [93] 提出使用所谓的双线性 LSTM 网络作为 MHT-DAM [83] 算法的门控步骤，即使用 LSTM 计算的亲和力分数来决定是否修剪某个分支 的假设树。 LSTM 单元具有修改后的前向传递(受 [83] 中提出的在线循环最小二乘估计器的启发)并将过去帧中轨迹的外观特征作为输入，用 ResNet-50 CNN 提取。 LSTM cell的输出是一个代表tracklet历史外观的特征矩阵，然后将该矩阵乘以需要与tracklet进行比较的检测外观特征向量。 其上的 FC 层最终计算出 tracklet 和检测之间的亲和力分数。 作者声称，这种修改后的 LSTM 能够存储比经典 LSTM 更长期的外观模型。 他们还建议添加运动建模经典 LSTM 来计算历史运动特征(使用归一化边框坐标和大小)，然后在处理 FC 层和输出亲和力分数的最终 softmax 之前将其连接到外观特征。 这两个 LSTM 首先单独训练，然后联合微调。 训练数据也得到了增强，包括定位错误和缺失检测，以更接近真实世界的数据。 他们使用 MOT15、MOT17、ETH、KITTI 和其他小型数据集进行训练，并在 MOT16 和 MOT17 上评估模型。 他们表明，他们的模型对检测质量很敏感，因为他们在使用公共 Faster R-CNN 和 SDP 检测时改进了 MHT-DAM 的 MOTA 性能，而在公共 DPM 检测中的表现比它差。 无论如何，无论使用何种检测，他们似乎都获得了更高的 IDF1 分数，并且他们的总体结果反映了这一点，因为他们在使用基于 MHT 的算法的所有方法中获得了最高的 IDF1。 然而，在 MOTA 和 IDF1 中测量的跟踪质量仍然低于其他最先进的算法。

A similar use of a RNN has been recently presented by Maksai et al. [113], who also employed a LSTM to compute tracklet scores in a variation of the MHT algorithm, that grows and prunes tracklets iteratively and then tries to select the set of tracklets that maximizes said score.16 The goal of their work was to solve two frequent problems in training recurrent networks for multiple object tracking: the loss-evaluation mismatch, that arises when a network is trained by optimizing a loss that is not well-aligned to the evaluation metric used at inference time (e.g. classification score vs. MOTA); the exposure bias, that is present when the model is not exposed to its own errors during the training process. In order to solve the first problem, they introduced a novel way to score tracklets (using a RNN) that is a direct proxy of the IDF1 metric and does not use the ground truth; the network can then be trained to optimize such metric. The second problem was solved instead by adding to the training set of the network the tracklets computed using the current version of the network, together with hard example mining and random tracklet merging during training; in this way, the training set distribution should be more similar to the inference-time input data distribution. The network used was a Bidirectional LSTM, on top of an embedding layer that took as input various features. The authors presented a version of the algorithm using only geometric features and a version that instead used appearance-based features, that performed better. Lots of ablation studies were run, and various alternative approaches were tested. The final algorithm was able to reach top-performance on various MOT datasets (MOT15, MOT17, DukeMTMC [21]) when considering the IDF1 metric, even though it didn’t excel in MOTA.

Maksai et al. [113]最近提出了 RNN 的类似用途，他们还使用 LSTM 计算 MHT 算法变体中的 tracklet 分数，该算法迭代地增长和修剪 tracklet，然后尝试选择最大化所述分数的一组 tracklet。16 他们工作的目标是解决 训练循环网络进行多目标跟踪的两个常见问题：损失评估不匹配，当通过优化与推理时使用的评估指标(例如分类得分与 MOTA)不一致的损失来训练网络时出现 ); 暴露偏差，当模型在训练过程中没有暴露自己的错误时就会出现。 为了解决第一个问题，他们引入了一种新的 tracklets 评分方法(使用 RNN)，它是 IDF1 指标的直接智能体，不使用 ground truth;  然后可以训练网络以优化此类指标。 第二个问题的解决方法是将使用当前版本的网络计算的轨迹添加到网络的训练集中，同时在训练过程中进行硬样本挖掘和随机轨迹合并;  这样，训练集分布应该更类似于推理时输入数据分布。 使用的网络是一个双向 LSTM，在一个嵌入层之上，该层将各种特征作为输入。 作者介绍了该算法的一个版本，该版本仅使用几何特征，而另一个版本使用基于外观的特征，效果更好。 进行了大量的消融研究，并测试了各种替代方法。 考虑到 IDF1 指标时，最终算法能够在各种 MOT 数据集(MOT15、MOT17、DukeMTMC [21])上达到最佳性能，尽管它在 MOTA 中并不出色。

Among the other approaches in the MHT family using RNNs we can also find [104], where Chen et al. used a so-called Recurrent Metric Network (RMNet) to compute appearance affinity between tracklet hypotheses and detections (together with a motion-based affinity) in their Batch Multi-Hypothesis Tracking strategy. The RMNet is an LSTM that takes as input appearance features of the detection sequence under consideration, extracted with a ResNet CNN, and outputs a similarity score together with bounding box regression parameters. A dual-threshold approach in gating and forming hypothesis was used, and a re-find reward was employed to encourage recovery from occlusions. The hypotheses were selected by casting the problem as a binary linear programming one, solved using lpsolve. Kalman filter was finally used to smooth the trajectories. Evaluation was performed on MOT15, PETS2009 [34], TUD [140] and KITTI, obtaining better results on the IDF1 metric, that gives more weight to people re-identification, than on MOTA.

在使用 RNN 的 MHT 家族的其他方法中，我们还可以找到 [104]，其中 Chen et al. 使用所谓的循环度量网络 (RMNet) 在其批量多假设跟踪策略中计算轨迹假设和检测之间的外观亲和力(连同基于运动的亲和力)。 RMNet 是一种 LSTM，它将所考虑的检测序列的外观特征作为输入，用 ResNet CNN 提取，并输出相似度分数和边框回归参数。 使用门控和形成假设的双阈值方法，并采用重新发现奖励来鼓励从遮挡中恢复。 通过将问题转换为二进制线性规划问题来选择假设，并使用 lpsolve 求解。 卡尔曼滤波器最终用于平滑轨迹。 对 MOT15、PETS2009 [34]、TUD [140] 和 KITTI 进行了评估，在 IDF1 指标上获得了比在 MOTA 上给予更多权重的 IDF1 指标更好的结果。

#### 3.3.5 Other recurrent networks
Fang et al. [96] used instead Gated Recurrent Units (GRUs) [141] inside their Recurrent Autoregressive Network (RAN) framework for pedestrian tracking. The GRUs were used to estimate the parameters of autoregressive models, one for motion and the other for appearance for each tracked target, that computed the probability of observing a given detection motion/appearance based on the tracklet’s past motion/appearance features. The two probabilities, that can be easily seen as a kind of affinity measure, were then multiplied together to obtain a final association probability, used to solve a bipartite matching problem for association between tracklets and detections following the algorithm in [134]. The RAN training step was formulated as a maximum likelihood estimation problem.

Fang et al. [96] 在他们的循环自回归网络 (RAN) 框架中使用门控循环单元 (GRU) [141] 来代替行人跟踪。 GRU 用于估计自回归模型的参数，一个用于运动，另一个用于每个被跟踪目标的外观，该模型根据 tracklet 过去的运动/外观特征计算观察到给定检测运动/外观的概率。 这两个概率可以很容易地看作是一种亲和力度量，然后相乘以获得最终的关联概率，用于按照 [134] 中的算法解决轨迹和检测之间关联的二分匹配问题。 RAN 训练步骤被表述为最大似然估计问题。

Kieritz et al. [60] used a recurrent 2-hidden-layer multi-layer perceptron (MLP) to compute an appearance affinity score between a detection and a tracklet. Such affinity was then given as input to another MLP, together with track and detection confidence scores, to predict an aggregate affinity score (called association metric). Such score was finally used by the Hungarian algorithm to perform association. The method reached top performance on the UA-DETRAC dataset [32], but the performance on MOT16 was not very good when compared with other algorithms using private detections.

Kieritz et al. [60] 使用循环 2 隐藏层多层感知器 (MLP) 来计算检测和轨迹之间的外观亲和力分数。 然后将这种亲和力作为另一个 MLP 的输入，连同跟踪和检测置信度分数，以预测聚合亲和力分数(称为关联度量)。 这样的分数最终被匈牙利算法用来进行关联。 该方法在 UA-DETRAC 数据集 [32] 上达到了最佳性能，但与使用私有检测的其他算法相比，在 MOT16 上的性能并不是很好。

#### 3.3.6 CNNs for affinity computation 用于亲和力计算的CNN
Other algorithms used instead CNNs to compute some kind of similarity score. Tang et al. [51] tested the use of 4 different CNNs to compute an affinity score between nodes in a graph, with the association task being formulated as a minimum cost lifted multicut problem [142]: it can be seen as a graph clustering problem, where each output cluster represents a single tracked object. The costs associated to the edges accounted for the similarity between two detections. Such similarity was a combination of person re-identification confidence, deep correspondence matching and spatio-temporal relations. To compute the person re-identification affinity, various architectures were tested (after being trained on a dataset of 2511 identities extracted from MOT15, MOT16, CUHK03, Market-1501 datasets), but the best performing one was the novel StackNetPose. It incorporated body part information extracted using the DeepCut body part detector [105] (see section 3.2.2). The 14 score maps for the body parts of two images were stacked together with the two images themselves to produce a 20-channel input. The network followed the VGG-16 architecture and output an affinity score between the two input identities. Differently from Siamese CNNs, the pair of images were able to ‘communicate’ in the early stages of the network. The authors showed that the StackNetPose network performed better in the person re-identification task, and thus they used it to compute the ReID affinity. The combined affinity score was computed by multiplying a weight vector (learned with logistic regression, and dependent on the time interval between the two detections) with a 14-d vector containing ReID affinity, DeepMatching-based affinity [143], a spatio-temporal affinity score, the minimum of the two detection confidences and quadratic terms with all the pairwise combinations of the previously mentioned terms. The authors showed that combining all these features produced better results, and together with the improvement in framing the problem as a minimum cost lifted multicut problem (solved heuristically using the algorithm proposed in [144]), they managed to reach state-of-the-art performance (measured in MOTA score) on the MOT16 dataset at the time of publishing.

其他算法使用 CNN 来计算某种相似性分数。 Tang et al. [51] 测试了使用 4 种不同的 CNN 来计算图中节点之间的亲和力得分，关联任务被表述为最小成本提升多切问题 [142]：它可以被视为图聚类问题，其中每个 输出簇表示单个跟踪目标。 与边缘相关的成本解释了两次检测之间的相似性。 这种相似性是行人重新识别置信度、深度对应匹配和时空关系的结合。 为了计算人员重新识别亲和力，测试了各种架构(在从 MOT15、MOT16、CUHK03、Market-1501 数据集提取的 2511 个身份的数据集上进行训练后)，但性能最好的是新颖的 StackNetPose。 它结合了使用 DeepCut 身体部位检测器 [105] 提取的身体部位信息(参见第 3.2.2 节)。 将两幅图像的身体部位的 14 个得分图与两幅图像本身堆叠在一起，以产生 20 通道的输入。 该网络遵循 VGG-16 架构并输出两个输入身份之间的亲和力分数。 与 Siamese CNN 不同的是，这对图像能够在网络的早期阶段进行“交流”。 作者表明 StackNetPose 网络在行人重新识别任务中表现更好，因此他们使用它来计算 ReID 亲和力。 通过将权重向量(通过逻辑回归学习，并取决于两次检测之间的时间间隔)与包含 ReID 亲和力、基于深度匹配的亲和力 [143]、时空 亲和力得分，两个检测置信度和二次项中的最小值以及前面提到的项的所有成对组合。 作者表明，将所有这些特征结合起来会产生更好的结果，并且随着将问题框架化为最小成本提升多切割问题的改进(使用 [144] 中提出的算法启发式解决)，他们设法达到了最佳状态 - 发布时在 MOT16 数据集上的艺术表现(以 MOTA 分数衡量)。

16While it is not an explicit affinity measure, it can still be seen as an evaluation of the effect of merging two tracklets and thus plays a similar role (i.e. taking decisions about associating tracklets) as other affinities presented in this section. 
16虽然它不是一个明确的亲和力度量，但它仍然可以被视为对合并两个 tracklet 的效果的评估，因此与本节中介绍的其他亲和力起着类似的作用(即做出关于关联 tracklet 的决定)。
 

Another approach using CNNs was presented in [145], where Chen et al. used a Particle Filter [146] to predict target motion, weighting the importance of each particle using a modified Faster R-CNN network. Such model was trained to predict the probability that the bounding box contains an object, but it was also augmented with a target-specific branch, that took as input features from lower layers of the CNN and merged them with the target historical features to predict the probability of the two objects being the same. The difference with the previous approaches is that here the affinity is computed between the sampled particles and the tracked target, instead of being computed between targets and detections. The detections that did not overlap with the tracked objects were instead used to initialize new tracks or retrieve missing objects. Despite being an online tracking algorithm, it was able to reach top performance on MOT15 at the time of publishing, both when using public detections and when using private ones (obtained from [147]).

[145] 中提出了另一种使用 CNN 的方法，其中 Chen et al. 使用粒子滤波器 [146] 来预测目标运动，使用改进的 Faster R-CNN 网络对每个粒子的重要性进行加权。 这样的模型被训练来预测边框包含一个目标的概率，但它也增加了一个特定于目标的分支，它将来自 CNN 较低层的输入特征作为输入特征，并将它们与目标历史特征合并以预测 两个目标相同的概率。 与以前方法的不同之处在于，这里的亲和力是在采样粒子和跟踪目标之间计算的，而不是在目标和检测之间计算的。 不与跟踪目标重叠的检测被用于初始化新跟踪或检索丢失的目标。 尽管是一种在线跟踪算法，但在发布时它能够在 MOT15 上达到最佳性能，无论是使用公共检测还是使用私有检测(从 [147] 获得)。

Zhou et al. [117] used a visual-similarity CNN, similar to the ResNet-101 based visual-displacement CNN presented in section 3.2.3, that outputs affinity scores between the detections and the tracklet boxes predicted by the Deep Continuous Conditional Random Fields. This visual affinity score was merged with a spatial similarity using IoU, and then the detection with the highest score was associated to each tracklet; in case of conflicts, the Hungarian algorithm was employed. The method reached results comparable to state-of-the-art online MOT algorithms on MOT15 and MOT16 in terms of MOTA score.

周et al. [117] 使用视觉相似性 CNN，类似于第 3.2.3 节中介绍的基于 ResNet-101 的视觉位移 CNN，它输出检测与深度连续条件随机场预测的轨迹框之间的亲和力分数。 使用 IoU 将该视觉亲和度分数与空间相似性合并，然后将得分最高的检测与每个轨迹相关联;  在发生冲突的情况下，采用匈牙利算法。 就 MOTA 分数而言，该方法在 MOT15 和 MOT16 上达到了与最先进的在线 MOT算法 相当的结果。

#### 3.3.7 Siamese CNNs
Siamese CNNs are also a common approach used in affinity computation. An example of Siamese CNN is shown in figure 5. The approaches presented here decided to directly use the output of the Siamese CNN as an affinity, instead of employing classical distances between feature vectors extracted from the penultimate layer of the network, like the algorithms presented in section 3.2.3. For example, Ma et al. [148] used one to compute affinities between tracklets in a two-step algorithm. They chose to apply hierarchical correlation clustering, solving two successive lifted multicut problems: local data association and global data association. In the local data association step temporally-close detections were joined together by using the robust similarity measure presented in [149], that uses DeepMatching and detection confidences to compute an affinity score between detections. In this step, only edges between close detections were inserted into the graph. The multicut problem was solved with the heuristic algorithm proposed in [144]. In the global data association step, local tracks that were split by long-term occlusion needed to be joined together, and a fully-connected graph with all the tracklets was then built. The Siamese CNN was used to compute the affinities that would serve as edge costs in the graph. The architecture was based on GoogLeNet [2] and it was pretrained on ImageNet. The net was then trained on the Market-1501 ReID dataset and then fine-tuned on the MOT15 and MOT16 training sequences. Besides the verification layer, that output a similarity score between the two images, two classification layers were added to the network only during training to classify the identity of each training image; this was shown to improve the network performance in computing the affinity score. This so-called ‘generic’ ReID net was also fine-tuned on each test sequence in an unsupervised manner, without using any ground truth information, to adapt the net to the illumination conditions, resolution, camera angle, etc. of each particular sequence. This was done by sampling positive and negative detection pairs by looking at the tracklets built in the local data association step. The effectiveness of the algorithm was proven by the results obtained on MOT16, where it is at the time of writing the best performing method with a published paper, with a 49.3 MOTA score.

连体 CNN 也是亲和力计算中常用的方法。 Siamese CNN 的样本如图 5 所示。此处介绍的方法决定直接使用 Siamese CNN 的输出作为亲和力，而不是像所介绍的算法那样使用从网络倒数第二层提取的特征向量之间的经典距离 在第 3.2.3 节中。 例如，马 et al. [148] 使用一个来计算两步算法中 tracklet 之间的亲和力。 他们选择应用层次相关聚类，解决两个连续的提升多割问题：局部数据关联和全局数据关联。 在局部数据关联步骤中，通过使用 [149] 中提出的稳健相似性度量将时间上接近的检测连接在一起，该度量使用深度匹配和检测置信度来计算检测之间的亲和力分数。 在这一步中，只有紧密检测之间的边缘被插入到图中。 用[144]中提出的启发式算法解决了多切割问题。 在全局数据关联步骤中，需要将因长期遮挡而分裂的局部轨迹连接在一起，然后构建一个包含所有轨迹的全连接图。 Siamese CNN 用于计算将用作图中边缘成本的亲和力。 该架构基于 GoogLeNet [2]，并在 ImageNet 上进行了预训练。 然后在 Market-1501 ReID 数据集上对网络进行训练，然后在 MOT15 和 MOT16 训练序列上进行微调。 除了输出两幅图像之间相似度得分的验证层外，仅在训练期间向网络添加两个分类层以对每幅训练图像的身份进行分类;  这被证明可以提高计算亲和力分数的网络性能。 这个所谓的“通用”ReID 网络也在每个测试序列上以无人监督的方式进行了微调，不使用任何基准实况信息，以使网络适应每个特定序列的照明条件、分辨率、摄像机角度等 . 这是通过查看在本地数据关联步骤中构建的轨迹来对正负检测对进行采样来完成的。 该算法的有效性已通过在 MOT16 上获得的结果得到证明，该算法在撰写已发表论文时表现最佳，MOTA 得分为 49.3。

As explained in section 3.2.3, Lee et al. [119] used a Feature Pyramid Siamese Network to extract appearance features. When employing this kind of network in the MOT problem, a vector of motion features was concatenated to the appearance features and 3 fully-connected layers were then added on top to predict an affinity score between a track and a detection; the network was trained end-to-end. Detections were then associated iteratively, starting from the pairs with highest affinity scores and stopping when the score got below a threshold. The method obtained top performance results among the online algorithms on the MOT17 dataset at the time of publishing.

如第 3.2.3 节所述，Lee et al. [119] 使用特征金字塔孪生网络来提取外观特征。 在 MOT 问题中使用这种网络时，将运动特征向量连接到外观特征，然后在顶部添加 3 个全连接层以预测轨迹和检测之间的亲和力得分;  网络是端到端训练的。 然后迭代地关联检测，从具有最高亲和力分数的对开始，并在分数低于阈值时停止。 在发布时，该方法在 MOT17 数据集上的在线算法中获得了最佳性能结果。

### 3.4 DL in Association/Tracking step 关联/跟踪步骤中的深度学习
Some works, albeit not as many as for the other steps in the pipeline, have used deep learning models to improve the association process performed by classical algorithms, like the Hungarian algorithm, or to manage the track status (e.g. by deciding to start or terminate a track). We are going to present them in this section, including the use of RNNs (section 3.4.1), deep multi-layer perceptrons (section 3.4.2) and deep reinforcement learning agents (section 3.4.3).

一些工作，尽管没有管道中的其他步骤那么多，已经使用深度学习模型来改进经典算法(如匈牙利算法)执行的关联过程，或管理轨道状态(例如，通过决定开始或终止 轨道)。 我们将在本节中介绍它们，包括 RNN(第 3.4.1 节)、深度多层感知器(第 3.4.2 节)和深度强化学习智能体(第 3.4.3 节)的使用。

#### 3.4.1 Recurrent neural networks
A first example of algorithms employing DL to manage the track status is the one presented by Milan et al. in [133], already described in section 3.3.1, that used a RNN to predict the probability of existence of a track in each frame, thus helping with the decision of when to initiate or terminate the tracks.

第一个使用 DL 来管理轨道状态的算法样本是由 Milan et al. 提出的。 在 [133] 中，已经在第 3.3.1 节中描述过，它使用 RNN 来预测每个帧中轨迹存在的概率，从而帮助决定何时启动或终止轨迹。

Ma et al. [116] used a bidirectional GRU RNN to decide where to split tracklets. The algorithm proceeded in three main stages: a tracklet generation step, that included a NMS step to remove redundant detections and then employed the Hungarian algorithm with appearance and motion affinity together to form high-confidence tracklets; then, a tracklet cleaving step was performed: since a tracklet might contain an ID switch error due to occlusions, this step aimed to split the tracklets at the point where the ID switch happened, in order to obtain two separate tracklets that contained the same identity; finally, a tracklet reconnection step was employed, using a customized association algorithm that made use of features extracted by a Siamese bidirectional GRU. The gaps within the newly-formed tracklets were then filled with polynomial curve fitting. The cleaving step was performed with a bidirectional GRU RNN, that used features extracted by a Wide Residual Network CNN [92]. The GRU output a pair of feature vectors for each frame (one for each direction of the GRU); then the distance between pairs of such feature vectors was computed and a distance vector was obtained. The highest value in this vector indicated where to split the tracklet, provided that the score was higher than a threshold. The reconnection GRU was similar, but it had an additional FC layer on top of the GRU and a temporal pooling layer to extract a feature vector representing the whole tracklet; the distance between the features of the two tracklets was then used to decide which tracklets to reconnect. The algorithm reached results comparable to state-of-the-art on the MOT16 dataset.

马et al. [116] 使用双向 GRU RNN 来决定在哪里分割 tracklets。 该算法分为三个主要阶段：一个轨迹生成步骤，包括一个 NMS 步骤以去除冗余检测，然后采用具有外观和运动亲和力的匈牙利算法来形成高置信度的轨迹;  然后，执行轨迹切割步骤：由于轨迹可能包含由于遮挡而导致的 ID 切换错误，因此此步骤旨在在 ID 切换发生的位置拆分轨迹，以获得包含相同身份的两个独立轨迹 ; 最后，采用了轨迹重新连接步骤，使用定制的关联算法，该算法利用了 Siamese 双向 GRU 提取的特征。 然后用多项式曲线拟合填充新形成的轨道内的间隙。 切割步骤是使用双向 GRU RNN 执行的，它使用了由 Wide Residual Network CNN [92] 提取的特征。 GRU为每一帧输出一对特征向量(GRU的每个方向一个);  然后计算这些特征向量对之间的距离并获得距离向量。 如果得分高于阈值，则此向量中的最高值表示在何处拆分 tracklet。 重新连接 GRU 类似，但它在 GRU 之上有一个额外的 FC 层和一个时间池层来提取代表整个 tracklet 的特征向量;  然后使用两个轨迹特征之间的距离来决定重新连接哪些轨迹。 该算法在 MOT16 数据集上达到了与最先进技术相当的结果。

#### 3.4.2 Deep Multi-Layer Perceptron
Despite not being a very common approach, deep multi-layer perceptrons (MLP) have also been employed to guide the tracking process. For example, Kieritz et al. [60] used a MLP with two hidden layers to compute track confidence scores, taking as input the track score at the previous step and various information about the last associated detection (like association score and detection confidence). This confidence score was then used to manage the termination of tracks: they decided in fact to keep a fixed number of targets through time, replacing with new tracks the older ones that had the lowest confidence scores. The rest of the algorithm has been explained in section 3.3.5.

尽管不是很常见的方法，但深度多层感知器 (MLP) 也已被用来指导跟踪过程。 例如，Kieritz et al. [60] 使用具有两个隐藏层的 MLP 来计算轨道置信度分数，将上一步的轨道分数和有关最后关联检测的各种信息(如关联分数和检测置信度)作为输入。 然后使用这个置信度分数来管理轨迹的终止：事实上，他们决定随着时间的推移保持固定数量的目标，用新轨迹替换置信度分数最低的旧轨迹。 该算法的其余部分已在第 3.3.5 节中进行了解释。

#### 3.4.3 Deep Reinforcement Learning agents
Some works have used Deep Reinforcement Learning (RL) agents to take decisions in the tracking process. Rosello et al. [131], as explained in section 3.2.6, used multiple deep RL agents to manage the various tracked targets, deciding when to start and stop tracks and influencing the operation of the Kalman filter. The agent was modeled with a MLP with 3 hidden layers.

一些工作使用深度强化学习 (RL) 智能体在跟踪过程中做出决定。 罗塞洛 et al. [131]，如第 3.2.6 节所述，使用多个深度 RL 智能体来管理各种跟踪目标，决定何时开始和停止跟踪并影响卡尔曼滤波器的操作。 该智能体使用具有 3 个隐藏层的 MLP 建模。

Ren et al. [150] also used multiple deep RL agents in a collaborative environment to manage the association task. The algorithm was mainly composed of two parts: a prediction network and a decision network. The prediction network was a CNN that was learned to predict the movement of the target in the new frame looking at the target and at the new image, and also using the recent tracklet trajectory. The decision network was instead a collaborative system that consisted of multiple agents (one for each tracked target) and the environment. Each agent took decisions based on the information about themselves, the neighbours and the environment; the interactions between the agents and the environment were exploited by maximizing a shared utility function: the agents thus did not operate independently from each other. Every agent/object was represented by a trajectory, its appearance features (extracted using MDNet [151]) and its current position. The environment was represented by the detections in the new frame. The detection network took as input, for each target, its predicted location in the new frame (output by the prediction network), the nearest target and the nearest detection, and based on various factors, such as the detection reliability and the target occlusion status, took one among various actions: updating the track and its appearance features using both the prediction and the detection, ignoring the detection and only using the prediction to update the track, detecting an occlusion of the tracked target, deleting the track. The agents were modelled using 3 FC layers on top of the feature extraction part of the MDNet. Various ablation studies showed the effectiveness of using the prediction and detection networks instead of linear motion models and Hungarian algorithm, respectively, and the method obtained very good results on the MOT15 and MOT16 datasets, reaching state-of-the-art performance among online methods, despite suffering from a relatively high number of ID switches. 

任et al. [150] 还在协作环境中使用多个深度 RL 智能体来管理关联任务。 该算法主要由预测网络和决策网络两部分组成。 预测网络是一个 CNN，它被学习来预测目标在新帧中看着目标和新图像的运动，并且还使用最近的 tracklet 轨迹。 决策网络是一个协作系统，由多个智能体(每个跟踪目标一个)和环境组成。 每个智能体人根据有关自己、邻居和环境的信息做出决定;  通过最大化共享效用函数来利用智能体与环境之间的交互：智能体因此不会彼此独立运行。 每个智能体/目标都由轨迹、外观特征(使用 MDNet [151] 提取)及其当前位置表示。 环境由新框架中的检测表示。 检测网络以每个目标在新帧中的预测位置(由预测网络输出)、最近目标和最近检测作为输入，并基于检测可靠性和目标遮挡状态等各种因素 ，采取了多种操作之一：同时使用预测和检测来更新轨迹及其外观特征，忽略检测并仅使用预测来更新轨迹，检测被跟踪目标的遮挡，删除轨迹。 在 MDNet 的特征提取部分之上使用 3 个 FC 层对智能体进行建模。 各种消融研究表明使用预测和检测网络分别代替线性运动模型和匈牙利算法的有效性，并且该方法在 MOT15 和 MOT16 数据集上获得了非常好的结果，达到了在线方法中最先进的性能 ，尽管 ID 切换次数相对较多。


### 3.5 Other uses of DL in MOT
In this section we will present other interesting uses of deep learning models that don’t neatly fit into one of the four common steps of a multiple object tracking algorithm. For this reason, such works have not been included in table 8, but are summarized instead in table 1.

在本节中，我们将介绍深度学习模型的其他有趣用途，这些用途并不完全适合多目标跟踪算法的四个常见步骤之一。 因此，此类工作未包含在表 8 中，而是汇总在表 1 中。

Table 1: Information summary about methods using DL that don’t fit the 4-step scheme.
表 1：有关使用不适合 4 步方案的 DL 方法的信息摘要。

One example is [152], where Jiang et al. use a Deep RL agent to perform bounding box regression after the use of one of many MOT algorithms. The procedure is in fact completely independent from the tracking algorithm employed, and can be used a posteriori to increase the accuracy of the model. A VGG-16 CNN was used to extract appearance features from the region enclosed by the bounding box, then those features were concatenated to a vector representing the history of the last 10 actions taken by the agent. Finally a Q-network [160] made of 3 fully-connected layers was used to predict one among 13 possible actions, that included motion and scaling of the bounding box and a termination action, to signal the completion of the regression. The use of this bounding box regression technique on various state-of-the-art MOT algorithms allowed an improvement between 2 and 7 absolute MOTA points on the MOT15 dataset, reaching top score among public detections methods. The authors also showed that their regression approach had better results than using conventional methods, such as the bounding box regression computed by a Faster R-CNN model.

一个例子是 [152]，其中 Jiang et al. 在使用许多 MOT算法 之一后，使用深度 RL 智能体执行边框回归。 该过程实际上完全独立于所采用的跟踪算法，并且可以使用后验来提高模型的准确性。 VGG-16 CNN 用于从边框包围的区域中提取外观特征，然后将这些特征连接到表示智能体最近 10 次动作历史的向量。 最后，使用由 3 个全连接层组成的 Q 网络 [160] 来预测 13 个可能动作中的一个，其中包括边框的运动和缩放以及终止动作，以表示回归完成。 在各种最先进的 MOT算法 上使用这种边框回归技术，可以在 MOT15 数据集上改进 2 到 7 个绝对 MOTA 点，在公共检测方法中达到最高分。 作者还表明，他们的回归方法比使用传统方法(例如由 Faster R-CNN 模型计算的边框回归)有更好的结果。

Lee et al. [153] proposed a multi-class multi-object tracker that used an ensemble of detectors, including CNN models like VGG-16 and ResNet, to compute the likelihood of each target being at a certain location in the next frame. A Markov Chain Monte Carlo sampling from a distribution that was influenced by said likelihoods was used to predict the next position for each target, and together with an estimation of track birth and death probabilities, short track segments were built. Finally, a changing point detection [161] algorithm was employed to detect abrupt changes in stationary time series representing track segments; this was done in order to detect track drift, to remove unstable track segments and to combine the segments together. The algorithm reached results comparable to state-of-the-art MOT methods using private detections.

Lee et al. [153] 提出了一种多类多目标跟踪器，它使用一组检测器(包括 VGG-16 和 ResNet 等 CNN 模型)来计算每个目标在下一帧中位于特定位置的可能性。 从受所述可能性影响的分布中抽样的马尔可夫链蒙特卡洛样本用于预测每个目标的下一个位置，并与轨道出生和死亡概率的估计一起，建立了短轨道段。 最后，采用变点检测[161]算法来检测代表轨道段的静止时间序列的突然变化;  这样做是为了检测轨道漂移，移除不稳定的轨道段并将这些段组合在一起。 该算法达到的结果可与使用私人检测的最先进的 MOT 方法相媲美。

Hoak et al. [154] proposed a 5-layer custom CNN network, trained on the Caltech pedestrian detection dataset [162], to compute the likelihood of a target being at a certain location in the image. They used a multi-Bernoulli filter (implemented using the particle filter algorithm presented in [163]), and a novel Interactive Likelihood (ILH) was computed for each particle, in order to weigh them based on their distance from particles belonging to other targets; this was done to prevent the algorithm from sampling from areas that belong to different objects. The algorithm obtained good results on the VSPETS 2003 INMOVE soccer dataset17 and the AFL dataset [164].

Hoak et al. [154] 提出了一个 5 层自定义 CNN 网络，在加州理工学院行人检测数据集 [162] 上训练，以计算目标在图像中某个位置的可能性。 他们使用了一个多伯努利滤波器(使用[163]中提出的粒子滤波器算法实现)，并为每个粒子计算了一个新的交互似然(ILH)，以便根据它们与属于其他目标的粒子的距离来衡量它们 ; 这样做是为了防止算法从属于不同目标的区域进行采样。 该算法在 VSPETS 2003 INMOVE 足球数据集 17 和 AFL 数据集 [164] 上取得了良好的效果。

Henschel et al. [155] used head detections, extracted with a CNN [156], in addition to the usual body detections to perform pedestrian tracking. The presence/absence of a head and its position relative to the bounding box can help determine if a bounding box is a true or a false positive. The association problem was modelled as a correlation clustering problem on graphs, that the authors solved with a modified version of the Frank-Wolfe algorithm [165]; the association costs were computed as a combination of spatial and temporal costs: the spatial costs were the distance and the angle between the detected and the predicted head positions; the temporal costs were computed using the correspondences between pixels between the two frames, obtained using DeepMatching [79]. The algorithm reached top MOTA score on MOT17 and second-best score on MOT16 at the time of publication.

Henschel et al. [155] 除了通常的身体检测之外，还使用了用 CNN [156] 提取的头部检测来执行行人跟踪。 头部的存在/不存在及其相对于边框的位置可以帮助确定边框是真阳性还是假阳性。 关联问题被建模为图上的相关聚类问题，作者使用 Frank-Wolfe 算法的修改版本 [165] 解决了这个问题;  关联成本计算为空间成本和时间成本的组合：空间成本是检测到的头部位置和预测的头部位置之间的距离和角度;  使用深度匹配 [79] 获得的两帧像素之间的对应关系计算时间成本。 在发布时，该算法在 MOT17 上获得了最高的 MOTA 分数，在 MOT16 上获得了第二好的分数。

Gan et al. [157] employed a modified MDNet [151] in their online pedestrian tracking framework. Besides 3 shared convolutional layers, common to all the targets, each target also had 3 specific FC layers, that were updated online to capture the appearance change of the target. A set of box candidates, including detections intersecting the last bounding box of the target and a set of boxes sampled from a Gaussian distribution with parameters estimated using a linear motion model, were given as input to the network, that output a confidence score for each of them. The candidate with the highest score was considered the optimal estimated target location. To reduce the number of ID switch errors, the algorithms tried to find the past trajectory that was most similar to the estimated box, using another affinity measure between the pairs; such affinity was computed using appearance and motion cues, together with the tracklet confidence score and a collision factor. Detections were also used to initialize new tracklets and to fix the motion prediction errors when occlusions happened.

Gan et al. [157] 在他们的在线行人跟踪框架中采用了修改后的 MDNet [151]。 除了所有目标共有的 3 个共享卷积层外，每个目标还有 3 个特定的 FC 层，这些层在线更新以捕获目标的外观变化。 一组候选框，包括与目标最后一个边框相交的检测和一组从高斯分布中采样的框，这些框使用线性运动模型估计参数，作为网络的输入，输出每个框的置信度分数 他们中的。 得分最高的候选人被认为是最佳估计目标位置。 为了减少 ID 切换错误的数量，算法尝试使用对之间的另一种亲和度度量来找到与估计框最相似的过去轨迹;  这种亲和力是使用外观和运动线索以及轨迹置信度分数和碰撞因子来计算的。 检测还用于初始化新的轨迹，并在发生遮挡时修复运动预测错误。

Xiang et al. [158] used a MetricNet to track pedestrians. The model unified an affinity model with trajectory estimation, done with a Bayesian filter. An appearance model, made of a VGG-16 CNN trained for person re-identification on various datasets, extracted features and performed bounding box regression; the motion model instead consisted of two parts: an LSTM-based feature extractor, that took as input the trajectory’s past coordinates, and a so-called BF-Net on top, made of various FC layers, that combined the features extracted by the LSTM and a detection box (chosen by the Hungarian algorithm) to perform the Bayesian filtering step and output the new position of the target. The MetricNet was trained using a triplet loss, similar to other models presented in the previous sections. The algorithm obtained the best and second-best results among online methods on MOT16 and MOT15, respectively.

Xiang et al. [158] 使用 MetricNet 来跟踪行人。 该模型将亲和力模型与轨迹估计统一起来，使用贝叶斯过滤器完成。 外观模型，由 VGG-16 CNN 组成，在各种数据集上进行行人再识别训练，提取特征并执行边框回归;  运动模型由两部分组成：一个基于 LSTM 的特征提取器，它将轨迹的过去坐标作为输入，以及一个所谓的 BF-Net 在上面，由各种 FC 层组成，它结合了 LSTM 提取的特征 和一个检测框(由匈牙利算法选择)执行贝叶斯过滤步骤并输出目标的新位置。 MetricNet 使用三元组损失进行训练，类似于前面部分介绍的其他模型。 该算法在 MOT16 和 MOT15 上分别获得了在线方法中最好和第二好的结果。

Finally, Chu et al. [159] used three different CNNs in their algorithm. The first one, called PafNet [166], was used to distinguish the background from the tracked objects. The second one, called PartNet [167], was employed to distinguish among the different targets. The third CNN, made of one convolutional layer and one FC layer, was instead used to decide whether to refresh the tracking model or not. The overall algorithm worked as follows: for every tracked target in the past frame, two score maps were computed in the current one, using PafNet and PartNet. Then, using the Kernel Correlation Filter tracker [168], a new position for the object was predicted. Moreover, after a certain number of frames, a so-called detection verification step was performed: the detections output by a detector (in their experiments, they chose to use the public detections provided with the dataset) were assigned to the tracked targets by solving a graph multicut problem. Targets that were not associated to a detection for a certain number of frames were terminated. Then, the third CNN was employed to check if the associated detection box was better than the predicted one. If so, the KCF model parameters were updated to reflect the change in the object characteristics. Such CNN used the maps extracted by PafNet, and was trained using reinforcement learning. Unassociated detections were then employed to recover from target occlusion, using a SVM classifier and the Hungarian algorithm. Finally, the remaining unassociated detections were used to initialize new targets. The algorithm was evaluated both on MOT15 and MOT16 datasets, reaching top performance overall on the first one, and top performance among online methods on the second. 

最后，Chu et al. [159] 在他们的算法中使用了三种不同的 CNN。 第一个称为 PafNet [166]，用于区分背景和跟踪目标。 第二个称为 PartNet [167]，用于区分不同的目标。 第三个 CNN 由一个卷积层和一个 FC 层组成，用于决定是否刷新跟踪模型。 整体算法的工作原理如下：对于过去帧中的每个跟踪目标，使用 PafNet 和 PartNet 在当前帧中计算两个得分图。 然后，使用内核相关过滤器跟踪器 [168]，预测了目标的新位置。 此外，在一定数量的帧之后，执行了所谓的检测验证步骤：通过求解将检测器输出的检测(在他们的实验中，他们选择使用数据集提供的公共检测)分配给跟踪目标 图形多切割问题。 与一定数量帧的检测无关的目标被终止。 然后，第三个 CNN 被用来检查相关的检测框是否比预测的更好。 如果是，则更新 KCF 模型参数以反映目标特征的变化。 这种 CNN 使用 PafNet 提取的地图，并使用强化学习进行训练。 然后，使用 SVM 分类器和匈牙利算法，使用非关联检测从目标遮挡中恢复。 最后，剩余的未关联检测用于初始化新目标。 该算法在 MOT15 和 MOT16 数据集上进行了评估，在第一个数据集上达到了总体性能最高，在第二个数据集上达到了在线方法中的最佳性能。

## 4 Analysis and comparisons
This section presents a comparison between all the works that have tested their algorithm on one of the MOTChallenge datasets. We will only focus on the MOTChallenge datasets since for other datasets there aren’t enough relevant papers using deep learning to perform a meaningful analysis.

本节介绍了在其中一个 MOTChallenge 数据集上测试其算法的所有工作之间的比较。 我们将只关注 MOTChallenge 数据集，因为对于其他数据集，没有足够的相关论文使用深度学习来执行有意义的分析。

We first describe the setup of the experimental analysis, including the considered metrics and the organization of the tables in section 4.1. Section 4.2 will then present the actual results and considerations derived from the analysis. 

我们首先描述实验分析的设置，包括考虑的指标和第 4.1 节中表格的组织。 然后，第 4.2 节将介绍分析得出的实际结果和考虑因素。

17ftp://ftp.cs.rdg.ac.uk/pub/VS-PETS/

### 4.1 Setup and organization
For a fair comparison, we only show results reported on the whole test sets. Some of the discussed papers report their results using subsets of the test set, or validation datasets extracted from the training splits of the MOTChallenge datasets. These results are discarded as they are not comparable with the others. Moreover, the reported results are divided into algorithms that use public detections and algorithms that use private detections, since the different quality of the detections has a big impact on performance. The results are further split into online and batch methods, since the online methods are at a disadvantage, being only able to access present and past information to assign IDs in each frame.

为了公平比较，我们只显示在整个测试集上报告的结果。 一些讨论的论文使用测试集的子集或从 MOTChallenge 数据集的训练拆分中提取的验证数据集报告了他们的结果。 这些结果被丢弃，因为它们无法与其他结果进行比较。 此外，报告的结果分为使用公共检测的算法和使用私有检测的算法，因为检测质量的不同对性能有很大影响。 结果进一步分为在线和批处理方法，因为在线方法处于劣势，只能访问当前和过去的信息以在每个帧中分配 ID。

For each algorithm we indicate the year of the referenced published paper, their mode of operation (batch vs. online); the MOTA, MOTP, IDF1, Mostly Tracked (MT) and Mostly Lost (ML) metrics, expressed in percentages; the absolute number of false positives (FP), false negatives (FN), ID switches (IDS) and fragmentations (Frag); the speed of the algorithm expressed in frames per second (Hz). For each metric, an arrow pointing up (↑) indicates that a higher score is better, while an arrow pointing down (↓) indicates the opposite. The metrics shown here are the same that can be found on the public leaderboards on the MOTChallenge website. The numerical results presented in the referenced works have been integrated with data from the MOTChallenge leaderboards.

对于每个算法，我们都指出了参考发表论文的年份、它们的操作模式(批处理与在线);  MOTA、MOTP、IDF1、Mostly Tracked (MT) 和 Mostly Lost (ML) 指标，以百分比表示;  误报 (FP)、漏报 (FN)、ID 开关 (IDS) 和碎片 (Frag) 的绝对数量;  算法的速度，以每秒帧数 (Hz) 表示。 对于每个指标，向上的箭头 (↑) 表示分数越高越好，而向下的箭头 (↓) 表示相反。 此处显示的指标与 MOTChallenge 网站上的公共排行榜上的指标相同。 参考工作中给出的数值结果已与 MOTChallenge 排行榜的数据相结合。

Attending to the classification presented before, a table for each of the combinations dataset/detection source is shown. Tables 2 and 3 show results on MOT15 using public and private detections respectively; tables 4 and 5 do the same on MOT16; finally, table 6 shows results on MOT17, who currently only has published algorithms that use public detections. Each table groups online and batch methods separately, and for each group the papers are sorted by year, and then by ascending MOTA score if the papers are from the same year, since it is the main metric considered in the MOTChallenge datasets18. If a work presents multiple results on the same dataset, using the same set of detections and the same mode of operation, we only show the result with the highest MOTA. The best performance for each metric is highlighted in bold, while the best performance among papers operating in the same mode (batch/online) is underlined. It is important to note though that comparisons on the Hz metric may not be reliable since the performance is usually reported only for the tracking part of the algorithms, without the detection step and sometimes without including the runtime of deep learning models, that are usually the most computational intensive part of the algorithms presented in this survey; moreover, the algorithms were run on widely different hardware.

关注前面介绍的分类，显示了每个组合数据集/检测源的表格。 表 2 和表 3 分别显示了使用公共检测和私有检测在 MOT15 上的结果;  表 4 和表 5 在 MOT16 上做同样的事情;  最后，表 6 显示了 MOT17 的结果，MOT17 目前只发布了使用公共检测的算法。 每个表分别对在线和批处理方法进行分组，对于每组论文按年份排序，如果论文来自同一年，则按 MOTA 分数升序排列，因为它是 MOTChallenge 数据集考虑的主要指标18。 如果一个工作在同一个数据集上呈现多个结果，使用相同的检测集和相同的操作模式，我们只展示具有最高 MOTA 的结果。 每个指标的最佳性能以粗体突出显示，而以相同模式(批处理/在线)运行的论文中的最佳性能则以下划线标出。 重要的是要注意，尽管对 Hz 度量的比较可能不可靠，因为通常只报告算法的跟踪部分的性能，没有检测步骤，有时不包括深度学习模型的运行时间，这通常是 本次调查中提出的算法中计算最密集的部分;  此外，这些算法在广泛不同的硬件上运行。

Table 2: Experimental results of MOT algorithms using deep learning and public detections on MOT15 dataset. 
表 2：MOT算法 在 MOT15 数据集上使用深度学习和公共检测的实验结果。

18If not differently specified, when we use in this section expressions such as "best performing" or similar, we are always referring to a higher MOTA score, since it’s the main evaluation metric used in the MOTChallenge benchmark. 

18如果没有特别说明，当我们在本节中使用“最佳表现”或类似表述时，我们总是指更高的 MOTA 分数，因为它是 MOTChallenge 基准测试中使用的主要评估指标。


Table 4: Experimental results of MOT algorithms using deep learning and public detections on MOT16 dataset.

### 4.2 Discussion of the results
#### General observations

As expected, the best performing algorithms on each dataset use private detections, confirming the fact that the detection quality dominates the overall performance of the tracker: 56.5% MOTA vs. 42.3% for MOT15 and 71.0% vs. 49.3% for MOT16. Moreover, on MOT16 and MOT17 the batch algorithms slightly outperform the online ones, even though the online methods are progressively getting closer to the performance of the batch ones. In fact, on MOT15 the best reported algorithm that uses deep learning runs in an online fashion. However, this can be an effect of a greater focus on developing online methods, which is a trend in the MOT deep learning research community. A common problem among online methods, that is not reflected in the MOTA score, is the higher number of fragmentations, as we can see in table 7. This happens because, when occlusions occur or detections are missing, online algorithms cannot look ahead in the video, re-identify the lost targets and interpolate the missing part of the trajectories [124, 89, 94]. We can see in figure 8 an example of trajectory that is fragmented by an online method, MOTDT [118], while it is correctly tracked by a batch method, eHAF16 [103].

正如预期的那样，每个数据集上性能最好的算法使用私有检测，证实了检测质量主导跟踪器整体性能的事实：56.5% MOTA vs. MOT15 42.3% 和 71.0% vs. MOT16 49.3%。 此外，在 MOT16 和 MOT17 上，批处理算法略优于在线算法，尽管在线方法的性能越来越接近批处理算法。 事实上，在 MOT15 上，使用深度学习的最佳报告算法以在线方式运行。 然而，这可能是更加注重开发在线方法的结果，这是 MOT 深度学习研究社区的趋势。 在线方法中的一个常见问题(未反映在 MOTA 分数中)是碎片数量较多，如我们在表 7 中所见。发生这种情况是因为，当发生遮挡或检测缺失时，在线算法无法向前看 视频，重新识别丢失的目标并插入轨迹的缺失部分 [124, 89, 94]。 我们可以在图 8 中看到一个轨迹样本，它被在线方法 MOTDT [118] 分段，而它被批处理方法 eHAF16 [103] 正确跟踪。

Another interesting thing to notice is that since the MOTA score is basically a normalized sum of FPs, FNs and ID switches, and since the number of FNs is usually at least an order of magnitude higher than the FPs and two order of magnitudes higher than the ID switches, the methods that manage to strongly reduce the number of FNs are the ones that obtain the best performances. We can in fact observe a strong correlation between MOTA and number of FNs, in accordance to what was found in [14]: MOTA and FN values are linked by a Pearson correlation coefficient of −0.95 on MOT15, −0.98 on MOT16 and −0.95 on MOT17. So, while there have been limited improvements in the reduction of FNs using public detections, the most effective way is still building and training a custom detector; the halving of the number of FNs is in fact the main reason why private detectors have lead to better tracking performances, being able to identify previously uncovered targets. In figure 9 we can see how the SORT algorithm, that is particularly sensitive to missing detections, is not able to detect a target as soon as the relative detection is missing.

另一个需要注意的有趣的事情是，由于 MOTA 分数基本上是 FP、FN 和 ID 开关的归一化总和，并且由于 FN 的数量通常至少比 FP 高一个数量级，比 ID 开关，设法大大减少 FN 数量的方法是获得最佳性能的方法。 事实上，根据 [14] 中的发现，我们可以观察到 MOTA 和 FN 数量之间存在很强的相关性：MOTA 和 FN 值通过 Pearson 相关系数联系起来，MOT15 为 -0.95，MOT16 为 -0.98，MOT16 为 -0.95 在 MOT17 上。 因此，虽然使用公共检测减少 FN 的改进有限，但最有效的方法仍然是构建和训练自定义检测器;  FN 数量减半实际上是私人检测器导致更好跟踪性能的主要原因，能够识别以前未发现的目标。 在图 9 中，我们可以看到对缺失检测特别敏感的 SORT 算法如何无法在相关检测缺失时立即检测到目标。
 
Table 5: Experimental results of MOT algorithms using deep learning and private detections on MOT16 dataset.
表 5：MOT算法 在 MOT16 数据集上使用深度学习和私有检测的实验结果。

Table 6: Experimental results of MOT algorithms using deep learning and public detections on MOT17 dataset. 
表 6：MOT算法 在 MOT17 数据集上使用深度学习和公共检测的实验结果。


To avoid this issue, many new algorithms are including new strategies to tackle this problem. In fact, while basic approaches that perform interpolation are able to recover missing boxes during occlusions, this is still insufficient to detect targets that are not covered by even a single detection, that have been shown to be 18% of the total on MOT15 and MOT16 [14]. For example, the eHAF16 algorithm presented by Sheng et al. [103] employed a superpixel extraction algorithm to complement the publicly provided detections and was in fact able to significantly reduce the number of false negatives on MOT17, reaching top MOTA score on the dataset. The MOTDT algorithm [118] instead used a R-FCN to integrate the missing detections with new candidates, and was able to reach best MOTA and lowest number of false negatives among online algorithms on MOT17. The AP-RCNN algorithm [145] was also able to avoid the problems caused by missing detections by employing a Particle Filter algorithm and relying on detections only to initialize new targets and to recover lost ones. The algorithm presented in [150] also reduces FNs by designing a deep prediction network, whose aim is to learn the motion model of the objects. At test time, the network is capable to predict the position of them in subsequent frames, and thus reducing the amount of false negatives produced by missing detections. In fact, it is the second best among online methods in MOT16 regarding this metric.

为了避免这个问题，许多新算法都包含了解决这个问题的新策略。 事实上，虽然执行插值的基本方法能够在遮挡期间恢复丢失的框，但这仍然不足以检测甚至没有被单次检测覆盖的目标，已显示占 MOT15 和 MOT16 总数的 18% [14]。 例如，Sheng et al. 提出的 eHAF16 算法。 [103] 采用超像素提取算法来补充公开提供的检测，实际上能够显著减少 MOT17 上的假阴性数量，达到数据集上的最高 MOTA 分数。 MOTDT 算法 [118] 改为使用 R-FCN 将缺失检测与新候选者集成，并且能够在 MOT17 的在线算法中达到最佳 MOTA 和最低数量的漏报。 AP-RCNN 算法 [145] 还能够通过采用粒子滤波器算法并仅依靠检测来初始化新目标和恢复丢失的目标，从而避免由丢失检测引起的问题。 [150] 中提出的算法还通过设计深度预测网络来减少 FN，其目的是学习目标的运动模型。 在测试时，网络能够预测它们在后续帧中的位置，从而减少因缺失检测而产生的假阴性数量。 事实上，就此指标而言，它在 MOT16 的在线方法中排名第二。

An important observation must be made regarding the training strategy for affinity networks. As noted by [93], training a network using ground truth trajectories to predict affinities might produce suboptimal results, as at test time those networks would be exposed to a different data distribution, made of noisy trajectories that can include missing/wrong detections. Many algorithms in fact have chosen to train networks using either actual detections [96] or by manually adding noise and errors to the ground truth trajectories [93, 115], although this may slow the training procedure sometimes and not always be feasible [60].

必须对亲和网络的训练策略进行重要观察。 正如 [93] 所指出的，使用地面真实轨迹来预测亲和力的网络可能会产生次优结果，因为在测试时，这些网络将暴露于不同的数据分布，由可能包括缺失/错误检测的嘈杂轨迹组成。 事实上，许多算法都选择使用实际检测 [96] 或通过手动向基准实况轨迹添加噪声和错误来训练网络 [93、115]，尽管这有时可能会减慢训练过程并且并不总是可行 [60] .

Figure 8: Example of fragmentation produced by an online method during occlusion. Above: tracking results for MOTDT [118], online algorithm. Below: tracking results for eHAF16 [103], batch algorithm. From left to right, frames 50, 60 and 70 of the MOT16-08 video are shown for both methods. Only the relevant boxes are shown to avoid clutter. As we can see in the image, while some online algorithms are able to re-identify lost targets after occlusions, they are usually unable to track them while the target is not visible, and this results in a fragmentation. Batch methods, on the other hand, are capable of reconstructing a fragmented trajectory by inferring the position of the target given past and future information.

图 8：遮挡期间在线方法产生的碎片样本。 上图：MOTDT [118] 的跟踪结果，在线算法。 下图：eHAF16 [103] 批处理算法的跟踪结果。 从左到右，两种方法都显示了 MOT16-08 视频的第 50、60 和 70 帧。 只显示相关框以避免混乱。 正如我们在图像中看到的，虽然一些在线算法能够在遮挡后重新识别丢失的目标，但它们通常无法在目标不可见时跟踪它们，这会导致碎片化。 另一方面，批处理方法能够通过在给定过去和未来信息的情况下推断目标的位置来重建碎片化的轨迹。

Mode | MOT15 | MOT16 | MOT17
--- | --- | --- | ---
Batch | 1143.8 | 1104.9 | 3188.2
Online | 1509.5 | 1820.2 | 7555.8

Table 7: Average number of fragmentations for online and batch methods in the three considered datasets. 
表 7：三个考虑的数据集中在线和批处理方法的平均碎片数。


#### Best approaches in the four MOT steps  四个步骤中的最佳方法

Speaking of private detections, the tables show that the best performing detectors are currently Faster R-CNN and its variants. In fact, the algorithm presented in [38], that uses a modified Faster R-CNN, has held its top ranking position among online methods on MOT16 for 3 years, and many of the other top-performing MOT16 algorithms have employed the same detections. In contrast, algorithms that employed the SSD detector, such as the ones presented in [60] and [61], tend to perform worse. A big advantage of SSD, though, is its faster speed: thanks to that the algorithm by Kieritz et al. [60] was able to reach near real-time performance (4.5 FPS) including the detection step19. Despite the great number of online methods, a major issue in using deep learning techniques in a MOT pipeline is still the difficulty in obtaining real-time predictions, making such algorithms not usable in most practical online scenarios.

说到私有检测，表格显示目前性能最好的检测器是 Faster R-CNN 及其变体。 事实上，[38] 中提出的算法使用改进的 Faster R-CNN，在 MOT16 的在线方法中已经保持了 3的最高排名，许多其他表现最好的 MOT16 算法也采用了相同的检测 . 相比之下，使用 SSD 检测器的算法，例如 [60] 和 [61] 中提出的算法，往往表现更差。 不过，SSD 的一大优势是速度更快：这要归功于 Kieritz et al. 的算法。 [60] 能够达到接近实时的性能(4.5 FPS)，包括检测步骤 19。 尽管在线方法众多，但在MOT pipeline中使用深度学习技术的一个主要问题仍然是难以获得实时预测，使得此类算法无法在大多数实际在线场景中使用。

Regarding feature extraction, all the top performing methods on the three considered datasets have used a CNN to extract appearance features, where GoogLeNet is the most common one. Methods that do not exploit appearance (either extracted with deep or classical methods) tend to perform worse. However, visual features are not enough: many of the best algorithms also employ other types of features to compute affinity, especially motion ones. In fact, algorithms like LSTMs and Kalman Filters are often employed to predict the position of the target in the next frame and this often helps in improving the quality of the association. Various Bayesian filter algorithms, such as particle filter and hypotheses density filter, are also used to predict target motion, and they benefit from the use of deep models [158, 145, 98]. Nonetheless, even when employed together with non-visual features, appearance still plays a major role in improving the overall performance of the algorithm [123, 158], especially in avoiding ID switches [83] or to re-identify targets after long occlusions [41]. In the latter case, simple motion predictors do not work since the linear motion assumption is easily broken, as noted by Zhou et al. [55].

关于特征提取，所有在所考虑的三个数据集上表现最好的方法都使用 CNN 来提取外观特征，其中 GoogLeNet 是最常见的方法。 不利用外观的方法(无论是用深度方法还是经典方法提取)往往表现更差。 然而，视觉特征还不够：许多最好的算法还使用其他类型的特征来计算亲和力，尤其是运动特征。 事实上，像 LSTM 和卡尔曼滤波器这样的算法经常被用来预测下一帧中目标的位置，这通常有助于提高关联的质量。 各种贝叶斯滤波算法，如粒子滤波和假设密度滤波，也被用于预测目标运动，它们受益于深度模型的使用[158、145、98]。 尽管如此，即使与非视觉特征一起使用，外观仍然在提高算法的整体性能方面发挥着重要作用 [123, 158]，特别是在避免 ID 切换 [83] 或长时间遮挡后重新识别目标方面 [ 41]。 在后一种情况下，简单的运动预测器不起作用，因为线性运动假设很容易被打破，正如 Zhou et al. 所指出的那样。 [55]。

19We remind the reader that the FPS reported by many algorithms tend to exclude the detection step, that can easily be the most computationally expensive part of the algorithm.  
19我们提醒读者，许多算法报告的 FPS 倾向于排除检测步骤，这很容易成为算法中计算量最大的部分。

Figure 9: Above: Public detections (generated using DPM v5 [26]) and private detections (obtained by [38] with a customized Faster R-CNN trained on multiple datasets) for frame 70 of MOT16-08 sequence. It can be observed that the man in the foreground is correctly detected by the custom Faster R-CNN detector (b), while it is ignored by DPM (a). Below: Results of tracking for the two detection sets using the SORT [35] algorithm, whose performance is heavily dependent on detection errors. We can indeed see that the mentioned missing detection produces a corresponding false negative in the tracking output (c), while in (d) the man is correctly tracked. 
图 9：上图：MOT16-08 序列第 70 帧的公共检测(使用 DPM v5 [26] 生成)和私有检测(由 [38] 使用在多个数据集上训练的自定义 Faster R-CNN 获得)。 可以观察到前景中的人被自定义 Faster R-CNN 检测器 (b) 正确检测到，而被 DPM (a) 忽略。 下图：使用 SORT [35] 算法跟踪两个检测集的结果，其性能在很大程度上取决于检测错误。 我们确实可以看到，提到的缺失检测在跟踪输出 (c) 中产生了相应的假阴性，而在 (d) 中，该人被正确跟踪。


While deep learning plays an important role in detection and feature extraction, the use of deep networks to learn affinity functions is less ubiquitous and has not yet been proven to be essential for a good MOT algorithm. Many algorithms in fact rely on a combination of hand-crafted distance metrics on a variety of deep and non-deep features. However, some works have already demonstrated how using affinity networks can produce top-performing algorithms [148, 145, 51, 96], with approaches ranging from the use of Siamese CNNs to recurrent neural networks. In particular, the adapted Siamese network proposed by Ma et al. [148] was able to produce reliable similarity measures that helped with the person re-identification after occlusions and allowed the algorithm to reach the highest MOTA score on MOT16. The integration of body part information was also crucial for the StackNetPose CNN proposed by Tang et al. [51]: it served as an attention mechanism that allowed the network to focus on the relevant parts of the input images, thus producing more accurate similarity measures. The algorithm was able to reach top performance on MOT16 using private detections.

虽然深度学习在检测和特征提取中起着重要作用，但使用深度网络学习亲和函数的情况并不普遍，而且尚未被证明对于良好的 MOT算法 至关重要。 许多算法实际上依赖于各种深度和非深度特征的手工距离度量的组合。 然而，一些工作已经展示了使用亲和网络如何产生性能最好的算法 [148、145、51、96]，方法从使用 Siamese CNN 到循环神经网络。 特别是，Ma et al. 提出的适应孪生网络。 [148] 能够产生可靠的相似性度量，有助于在遮挡后重新识别人员，并使算法在 MOT16 上达到最高的 MOTA 分数。 身体部位信息的整合对于 Tang et al. 提出的 StackNetPose CNN 也至关重要。 [51]：它作为一种注意力机制，允许网络关注输入图像的相关部分，从而产生更准确的相似性度量。 该算法能够使用私有检测在 MOT16 上达到最佳性能。

Very few works have instead explored the use deep learning models to guide the association process, and this could be a interesting research direction for future approaches.

很少有工作反而探索使用深度学习模型来指导关联过程，这可能是未来方法的一个有趣的研究方向。

Other trends in top-performing algorithms 顶级算法的其他趋势

Some other trends can be identified among current top ranked methods. For example, a successful approach in online methods is the use of Single Object Tracking algorithms, properly modified in order to solve the MOT task. Some of the top performing online algorithms on the 3 datasets have in fact employed a SOT tracker augmented with deep learning techniques to recover from occlusions or to refresh the target models [159, 115, 123]. Interestingly, to the best of our knowledge, no adapted SOT algorithm has been used to perform tracking with private detections. As we have already observed, the use of private detections reduces the number of completely uncovered targets; since SOT trackers don’t usually need detections to keep following a target once it’s been identified, the reduction in uncovered targets might translate in a much lower number of lost tracks, that in turn would enhance the overall performance of tracker. The application of a SOT tracker on private detections could thus be a good research direction to try to further improve the results on the MOTChallenge datasets. A batch method could also exploit a SOT tracker to look at past frames in order to recover missed detections before the target was first identified by the detector. However, SOT-based MOT trackers can sometimes still be prone to tracking drift and produce a higher number of ID switches. For example, the KCF16 algorithm [159], while reaching top MOTA score among online methods on MOT16 on public detections, it still produces a relatively high number of switches due to tracker drift, as can be seen in figure 10. Moreover, SOT-based MOT algorithms must be careful not to keep tracking spurious trajectories, caused by the inevitably higher number of false positive detections predicted by higher-quality detectors, for too many frames, as this might offset the reduction in the number of FNs. Current approaches [159, 115] still tend to use detection overlap (e.g. in how many recent frames the trajectory is covered by a detection) to understand if a trajectory is a true or a false positive in the long run, but better solutions should be investigated to avoid exclusive reliance on detections.

在当前排名靠前的方法中可以确定一些其他趋势。 例如，在线方法中的一个成功方法是使用单目标跟踪算法，适当修改以解决 MOT 任务。 一些在 3 个数据集上表现最好的在线算法实际上使用了一个 SOT 跟踪器，该跟踪器增强了深度学习技术，以从遮挡中恢复或刷新目标模型 [159、115、123]。 有趣的是，据我们所知，没有任何经过修改的 SOT 算法被用于执行私密检测跟踪。 正如我们已经观察到的，使用私人检测减少了完全未被发现的目标的数量;  由于 SOT 跟踪器通常不需要检测来继续跟踪目标，一旦目标被识别，未发现目标的减少可能会转化为更少的丢失跟踪数量，这反过来会提高跟踪器的整体性能。 因此，将 SOT 跟踪器应用于私人检测可能是一个很好的研究方向，试图进一步改进 MOTChallenge 数据集的结果。 批处理方法还可以利用 SOT 跟踪器查看过去的帧，以便在检测器首次识别目标之前恢复遗漏的检测。 然而，基于 SOT 的 MOT 跟踪器有时仍然容易出现跟踪漂移并产生更多的 ID 开关。 例如，KCF16 算法 [159]，虽然在公开检测的 MOT16 在线方法中达到了最高的 MOTA 分数，但由于跟踪器漂移，它仍然产生相对较多的切换次数，如图 10 所示。此外，SOT- 基于 MOT 的算法必须注意不要继续跟踪虚假轨迹，这是由更高质量的检测器预测的不可避免的更高数量的误报检测导致的，对于太多的帧，因为这可能会抵消 FN 数量的减少。 当前的方法 [159、115] 仍然倾向于使用检测重叠(例如，检测覆盖轨迹的最近帧数)来了解轨迹从长远来看是真还是假阳性，但更好的解决方案应该是 进行调查以避免完全依赖检测。

Figure 10: Example of SOT drift in the context of a MOT algorithm (KFC16 [159]). The four images are cropped from the MOT16-07 video, and are best viewed in color, since each color represents a different target ID. At first (a) the three persons are tracked. After a few frames (b) the red box starts drifting towards the occluded man, while the light blue box starts drifting towards the foreground woman. In (c) the white track is interrupted and the first two ID switches are completed. In (d) a new identity is assigned to the woman in the background, causing a third ID switch. 
图 10：MOT算法 背景下的 SOT 漂移样本(KFC16 [159])。 这四张图片是从 MOT16-07 视频中裁剪下来的，最好用彩色观看，因为每种颜色代表不同的目标 ID。 首先 (a) 这三个人被跟踪。 几帧后 (b)，红色框开始向被遮挡的男人漂移，而浅蓝色框开始向前景女人漂移。 在 (c) 中，白色轨道被中断，前两个 ID 开关完成。 在 (d) 中，一个新的身份被分配给背景中的女人，导致第三次身份转换。


While many methods perform association by formulating the task as a graph optimization problem, batch methods benefit in particular from this, since they can perform global optimization on them. For example, the minimum cost lifted multicut problem has reached top performance on MOT16, helped by CNN-computed affinities [148, 51], while heterogeneous association graph fusion and correlation clustering are used on the two top MOT17 methods [103, 155].

虽然许多方法通过将任务表述为图形优化问题来执行关联，但批处理方法尤其受益于此，因为它们可以对其执行全局优化。 例如，在 CNN 计算的亲和力 [148、51] 的帮助下，最小成本提升多切割问题在 MOT16 上达到了最佳性能，而异构关联图融合和相关聚类被用于两个顶级 MOT17 方法 [103、155]。

Finally, it can be noticed that the accuracy of bounding boxes radically affects the final performance of the algorithms. In fact, the top ranked tracker on MOT15 [89] obtained a relatively high MOTA score by just performing a bounding box regression step on the output of a previous state-of-the-art algorithm [89] using a deep RL agent. Developing an effective bounding box regressor to be incorporated in future MOT algorithms could be an interesting research direction that has not yet been explored thoroughly. Moreover, instead of relying on a single frame to fix the boxes, that could make them enclose the wrong target in case of an occlusion, batch methods could also try to exploit future and past target appearance to more accurately regress the bounding boxes around the right target. 

最后，可以注意到边框的准确性从根本上影响算法的最终性能。 事实上，MOT15 [89] 上排名最高的跟踪器仅通过使用深度 RL 智能体对先前最先进算法 [89] 的输出执行边框回归步骤就获得了相对较高的 MOTA 分数。 开发一个有效的边框回归器以纳入未来的 MOT算法 可能是一个有趣的研究方向，但尚未得到彻底探索。 此外，与其依靠单个框架来固定框，这可能会使它们在发生遮挡的情况下包围错误的目标，批处理方法还可以尝试利用未来和过去的目标外观来更准确地回归右侧周围的边框 目标。

## 5 Conclusion and future directions
We have presented a comprehensive description of all MOT algorithms employing deep learning techniques, focusing on single-camera videos and 2D data. Four main steps have been shown to characterize a generic MOT pipeline: detection, feature extraction, affinity computation, association. The use of deep learning in each of these four steps has been explored. While most of the approaches have focused on the first two, some applications of deep learning to learn affinity functions are also present, but only very few approaches use deep learning to directly guide the association algorithm. 

我们全面描述了所有采用深度学习技术的 MOT算法 ，重点关注单摄像头视频和 2D 数据。 已显示四个主要步骤来表征通用 MOT 管道：检测、特征提取、亲和力计算、关联。 已经探索了深度学习在这四个步骤中的每一个步骤中的使用。 虽然大多数方法都集中在前两者上，但也有一些深度学习在学习亲和函数方面的应用，但只有极少数方法使用深度学习直接指导关联算法。

A numerical comparison of the results on the MOTChallenge datasets has also been provided, showing that, despite the wide variety of approaches, some commonalities can be found among the presented methods: 
* detection quality is important: the amount of false negatives still dominates the MOTA score. While deep learning has allowed for some improvement in this regard for algorithms employing public detections, the use of higher quality detections is still the most effective way to reduce false negatives. Thus, a careful use of deep learning in the detection step can considerably improve the performance of a tracking algorithm; 
* CNNs are essential in feature extraction: the use of appearance features is also fundamental for a good tracker and CNNs are particularly effective at extracting them. Moreover, strong trackers tend to use them in conjunction with motion features, that can be computed using LSTMs, Kalman filter or other Bayesian filters; 
* SOT trackers and global graph optimization work: the adaptation of SOT trackers to the MOT task, with the help of deep learning, has recently produced good-performing online trackers; batch methods have instead benefited from the integration of deep models in global graph optimization algorithms.

还提供了 MOTChallenge 数据集结果的数值比较，表明尽管方法多种多样，但在所提出的方法中可以找到一些共性：
* 检测质量很重要：假阴性的数量仍然主导着 MOTA 分数。 虽然深度学习允许使用公共检测的算法在这方面有所改进，但使用更高质量的检测仍然是减少漏报的最有效方法。 因此，在检测步骤中仔细使用深度学习可以显著提高跟踪算法的性能; 
* CNN 在特征提取中必不可少：外观特征的使用对于一个好的跟踪器也是至关重要的，而 CNN 在提取它们方面特别有效。 此外，强大的跟踪器倾向于将它们与运动特征结合使用，这些运动特征可以使用 LSTM、卡尔曼滤波器或其他贝叶斯滤波器计算; 
* SOT trackers and global graph optimization work: SOT trackers to the MOT task的适应，在深度学习的帮助下，最近产生了性能良好的在线跟踪器;  批处理方法反而受益于深度模型在全局图优化算法中的集成。

As deep learning has been introduced only recently in the field of MOT, a number of promising future research directions have also been identified: 
* researching more strategies to mitigate detection errors: although modern detectors are constantly reaching better and better performances, they are still prone to produce a significant number of false negatives and false positives in complex scenarios such as dense pedestrian tracking. Some algorithms have provided solutions to reduce the exclusive reliance on detections by integrating them with information extracted from other sources (e.g. superpixels [103], R-FCN [118], Particle Filter [145], etc.), but further strategies should be investigated; 
* applying DL to track different targets: most of DL-based MOT algorithms have focused on pedestrian tracking. Since different types of targets pose different challenges, possible improvements in tracking vehicles, animals, or other objects with the use of deep networks should be investigated; 
* investigating the robustness of current algorithms: how do current methods perform under different camera conditions? How do a varying contrast, illumination, the presence of noisy/missing frames affect the result of current algorithms? Are existing DL networks able to generalize to different tracking contexts? For example, the vast majority of people tracking frameworks are trained to follow pedestrians or athletes, but tracking could be useful in other scenarios. A possible new application could be helping with scene understanding in different contexts: inside movies, in order to generate textual descriptions to provide a coarse way of searching for a scene in a movie; or on social networks, in order to generate descriptions for blind users or to detect inappropriate videos that should be removed from the platform. These different scenarios would probably require changes to the current detection and tracking algorithms, since the people could appear in unusual poses and behaviors that are not present in the existing datasets for MOT; 
* applying DL to guide association: the use of deep learning to guide the association algorithm and to directly perform tracking is still in its infancy: more research is needed in this direction to understand if deep algorithms can be useful in this step too; 
* combining SOT trackers with private detections: a possible way to reduce the number of lost tracks, and thus reduce the false negatives, could be the combination of SOT trackers with private detections, especially in a batch setting, where it would be possible to recover past detections that were previously missed; 
* investigating bounding box regression: the use of bounding box regression has been shown to be a promising step in obtaining a higher MOTA score, but this has not yet been explored in detail and further improvements should be investigated, e.g. the use of past and future information to guide the regression; 
* investigating post-tracking processing: in batch contexts, it is possible to apply correction algorithms on the output of a tracker to increase its performance. This has already been shown by Babaee et al. [132], that have applied occlusion handling on top of existing algorithms, and by Jiang et al. [152] with the aforementioned bounding box regression step. More complex processing could be applied on the results from a tracker to further improve the results.

由于最近才在 MOT 领域引入深度学习，因此也确定了一些有前途的未来研究方向：
* 研究更多减少检测错误的策略：虽然现代检测器的性能不断提高，但在密集的行人跟踪等复杂场景中，它们仍然容易产生大量的漏报和漏报。 一些算法通过将检测与从其他来源(例如超像素 [103]、R-FCN [118]、粒子滤波器 [145] 等)提取的信息相结合，提供了减少对检测的排他性依赖的解决方案，但进一步的策略应该是 调查; 
* 应用 DL 跟踪不同的目标：大多数基于 DL 的 MOT算法 都专注于行人跟踪。 由于不同类型的目标带来不同的挑战，因此应研究使用深度网络跟踪车辆、动物或其他物体的可能改进; 
* 调查当前算法的稳健性：当前方法在不同相机条件下的表现如何？ 变化的对比度、光照、噪声/缺失帧的存在如何影响当前算法的结果？ 现有的 DL 网络是否能够泛化到不同的跟踪上下文？ 例如，绝大多数人员跟踪框架都经过训练可以跟踪行人或运动员，但跟踪在其他情况下可能会有用。 一个可能的新应用程序可能有助于理解不同上下文中的场景：在电影中，以生成文本描述以提供一种粗略的方式来搜索电影中的场景;  或在社交网络上，以便为盲人用户生成描述或检测应从平台中删除的不当视频。 这些不同的场景可能需要改变当前的检测和跟踪算法，因为人们可能会出现不寻常的姿势和行为，而这些姿势和行为在现有的 MOT 数据集中是不存在的; 
* 应用深度学习来指导关联：使用深度学习来指导关联算法并直接执行跟踪仍处于起步阶段：需要在这个方向上进行更多研究，以了解深度算法是否也可以用于此步骤; 
* 将 SOT 跟踪器与私人检测相结合：一种减少丢失轨道数量的可能方法，从而减少漏报，可能是将 SOT 跟踪器与私人检测相结合，特别是在批处理设置中，可以恢复 以前错过的过去检测; 
* 调查边框回归：边框回归的使用已被证明是获得更高 MOTA 分数的有前途的一步，但这尚未得到详细探索，应该研究进一步的改进，例如 使用过去和未来的信息来指导回归; 
* 调查跟踪后处理：在批处理上下文中，可以对跟踪器的输出应用校正算法以提高其性能。 Babaee et al. 已经证明了这一点。 [132]，在现有算法之上应用遮挡处理，Jiang et al. [152] 与上述边框回归步骤。 可以对来自跟踪器的结果应用更复杂的处理以进一步改进结果。

Finally, as very few of the presented algorithms have provided public access to their source code, we would like to encourage future researchers to publish their code in order to allow for better reproducibility of their results and benefit the whole research community. 29

最后，由于所提出的算法中很少有提供对其源代码的公共访问权限，我们希望鼓励未来的研究人员发布他们的代码，以便更好地重现他们的结果并使整个研究社区受益。


## Acknowledgements
This research work is partially supported by the Spanish Ministry of Science and Technology under the project TIN2017- 89517-P and the project DeepSCOP-Ayudas Fundación BBVA a Equipos de Investigación Científica en Big Data 2018. Siham Tabik was supported by the Ramon y Cajal Programme (RYC-2015-18136).

这项研究工作得到西班牙科技部 TIN2017-89517-P 项目和 DeepSCOP-Ayudas Fundación BBVA a Equipos de Investigación Científica en Big Data 2018 项目的部分支持。Siham Tabik 得到了 Ramon y Cajal 计划的支持 (RYC-2015-18136)。

## References
1. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.
2. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1–9, 2015.
3. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
4. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91–99, 2015.
5. Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21–37. Springer, 2016.
6. Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7263–7271, 2017.
7. Ha¸sim Sak, Andrew Senior, and Françoise Beaufays. Long short-term memory recurrent neural network architectures for large scale acoustic modeling. In Fifteenth annual conference of the international speech communication association, 2014.
8. Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. Lstm neural networks for language modeling. In Thirteenth annual conference of the international speech communication association, 2012.
9. Yuchen Fan, Yao Qian, Feng-Long Xie, and Frank K Soong. Tts synthesis with bidirectional lstm based recurrent neural networks. In Fifteenth Annual Conference of the International Speech Communication Association, 2014.
10. Erik Marchi, Giacomo Ferroni, Florian Eyben, Leonardo Gabrielli, Stefano Squartini, and Björn Schuller. Multi-resolution linear prediction based features for audio onset detection with bidirectional lstm neural networks. In 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 2164–2168. IEEE, 2014.
11. Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Xiaowei Zhao, and Tae-Kyun Kim. Multiple object tracking: A literature review. arXiv preprint arXiv:1409.7618, 2014.
12. Massimo Camplani, Adeline Paiement, Majid Mirmehdi, Dima Damen, Sion Hannuna, Tilo Burghardt, and Lili Tao. Multiple human tracking in rgb-depth data: a survey. IET computer vision, 11(4):265–285, 2016.
13. Patrick Emami, Panos M Pardalos, Lily Elefteriadou, and Sanjay Ranka. Machine learning methods for solving assignment problems in multi-target tracking. arXiv preprint arXiv:1802.06897, 2018.
14. Laura Leal-Taixé, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, and Stefan Roth. Tracking the trackers: An analysis of the state of the art in multiple object tracking. arXiv preprint arXiv:1704.02781, 2017.
15. Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler. Motchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint arXiv:1504.01942, 2015.
16. Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler. Mot16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831, 2016.
17. Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961–2969, 2017.
18. Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. In Advances in neural information processing systems, pages 379–387, 2016.
19. Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans based on static body part detection. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), volume 1, pages 951–958. IEEE, 2006. 30 
20. Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. Journal on Image and Video Processing, 2008:1, 2008.
21. Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European Conference on Computer Vision, pages 17–35. Springer, 2016.
22. Rainer Stiefelhagen and John Garofolo. Multimodal Technologies for Perception of Humans: First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006, Southampton, UK, April 6-7, 2006, Revised Selected Papers, volume 4122. Springer, 2007.
23. Rainer Stiefelhagen, Rachel Bowers, and Jonathan Fiscus. Multimodal Technologies for Perception of Humans: International Evaluation Workshops CLEAR 2007 and RT 2007, Baltimore, MD, USA, May 8-11, 2007, Revised Selected Papers, volume 4625. Springer, 2008.
24. Piotr Dollár, Ron Appel, Serge Belongie, and Pietro Perona. Fast feature pyramids for object detection. IEEE transactions on pattern analysis and machine intelligence, 36(8):1532–1545, 2014.
25. Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan. Object detection with discriminatively trained part-based models. IEEE transactions on pattern analysis and machine intelligence, 32(9):1627–1645, 2009.
26. Ross B. Girshick, Pedro F. Felzenszwalb, and David McAllester. Discriminatively trained deformable part models, release 5. http://people.cs.uchicago.edu/~rbg/latent-release5/, 2012.
27. Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2129–2137, 2016.
28. Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixe. Cvpr19 tracking and detection challenge: How crowded can it get?, 2019.
29. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354–3361. IEEE, 2012.
30. Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231–1237, 2013.
31. Xiaoyu Wang, Ming Yang, Shenghuo Zhu, and Yuanqing Lin. Regionlets for generic object detection. In Proceedings of the IEEE international conference on computer vision, pages 17–24, 2013.
32. Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang Qi, Jongwoo Lim, Ming-Hsuan Yang, and Siwei Lyu. Ua-detrac: A new benchmark and protocol for multi-object detection and tracking. arXiv preprint arXiv:1511.04136, 2015.
33. Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele. Monocular 3d pose estimation and tracking by detection. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 623–630. IEEE, 2010.
34. James Ferryman and Ali Shahrokni. Pets2009: Dataset and challenge. In 2009 Twelfth IEEE International Workshop on Performance Evaluation of Tracking and Surveillance, pages 1–6. IEEE, 2009.
35. Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In 2016 IEEE International Conference on Image Processing (ICIP), pages 3464–3468. IEEE, 2016.
36. Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. Journal of basic Engineering, 82(1):35–45, 1960.
37. Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83–97, 1955.
38. Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi, and Junjie Yan. Poi: Multiple object tracking with high performance detection and appearance feature. In European Conference on Computer Vision, pages 36–42. Springer, 2016.
39. Sean Bell, C Lawrence Zitnick, Kavita Bala, and Ross Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2874–2883, 2016.
40. Spyros Gidaris and Nikos Komodakis. Object detection via a multi-region and semantic segmentation-aware cnn model. In Proceedings of the IEEE International Conference on Computer Vision, pages 1134–1142, 2015. 31 
41. Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In 2017 IEEE International Conference on Image Processing (ICIP), pages 3645–3649. IEEE, 2017.
42. Nima Mahmoudi, Seyed Mohammad Ahadi, and Mohammad Rahmati. Multi-target tracking using cnn-based features: Cnnmtt. Multimedia Tools and Applications, 78(6):7077–7096, 2019.
43. Xingyu Wan, Jinjun Wang, and Sanping Zhou. An online and flexible multi-object tracking framework using long short-term memory. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1230–1238, 2018.
44. Takayuki Ujiie, Masayuki Hiromoto, and Takashi Sato. Interpolation-based object detection using motion vectors for embedded real-time tracking systems. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 616–624, 2018.
45. Qizheng He, Jianan Wu, Gang Yu, and Chi Zhang. Sot for mot. arXiv preprint arXiv:1712.01059, 2017.
46. Minghua Li, Zhengxi Liu, Yunyu Xiong, and Zheng Li. Multi-person tracking by discriminative affinity model and hierarchical association. In 2017 3rd IEEE International Conference on Computer and Communications (ICCC), pages 1741–1745. IEEE, 2017.
47. Wenbo Li, Ming-Ching Chang, and Siwei Lyu. Who did what at where and when: Simultaneous multi-person tracking and activity recognition. arXiv preprint arXiv:1807.01253, 2018.
48. Felipe Jorquera, Sergio Hernández, and Diego Vergara. Probability hypothesis density filter using determinantal point processes for multi object tracking. Computer Vision and Image Understanding, 2019.
49. Zhao Zhong, Zichen Yang, Weitao Feng, Wei Wu, Yangyang Hu, and Cheng-lin Liu. Decision controller for object tracking with deep reinforcement learning. IEEE Access, 2019.
50. Weigang Lu, Zhiping Zhou, Lijuan Zhang, and Guoqiang Zheng. Multi-target tracking by non-linear motion patterns based on hierarchical network flows. Multimedia Systems, pages 1–12, 2019.
51. Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele. Multiple people tracking by lifted multicut and person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3539–3548, 2017.
52. Nan Ran, Longteng Kong, Yunhong Wang, and Qingjie Liu. A robust multi-athlete tracking algorithm by exploiting discriminant features and long-term dependencies. In International Conference on Multimedia Modeling, pages 411–423. Springer, 2019.
53. Haigen Hu, Lili Zhou, Qiu Guan, Qianwei Zhou, and Shengyong Chen. An automatic tracking method for multiple cells based on multi-feature fusion. IEEE Access, 6:69782–69793, 2018.
54. Lei Zhang, Helen Gray, Xujiong Ye, Lisa Collins, and Nigel Allinson. Automatic individual pig detection and tracking in pig farms. Sensors, 19(5):1188, 2019.
55. Zongwei Zhou, Junliang Xing, Mengdan Zhang, and Weiming Hu. Online multi-target tracking with tensorbased high-order graph matching. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 1809–1814. IEEE, 2018.
56. Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Eco: efficient convolution operators for tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6638–6646, 2017.
57. Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In international Conference on computer vision & Pattern Recognition (CVPR’05), volume 1, pages 886–893. IEEE Computer Society, 2005.
58. Joost Van De Weijer, Cordelia Schmid, Jakob Verbeek, and Diane Larlus. Learning color names for real-world applications. IEEE Transactions on Image Processing, 18(7):1512–1523, 2009.
59. Yongyi Lu, Cewu Lu, and Chi-Keung Tang. Online video object detection using association lstm. In Proceedings of the IEEE International Conference on Computer Vision, pages 2344–2352, 2017.
60. Hilke Kieritz, Wolfgang Hubner, and Michael Arens. Joint detection and online multi-object tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1459–1467, 2018.
61. Dawei Zhao, Hao Fu, Liang Xiao, Tao Wu, and Bin Dai. Multi-object tracking with correlation filter for autonomous vehicle. Sensors, 18(7):2004, 2018.
62. Karl Pearson. Liii. on lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559–572, 1901. 32 
63. Mengmeng Wang, Yong Liu, and Zeyi Huang. Large margin object tracking with circulant feature maps. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4021–4029, 2017.
64. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779–788, 2016.
65. Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767, 2018.
66. Sang Jun Kim, Jae-Yeal Nam, and Byoung Chul Ko. Online tracker optimization for multi-pedestrian tracking using a moving vehicle camera. IEEE Access, 6:48675–48687, 2018.
67. Sarthak Sharma, Junaid Ahmed Ansari, J Krishna Murthy, and K Madhava Krishna. Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking. In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages 3508–3515. IEEE, 2018.
68. Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan, Yu-Wing Tai, and Li Xu. Accurate single stage detector using recurrent rolling convolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5420–5428, 2017.
69. Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Subcategory-aware convolutional neural networks for object proposals and detection. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 924–933. IEEE, 2017.
70. Federico Pernici, Federico Bartoli, Matteo Bruni, and Alberto Del Bimbo. Memory based online learning of deep representations from video streams. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2324–2334, 2018.
71. Peiyun Hu and Deva Ramanan. Finding tiny faces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 951–959, 2017.
72. Weidong Min, Mengdan Fan, Xiaoguang Guo, and Qing Han. A new approach to track multiple vehicles with the combination of robust detection and two classifiers. IEEE Transactions on Intelligent Transportation Systems, 19(1):174–186, 2018.
73. Olivier Barnich and Marc Van Droogenbroeck. Vibe: A universal background subtraction algorithm for video sequences. IEEE Transactions on Image processing, 20(6):1709–1724, 2011.
74. Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning, 20(3):273–297, 1995.
75. Shaoyong Yu, Yun Wu, Wei Li, Zhijun Song, and Wenhua Zeng. A model for fine-grained vehicle classification based on deep learning. Neurocomputing, 257:97–103, 2017.
76. Sebastian Bullinger, Christoph Bodensteiner, and Michael Arens. Instance flow based online multiple object tracking. In 2017 IEEE International Conference on Image Processing (ICIP), pages 785–789. IEEE, 2017.
77. Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic segmentation via multi-task network cascades. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3150–3158, 2016.
78. Gunnar Farnebäck. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Image analysis, pages 363–370. Springer, 2003.
79. Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. Deepmatching: Hierarchical deformable dense matching. International Journal of Computer Vision, 120(3):300–323, 2016.
80. Yinlin Hu, Rui Song, and Yunsong Li. Efficient coarse-to-fine patchmatch for large displacement optical flow. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5704–5712, 2016.
81. Li Wang, Nam Trung Pham, Tian-Tsong Ng, Gang Wang, Kap Luk Chan, and Karianto Leman. Learning deep features for multiple object tracking by using a multi-task learning strategy. In 2014 IEEE International Conference on Image Processing (ICIP), pages 838–842. IEEE, 2014.
82. Charles Cadieu and Bruno A Olshausen. Learning transformational invariants from natural movies. In Advances in neural information processing systems, pages 209–216, 2009.
83. Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M Rehg. Multiple hypothesis tracking revisited. In Proceedings of the IEEE International Conference on Computer Vision, pages 4696–4704, 2015.
84. Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, and Qi Tian. Person reidentification in the wild. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1367–1376, 2017. 33 
85. Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person reidentification: A benchmark. In Proceedings of the IEEE International Conference on Computer Vision, pages 1116–1124, 2015.
86. Douglas Gray and Hai Tao. Viewpoint invariant pedestrian recognition with an ensemble of localized features. In European conference on computer vision, pages 262–275. Springer, 2008.
87. Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter pairing neural network for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 152–159, 2014.
88. Jiahui Chen, Hao Sheng, Yang Zhang, and Zhang Xiong. Enhancing detection model for multiple hypothesis tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 18–27, 2017.
89. Min Yang, Yuwei Wu, and Yunde Jia. A hybrid data association framework for robust online multi-object tracking. IEEE Transactions on Image Processing, 26(12):5667–5679, 2017.
90. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based convolutional networks for accurate object detection and segmentation. IEEE transactions on pattern analysis and machine intelligence, 38(1):142–158, 2015.
91. Shuo Hong Wang, Jing Wen Zhao, and Yan Qiu Chen. Robust tracking of fish schools using cnn for head identification. Multimedia Tools and Applications, 76(22):23679–23697, 2017.
92. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.
93. Chanho Kim, Fuxin Li, and James M Rehg. Multi-object tracking with neural gating using bilinear lstm. In Proceedings of the European Conference on Computer Vision (ECCV), pages 200–215, 2018.
94. Seung-Hwan Bae and Kuk-Jin Yoon. Confidence-based data association and discriminative deep appearance learning for robust online multi-object tracking. IEEE transactions on pattern analysis and machine intelligence, 40(3):595–610, 2017.
95. Mohib Ullah and Faouzi Alaya Cheikh. Deep feature based end-to-end transportation network for multi-target tracking. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 3738–3742. IEEE, 2018.
96. Kuan Fang, Yu Xiang, Xiaocheng Li, and Silvio Savarese. Recurrent autoregressive networks for online multiobject tracking. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 466–475. IEEE, 2018.
97. Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang Wang. Learning deep feature representations with domain guided dropout for person re-identification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1249–1258, 2016.
98. Zeyu Fu, Federico Angelini, Syed Mohsen Naqvi, and Jonathon A Chambers. Gm-phd filter based online multiple human tracking using deep discriminative correlation matching. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4299–4303. IEEE, 2018.
99. B-N Vo and W-K Ma. The gaussian mixture probability hypothesis density filter. IEEE Transactions on signal processing, 54(11):4091–4104, 2006.
100. Longyin Wen, Dawei Du, Shengkun Li, Xiao Bian, and Siwei Lyu. Learning non-uniform hypergraph for multi-object tracking. Thirty-Third AAAI Conference on Artificial Intelligence, 2019.
101. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3):211–252, 2015.
102. Flip Korn and Suresh Muthukrishnan. Influence sets based on reverse nearest neighbor queries. ACM Sigmod Record, 29(2):201–212, 2000.
103. Hao Sheng, Yang Zhang, Jiahui Chen, Zhang Xiong, and Jun Zhang. Heterogeneous association graph fusion for target association in multiple object tracking. IEEE Transactions on Circuits and Systems for Video Technology, 2018.
104. Longtao Chen, Xiaojiang Peng, and Mingwu Ren. Recurrent metric networks and batch multiple hypothesis for multi-object tracking. IEEE Access, 7:3093–3105, 2019.
105. Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, Peter V Gehler, and Bernt Schiele. Deepcut: Joint subset partition and labeling for multi person pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4929–4937, 2016. 34 
106. Minyoung Kim, Stefano Alletto, and Luca Rigazio. Similarity mapping with enhanced siamese network for multiobject tracking. In Machine Learning for Intelligent Transportation Systems (MLITS), 2016 NIPS Workshop, 2016.
107. Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. Signature verification using a" siamese" time delay neural network. In Advances in neural information processing systems, pages 737–744, 1994.
108. Bing Wang, Li Wang, Bing Shuai, Zhen Zuo, Ting Liu, Kap Luk Chan, and Gang Wang. Joint learning of convolutional neural networks and temporally constrained metrics for tracklet association. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1–8, 2016.
109. Shun Zhang, Yihong Gong, Jia-Bin Huang, Jongwoo Lim, Jinjun Wang, Narendra Ahuja, and Ming-Hsuan Yang. Tracking persons-of-interest via adaptive discriminative features. In European conference on computer vision, pages 415–433. Springer, 2016.
110. Laura Leal-Taixé, Cristian Canton-Ferrer, and Konrad Schindler. Learning by tracking: Siamese cnn for robust target association. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 33–40, 2016.
111. Laura Leal-Taixé, Gerard Pons-Moll, and Bodo Rosenhahn. Everybody needs somebody: Modeling social and grouping behavior on a linear programming multiple people tracker. In 2011 IEEE international conference on computer vision workshops (ICCV workshops), pages 120–127. IEEE, 2011.
112. Jeany Son, Mooyeol Baek, Minsu Cho, and Bohyung Han. Multi-object tracking with quadruplet convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5620–5629, 2017.
113. Andrii Maksai and Pascal Fua. Eliminating exposure bias and loss-evaluation mismatch in multiple object tracking. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019.
114. Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737, 2017.
115. Ji Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang, and Ming-Hsuan Yang. Online multi-object tracking with dual matching attention networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 366–382, 2018.
116. Cong Ma, Changshui Yang, Fan Yang, Yueqing Zhuang, Ziwei Zhang, Huizhu Jia, and Xiaodong Xie. Trajectory factory: Tracklet cleaving and re-connection by deep siamese bi-gru for multiple object tracking. In 2018 IEEE International Conference on Multimedia and Expo (ICME), pages 1–6. IEEE, 2018.
117. Hui Zhou, Wanli Ouyang, Jian Cheng, Xiaogang Wang, and Hongsheng Li. Deep continuous conditional random fields with asymmetric inter-object constraints for online multi-object tracking. IEEE Transactions on Circuits and Systems for Video Technology, 2018.
118. Chen Long, Ai Haizhou, Zhuang Zijie, and Shang Chong. Real-time multiple people tracking with deeply learned candidate selection and person re-identification. In ICME, 2018.
119. Sangyun Lee and Euntai Kim. Multiple object tracking via feature pyramid siamese networks. IEEE Access, 7:8181–8194, 2019.
120. Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.
121. Mohib Ullah, Ahmed Kedir Mohammed, Faouzi Alaya Cheikh, and Zhaohui Wang. A hierarchical feature model for multi-target tracking. In 2017 IEEE International Conference on Image Processing (ICIP), pages 2612–2616. IEEE, 2017.
122. Stéphane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. IEEE Transactions on signal processing, 41(12):3397–3415, 1993.
123. Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Tracking the untrackable: Learning to track multiple cues with long-term dependencies. In Proceedings of the IEEE International Conference on Computer Vision, pages 300–311, 2017.
124. Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal attention mechanism. In Proceedings of the IEEE International Conference on Computer Vision, pages 4836–4845, 2017. 35 
125. Mustafa Ozuysal, Michael Calonder, Vincent Lepetit, and Pascal Fua. Fast keypoint recognition using random ferns. IEEE transactions on pattern analysis and machine intelligence, 32(3):448–461, 2009.
126. Mohib Ullah and Faouzi Alaya Cheikh. A directed sparse graphical model for multi-target tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1816–1823, 2018.
127. Lawrence R Rabiner and Biing-Hwang Juang. An introduction to hidden markov models. ieee assp magazine, 3(1):4–16, 1986.
128. Lu Wang, Lisheng Xu, Min Young Kim, Luca Rigazico, and Ming-Hsuan Yang. Online multiple object tracking via flow and convolutional features. In 2017 IEEE International Conference on Image Processing (ICIP), pages 3630–3634. IEEE, 2017.
129. Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical convolutional features for visual tracking. In Proceedings of the IEEE international conference on computer vision, pages 3074–3082, 2015.
130. Bruce D. Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In Proceedings of Imaging Understanding Workshop, pages 121–130. Vancouver, British Columbia, 1981.
131. Pol Rosello and Mykel J Kochenderfer. Multi-agent reinforcement learning for multi-object tracking. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pages 1397–1404. International Foundation for Autonomous Agents and Multiagent Systems, 2018.
132. Maryam Babaee, Zimu Li, and Gerhard Rigoll. Occlusion handling in tracking multiple people using rnn. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 2715–2719. IEEE, 2018.
133. Anton Milan, S Hamid Rezatofighi, Anthony Dick, Ian Reid, and Konrad Schindler. Online multi-target tracking using recurrent neural networks. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.
134. Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online multi-object tracking by decision making. In Proceedings of the IEEE international conference on computer vision, pages 4705–4713, 2015.
135. Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. Rmpe: Regional multi-person pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2334–2343, 2017.
136. Yiming Liang and Yue Zhou. Lstm multiple object tracker combining multiple cues. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 2351–2355. IEEE, 2018.
137. Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin, Yubing Li, Yihong Gong, and Nanning Zheng. Deep self-paced learning for person re-identification. Pattern Recognition, 76:739–751, 2018.
138. Kwangjin Yoon, Du Yong Kim, Young-Chul Yoon, and Moongu Jeon. Data association for multi-object tracking via deep neural networks. Sensors, 19(3):559, 2019.
139. Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Learning social etiquette: Human trajectory understanding in crowded scenes. In European conference on computer vision, pages 549–565. Springer, 2016.
140. Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele. People-tracking-by-detection and people-detection-bytracking. In 2008 IEEE Conference on computer vision and pattern recognition, pages 1–8. IEEE, 2008.
141. Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.
142. Bjoern Andres, Andrea Fuksová, and Jan-Hendrik Lange. Lifting of multicuts. CoRR, abs/1503.03791, 3, 2015.
143. Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid. Deepflow: Large displacement optical flow with deep matching. In Proceedings of the IEEE International Conference on Computer Vision, pages 1385–1392, 2013.
144. Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas Brox, and Bjorn Andres. Efficient decomposition of image and mesh graphs by lifted multicuts. In Proceedings of the IEEE International Conference on Computer Vision, pages 1751–1759, 2015.
145. Long Chen, Haizhou Ai, Chong Shang, Zijie Zhuang, and Bo Bai. Online multi-object tracking with convolutional neural networks. In 2017 IEEE International Conference on Image Processing (ICIP), pages 645–649. IEEE, 2017.
146. M Sanjeev Arulampalam, Simon Maskell, Neil Gordon, and Tim Clapp. A tutorial on particle filters for online nonlinear/non-gaussian bayesian tracking. IEEE Transactions on signal processing, 50(2):174–188, 2002.
147. Ricardo Sanchez-Matilla, Fabio Poiesi, and Andrea Cavallaro. Online multi-target tracking with strong and weak detections. In European Conference on Computer Vision, pages 84–99. Springer, 2016. 36 
148. Liqian Ma, Siyu Tang, Michael J. Black, and Luc Van Gool. Customized multi-person tracker. In Computer Vision – ACCV 2018. Springer International Publishing, December 2018.
149. Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele. Multi-person tracking by multicut and deep matching. In European Conference on Computer Vision, pages 100–111. Springer, 2016.
150. Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and Jie Zhou. Collaborative deep reinforcement learning for multi-object tracking. In Proceedings of the European Conference on Computer Vision (ECCV), pages 586–602, 2018.
151. Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural networks for visual tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4293–4302, 2016.
152. Yifan Jiang, Hyunhak Shin, and Hanseok Ko. Precise regression for bounding box correction for improved tracking based on deep reinforcement learning. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1643–1647. IEEE, 2018.
153. Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, Mi Young Nam, Young Giu Jung, and Phill Kyu Rhee. Multiclass multi-object tracking using changing point detection. In European Conference on Computer Vision, pages 68–83. Springer, 2016.
154. Anthony Hoak, Henry Medeiros, and Richard Povinelli. Image-based multi-target tracking through multibernoulli filtering with interactive likelihoods. Sensors, 17(3):501, 2017.
155. Roberto Henschel, Laura Leal-Taixé, Daniel Cremers, and Bodo Rosenhahn. Fusion of head and full-body detectors for multi-object tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1428–1437, 2018.
156. Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2325–2333, 2016.
157. Weihao Gan, Shuo Wang, Xuejing Lei, Ming-Sui Lee, and C-C Jay Kuo. Online cnn-based multiple object tracking with enhanced model updates and identity association. Signal Processing: Image Communication, 66:95–102, 2018.
158. Jun Xiang, Guoshuai Zhang, and Jianhua Hou. Online multi-object tracking based on feature representation and bayesian filtering within a deep learning architecture. IEEE Access, 2019.
159. Peng Chu, Heng Fan, Chiu C Tan, and Haibin Ling. Online multi-object tracking with instance-aware tracker and dynamic model refreshment. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 161–170. IEEE, 2019.
160. Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. PhD thesis, King’s College, Cambridge, 1989.
161. Jun-ichi Takeuchi and Kenji Yamanishi. A unifying framework for detecting outliers and change points from time series. IEEE transactions on Knowledge and Data Engineering, 18(4):482–492, 2006.
162. Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 304–311. IEEE, 2009.
163. Reza Hoseinnezhad, Ba-Ngu Vo, Ba-Tuong Vo, and David Suter. Visual tracking of numerous targets via multi-bernoulli filtering of image data. Pattern Recognition, 45(10):3625–3635, 2012.
164. Anton Milan, Rikke Gade, Anthony Dick, Thomas B Moeslund, and Ian Reid. Improving global multi-target tracking with local updates. In European Conference on Computer Vision, pages 174–190. Springer, 2014.
165. Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research logistics quarterly, 3(1-2):95–110, 1956.
166. Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7291–7299, 2017.
167. Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. Deeply-learned part-aligned representations for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, pages 3219–3228, 2017.
168. João F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized correlation filters. IEEE transactions on pattern analysis and machine intelligence, 37(3):583–596, 2014. 37 
169. Longyin Wen, Wenbo Li, Junjie Yan, Zhen Lei, Dong Yi, and Stan Z Li. Multiple target tracking based on undirected hierarchical relation hypergraph. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1282–1289, 2014.
170. Zhi-Ming Qian, Xi En Cheng, and Yan Qiu Chen. Automatically detect and track multiple fish swimming in shallow water with frequent occlusion. PloS one, 9(9):e106506, 2014.
171. Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal greedy algorithms for tracking a variable number of objects. In CVPR 2011, pages 1201–1208. IEEE, 2011.
172. Steven Gold, Anand Rangarajan, et al. Softmax to softassign: Neural network algorithms for combinatorial optimization. Journal of Artificial Neural Networks, 2(4):381–399, 1996.
173. Chang Huang, Bo Wu, and Ramakant Nevatia. Robust object tracking by hierarchical association of detection responses. In European Conference on Computer Vision, pages 788–801. Springer, 2008.
174. Rodrigo Benenson, Markus Mathias, Radu Timofte, and Luc Van Gool. Pedestrian detection at 100 frames per second. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2903–2910. IEEE, 2012. 38  2019.  

## A Appendix 
Here we present a table containing a summary of the techniques used by each algorithm presented in this paper. The table follows the order of presentation of the papers. Since we think that the publication of open source code can greatly help the research community, we have also provided links to the source codes for the papers that provide them. 

Detection Feature extr. / mot. pred. Affinity / cost computation Association / Tracking Mode Source and data
35. Faster R-CNN Kalman filter IoU Hungarian algorithm O Source
38. Modified Faster R-CNN Modified GoogLeNet, Kalman filter Cosine distance + IoU Hungarian algorithm (online), modified H2T [169] (batch) O+B Detections and appearance features
52. Faster R-CNN CNN (app.), AlphaPose CNN, pose joints velocities, interaction grid Pose-based Triple Stream Network (LSTM-based) Custom algorithm O
53. Faster R-CNN CNN Euclidean distance, cosine distance Multifeature fusion re-tracking algorithm B [54] CNN HOG + Colour Names Variation of Discriminative Correlation Filter Custom algorithm + Hungarian algorithm O [59] SSD SSD, LSTM Cosine similarity Hungarian algorithm O
60. SSD SSD RNN Hungarian algorithm, MLP (track scores) O [61] SSD SSD + Correlation Filter IoU + APCE Hungarian algorithm O
55. Public / Mask R-CNN Siamese Mask R-CNN App. affinity, mot. consistency, spatial structural potential Tensor-based high-order graph matching O Code will be released
66. YOLOv2 Tiny Yolo, Particle filter, Random Ferns, KLT Pairwise overlap ratio, student Random Ferns, Euclidean distance Greedy bipartite assignment O [67] RRC or SubCNN Feature-based odometry, Pose Adjustment CNN, stacked-hourglass CNN 3D-2D cost + 3D-3D cost + appearance, shape and pose costs Hungarian algorithm O Source
70. DPM or Tiny (CNN) DPM or Tiny (CNN) Implicit in Reverse Nearest Neighbour Reverse Nearest Neighbour Matching O Code will be released
72. ViBe + SVM + CNN IoU Region Matching algorithm O [76] Multi-task Network Cascades (CNN) Optical flow Overlap of segmentation instances Hungarian algorithm O
81. Dalal-Triggs detector Autoencoders SVM Minimum spanning tree O 39  2019.  
83. Public CNN + PCA Multi-Output Regularized Least Squares Variation of Multiple Hypothesis Tracking O Source
88. Public CNN, Kalman Filter Multi-Output Regularized Least Squares + Kalman Filter + detection-scene score Maximum Weighted Independent Set B [89] Public R-CNN Observation cost + transition cost + birth-death cost Min-cost multi commodity flow problem, solved with Dantzig-Wolfe decomposition O [91] DoH [170] CNN CNN + Kalman filter Custom algorithm, SVM B [41] From [38] Kalman filter, Wide Residual Net Mahalanobis dist. (mot.) + cosine distance (app.), IoU Hungarian algorithm O Source
42. From 38. CNN appearance + motion + dynamic affinity Hungarian algorithm O
93. Public CNN Bilinear LSTM Variant of MHT-DAM [83] B [94] Public / SDP+RPN CNN Appearance + motion + shape affinities Hungarian algorithm O Source
95. Public GoogLeNet CNN App. similarity Bayesian inference using [171] B [96] Public / Faster R-CNN GoogLeNet CNN Recurrent Autoregressive Networks (GRU-based) Bipartite graph matching O [98] Public CNN Hybrid Likelihood Function (Discriminative Correlation Filter + Gaussian Mixture Probability Hypothesis Density) Hungarian algorithm O
100. Public CNN app. + HSV histogram + motion similarities Pairwise update algorithm + SSVM B Will be available at this link
103. Public GoogLeNet CNN, Optical flow Distance between app. features, common superpixels, optical flow predictions Multiple Hypotheses Tracking B [104] Public CNN LSTM (app.) + motion affinity Batch Multi-Hypothesis B 40  2019.  
51. Public / From
38. DeepCut CNN [105], StackNetPose CNN StackNetPose CNN Lifted multicut problem, solved as in
144. B Source
106. Public Siamese CNN Euclidean distance (app. feat.) + IoU + box area ratio Custom greedy algorithm O [108] DPM Siamese CNN with temporal constraints Mahalanobis distance (app. feat.) + motion affinity Generalized Linear Assignment solved with Softassign
172., Dual-threshold strategy [173] B [109] HeadHunter
174. CNN Euclidean distance (app. feat.), temporal and kinematic affinities Hungarian algorithm, Agglomerative clustering B Source
110. Public Siamese CNN, contextual features Gradient Boosting Linear programming B
112. Public CNN, sequence-specific statistics, optical flow, FC layers FC layer combining app. and mot. distances Minimax label propagation B [113] Public CNN + various app. and non-app. feat. embedding layer + bidirectional LSTM Variation of Multiple Hypothesis Tracking B [115] Public Linear motion model, Spatial Attention Network CNN Temporal Attention Network (bidirectional LSTM) Custom greedy algorithm, ECO (SOT tracker) O Source
116. Public Siamese CNN, LSTM, WRN CNN, Siamese Bi-GRU + CNN Euclidean dist. (app. feat.), spatial distance, GRU feature matching Hungarian algorithm, bi-GRU RNN (track split), custom algorithm B [117] Public DCCRF, visual-displacement CNN Visual-similarity CNN, IoU Hungarian algorithm O
118. Public R-FCN + Kalman Filter, GoogleNet Eucl. dist. (app. feat.), IoU Hierarchical Data Association O Source
119. Public Feature Pyramid Siamese Network, motion features Feature Pyramid Siamese Network Custom greedy algorithm O [121] Public Kalman Filter, GoogLeNet Distance between sparse coding of features using a learned dictionary Hungarian algorithm B
123. Public 3 LSTMs (app., mot., interaction features) using CNN, bb velocity, occupancy map LSTM Hungarian algorithm, SOT tracker [134] O 2019.  
124. Public Linear motion model, CNN CNN Association to highest classification score O [126] Manually generated Hidden Markov Models, CNN Mutual information (app. feat.) Dynamic programming algorithm from [171] B [128] Public LK optical flow, Convolutional Correlation Filter CNN, Kalman filter Optical flow aff., app. feat. aff., IoU, scale affinity, distance between detections Custom algorithm (with Hungarian alg.) O Source
131. Public Kalman filter + Deep RL agent IoU Hungarian algorithm + Deep RL agent O [132] N/A LSTM (mot.) Stitching score using IoU Custom iterative tracklet-stitching algorithm B [133] Public RNN (mot.) LSTM RNN O Source
136. Public 2 LSTMs, VGG16 CNN SVM, Siamese LSTM Greedy association B
43. From 38. Kalman filter or LK optical flow, CNN + motion features IoU, Siamese LSTM Hungarian algorithm B
138. Public FC layers + Bi-directional LSTM Hungarian algorithm O
145. Public / from
147. (combines DPM, SDP and ACF) Modified Faster R-CNN Modified Faster R-CNN Particle filter O
148. Public DeepMatching, Siamese CNN Edge potential as in
149., Siamese CNN Lifted multicut B
150. Public CNN (motion pred.), part of MDNet (CNN) N/A Deep RL agents O Table 8: Information summary about the methods commented in section 3. In each column, the approach for each paper in that step is shown. app. means appearance, mot. means motion, feat. means features, pred. means prediction; O and B in the Mode column indicate Online and Batch methods respectively. Text in the last column is clickable and contains links to the specified data. 
