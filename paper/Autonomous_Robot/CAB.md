# Raising context awareness in motion forecasting
æé«˜è¿åŠ¨é¢„æµ‹ä¸­çš„æƒ…å¢ƒæ„è¯† https://arxiv.org/abs/2109.08048

## Abstract
Learning-based trajectory prediction models have encountered great success, with the promise of leveraging contextual information in addition to motion history. Yet, we find that state-of-the-art forecasting methods tend to overly rely on the agentâ€™s current dynamics, failing to exploit the semantic contextual cues provided at its input. To alleviate this issue, we introduce CAB, a motion forecasting model equipped with a training procedure designed to promote the use of semantic contextual information. We also introduce two novel metrics â€” dispersion and convergence-to-range â€” to measure the temporal consistency of successive forecasts, which we found missing in standard metrics. Our method is evaluated on the widely adopted nuScenes Prediction benchmark as well as on a subset of the most difficult examples from this benchmark. The code is available at https://github.com/valeoai/CAB.

åŸºäºå­¦ä¹ çš„è½¨è¿¹é¢„æµ‹æ¨¡å‹å–å¾—äº†å·¨å¤§çš„æˆåŠŸï¼Œé™¤äº†è¿åŠ¨å†å²ä¹‹å¤–ï¼Œè¿˜æœ‰æœ›åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„é¢„æµ‹æ–¹æ³•å¾€å¾€è¿‡åº¦ä¾èµ–æ™ºèƒ½ä½“çš„å½“å‰åŠ¨æ€ï¼Œæœªèƒ½åˆ©ç”¨å…¶è¾“å…¥æä¾›çš„è¯­ä¹‰ä¸Šä¸‹æ–‡çº¿ç´¢ã€‚ ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¼•å…¥äº† CABï¼Œè¿™æ˜¯ä¸€ç§è¿åŠ¨é¢„æµ‹æ¨¡å‹ï¼Œé…å¤‡äº†æ—¨åœ¨ä¿ƒè¿›è¯­ä¹‰ä¸Šä¸‹æ–‡ä¿¡æ¯ä½¿ç”¨çš„è®­ç»ƒç¨‹åºã€‚ æˆ‘ä»¬è¿˜å¼•å…¥äº†ä¸¤ä¸ªæ–°æŒ‡æ ‡ â€”â€” åˆ†æ•£å’Œæ”¶æ•›åˆ°èŒƒå›´ â€”â€” æ¥è¡¡é‡è¿ç»­é¢„æµ‹çš„æ—¶é—´ä¸€è‡´æ€§ï¼Œæˆ‘ä»¬å‘ç°æ ‡å‡†æŒ‡æ ‡ä¸­ç¼ºå°‘è¿™äº›æŒ‡æ ‡ã€‚ æˆ‘ä»¬çš„æ–¹æ³•åœ¨å¹¿æ³›é‡‡ç”¨çš„ nuScenes é¢„æµ‹åŸºå‡†ä»¥åŠè¯¥åŸºå‡†ä¸­æœ€å›°éš¾çš„æ ·æœ¬çš„å­é›†ä¸Šè¿›è¡Œäº†è¯„ä¼°ã€‚
<!-- dispersion and convergence-to-range, åˆ†æ•£å’Œæ”¶æ•›åˆ°èŒƒå›´ -->

## 1. Introduction
Autonomous systems require an acute understanding of other agentsâ€™ intention to plan and act safely, and the capacity to accurately forecast the motion of surrounding agents is paramount to achieving this [3,4,49]. Historically, physics-based approaches have been developed to achieve these forecasts [27]. Over the last years, the paradigm has shifted towards learning-based models [37,42]. These models generally operate over two sources of information: (1) scene information about the agentâ€™s surroundings, e.g. LiDAR point clouds [7, 8, 29, 36] or bird-eye-view rasters [8, 21, 29, 37, 42], and (2) motion cues of the agent, e.g. its instantaneous velocity, acceleration, and yaw rate [12, 37] or its previous trajectory [9, 22, 35, 42]. But despite being trained with diverse modalities as input, we remark that, in practice, these models tend to base their predictions on only one modality: the previous dynamics of the agent. Indeed, trajectory forecasting models obtain very similar performances when the scene information about the agentâ€™s surroundings is removed from the input (see section 4). This phenomenon stems from the very strong auto-correlations often exhibited in trajectories [6, 10]. For instance, when a vehicle is driving straight with a constant speed over the last seconds, situations in which the vehicle keeps driving straight with a constant speed are overwhelmingly represented; similarly, if a vehicle starts braking, its future path is very likely a stopping trajectory. As a consequence, models tend to converge to a local minimum consisting in forecasting motion based on correlations with the past motion cues only, failing to take advantage of the available contextual information [3,11,15,25]. For example, in Figure 1, we observe that several predictions made by the Trajectron++ [42] model leave the driveable area which hints that the scene information was not correctly used by the model.

è‡ªæ²»ç³»ç»Ÿéœ€è¦æ•é”åœ°ç†è§£å…¶ä»–æ™ºèƒ½ä½“äººå®‰å…¨è®¡åˆ’å’Œè¡ŒåŠ¨æ„å›¾ï¼Œè€Œå‡†ç¡®é¢„æµ‹å‘¨å›´æ™ºèƒ½ä½“äººè¿åŠ¨çš„èƒ½åŠ›å¯¹äºå®ç°è¿™ä¸€ç‚¹è‡³å…³é‡è¦ [3,4,49]ã€‚ ä»å†å²ä¸Šçœ‹ï¼Œå·²ç»å¼€å‘äº†åŸºäºç‰©ç†å­¦çš„æ–¹æ³•æ¥å®ç°è¿™äº›é¢„æµ‹ [27]ã€‚ åœ¨è¿‡å»çš„å‡ å¹´é‡Œï¼ŒèŒƒå¼å·²ç»è½¬å‘åŸºäºå­¦ä¹ çš„æ¨¡å‹ [37,42]ã€‚ è¿™äº›æ¨¡å‹é€šå¸¸åœ¨ä¸¤ä¸ªä¿¡æ¯æºä¸Šè¿è¡Œï¼š(1)æ™ºèƒ½ä½“çš„å‘¨å›´ç¯å¢ƒåœºæ™¯ä¿¡æ¯ï¼Œä¾‹å¦‚ LiDAR ç‚¹äº‘ [7ã€8ã€29ã€36] æˆ–é¸Ÿç°å…‰æ … [8ã€21ã€29ã€37ã€42]ï¼Œä»¥åŠ (2) æ™ºèƒ½ä½“çš„è¿åŠ¨çº¿ç´¢ï¼Œä¾‹å¦‚ å®ƒçš„ç¬æ—¶é€Ÿåº¦ã€åŠ é€Ÿåº¦å’Œåèˆªç‡ [12ã€37] æˆ–å…¶å…ˆå‰çš„è½¨è¿¹ [9ã€22ã€35ã€42]ã€‚ ä½†æ˜¯ï¼Œå°½ç®¡æ¥å—äº†å¤šç§æ¨¡å¼ä½œä¸ºè¾“å…¥çš„è®­ç»ƒï¼Œä½†æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨å®è·µä¸­ï¼Œè¿™äº›æ¨¡å‹å€¾å‘äºä»…åŸºäºä¸€ç§æ¨¡å¼è¿›è¡Œé¢„æµ‹ï¼šæ™ºèƒ½ä½“çš„å…ˆå‰åŠ¨æ€ã€‚ äº‹å®ä¸Šï¼Œå½“ä»è¾“å…¥ä¸­åˆ é™¤å…³äºæ™ºèƒ½ä½“å‘¨å›´ç¯å¢ƒçš„åœºæ™¯ä¿¡æ¯æ—¶ï¼Œè½¨è¿¹é¢„æµ‹æ¨¡å‹è·å¾—éå¸¸ç›¸ä¼¼çš„æ€§èƒ½(è§ç¬¬ 4 èŠ‚)ã€‚ è¿™ç§ç°è±¡æºäºè½¨è¿¹ä¸­ç»å¸¸è¡¨ç°å‡ºçš„éå¸¸å¼ºçš„è‡ªç›¸å…³ [6, 10]ã€‚ ä¾‹å¦‚ï¼Œå½“è½¦è¾†åœ¨æœ€åå‡ ç§’å†…åŒ€é€Ÿç›´è¡Œæ—¶ï¼Œç»å¤§éƒ¨åˆ†ä»£è¡¨äº†è½¦è¾†ä¿æŒåŒ€é€Ÿç›´è¡Œçš„æƒ…å†µ;  åŒæ ·ï¼Œå¦‚æœè½¦è¾†å¼€å§‹åˆ¶åŠ¨ï¼Œå…¶æœªæ¥è·¯å¾„å¾ˆå¯èƒ½æ˜¯åœæ­¢è½¨è¿¹ã€‚ å› æ­¤ï¼Œæ¨¡å‹å¾€å¾€ä¼šæ”¶æ•›åˆ°å±€éƒ¨æœ€å°å€¼ï¼ŒåŒ…æ‹¬ä»…åŸºäºä¸è¿‡å»è¿åŠ¨çº¿ç´¢çš„ç›¸å…³æ€§æ¥é¢„æµ‹è¿åŠ¨ï¼Œè€Œæœªèƒ½åˆ©ç”¨å¯ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ [3,11,15,25]ã€‚ ä¾‹å¦‚ï¼Œåœ¨å›¾ 1 ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ° Trajectron++ [42] æ¨¡å‹åšå‡ºçš„å‡ ä¸ªé¢„æµ‹ç¦»å¼€äº†å¯é©¾é©¶åŒºåŸŸï¼Œè¿™æš—ç¤ºæ¨¡å‹æ²¡æœ‰æ­£ç¡®ä½¿ç”¨åœºæ™¯ä¿¡æ¯ã€‚

Such biased models relying too much on motion correlation and ignoring the scene information are unsatisfactory for several reasons. First, context holds crucial elements to perform good predictions when the target trajectory is not an extrapolation of the past motion. Indeed, a biased model will likely fail to forecast high-level behavior changes (e.g. start braking), when scene information is especially needed because of some event occurring in the surroundings (e.g. front vehicle starts braking). Leveraging context is thus paramount for motion anticipation, i.e. converging quickly and smoothly towards the ground truth ahead in time. Furthermore, a biased model has a flawed reasoning because it bases its predictions on motion signals rather than the underlying causes contained within the scene environment. For example, when applied on a vehicle that has started to decelerate, it will attribute its forecast on the past trajectory (e.g. â€˜The car will stop because it has started braking.â€™) instead of the underlying reason (e.g. â€˜The car will stop because it is approaching an intersection with heavy traffic.â€™) [33, 47]. As a direct consequence, explainability methods analyzing a biased model can lead to less satisfactory justifications. Overall, it is thus paramount for motion forecasting algorithms to efficiently leverage the contextual information and to ground motion forecasts on it. 

ç”±äºå¤šç§åŸå› ï¼Œè¿™ç§è¿‡åˆ†ä¾èµ–è¿åŠ¨ç›¸å…³æ€§å¹¶å¿½ç•¥åœºæ™¯ä¿¡æ¯çš„åå·®æ¨¡å‹å¹¶ä¸ä»¤äººæ»¡æ„ã€‚ é¦–å…ˆï¼Œå½“ç›®æ ‡è½¨è¿¹ä¸æ˜¯è¿‡å»è¿åŠ¨çš„å¤–æ¨æ—¶ï¼Œä¸Šä¸‹æ–‡åŒ…å«æ‰§è¡Œè‰¯å¥½é¢„æµ‹çš„å…³é”®å…ƒç´ ã€‚ äº‹å®ä¸Šï¼Œå½“ç”±äºå‘¨å›´å‘ç”Ÿçš„æŸäº›äº‹ä»¶(ä¾‹å¦‚å‰è½¦å¼€å§‹åˆ¶åŠ¨)è€Œç‰¹åˆ«éœ€è¦åœºæ™¯ä¿¡æ¯æ—¶ï¼Œæœ‰åå·®çš„æ¨¡å‹å¯èƒ½æ— æ³•é¢„æµ‹é«˜çº§è¡Œä¸ºå˜åŒ–(ä¾‹å¦‚å¼€å§‹åˆ¶åŠ¨)ã€‚ å› æ­¤ï¼Œåˆ©ç”¨ä¸Šä¸‹æ–‡å¯¹äºè¿åŠ¨é¢„æµ‹è‡³å…³é‡è¦ï¼Œå³å¿«é€Ÿã€å¹³ç¨³åœ°å‘å‰æ–¹çš„åŸºæœ¬äº‹å®æ”¶æ•›ã€‚ æ­¤å¤–ï¼Œæœ‰åå·®çš„æ¨¡å‹æ¨ç†æœ‰ç¼ºé™·ï¼Œå› ä¸ºå®ƒçš„é¢„æµ‹åŸºäºè¿åŠ¨ä¿¡å·ï¼Œè€Œä¸æ˜¯åœºæ™¯ç¯å¢ƒä¸­åŒ…å«çš„æ ¹æœ¬åŸå› ã€‚ ä¾‹å¦‚ï¼Œå½“åº”ç”¨äºå·²å¼€å§‹å‡é€Ÿçš„è½¦è¾†æ—¶ï¼Œå®ƒä¼šå°†å…¶é¢„æµ‹å½’å› äºè¿‡å»çš„è½¨è¿¹(ä¾‹å¦‚â€œæ±½è½¦å°†åœæ­¢ï¼Œå› ä¸ºå®ƒå·²å¼€å§‹åˆ¶åŠ¨â€ã€‚)è€Œä¸æ˜¯æ ¹æœ¬åŸå› (ä¾‹å¦‚â€œæ±½è½¦å°†åœæ­¢â€ å› ä¸ºå®ƒæ­£åœ¨æ¥è¿‘äº¤é€šç¹å¿™çš„åå­—è·¯å£ã€‚') [33, 47]ã€‚ ä½œä¸ºç›´æ¥ç»“æœï¼Œåˆ†ææœ‰åè§çš„æ¨¡å‹çš„å¯è§£é‡Šæ€§æ–¹æ³•å¯èƒ½å¯¼è‡´ä¸å¤ªä»¤äººæ»¡æ„çš„ç†ç”±ã€‚ æ€»çš„æ¥è¯´ï¼Œå¯¹äºè¿åŠ¨é¢„æµ‹ç®—æ³•æ¥è¯´ï¼Œæœ‰æ•ˆåœ°åˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯å¹¶å¯¹å®ƒè¿›è¡Œåœ°é¢è¿åŠ¨é¢„æµ‹æ˜¯è‡³å…³é‡è¦çš„ã€‚ 

Figure 1. Predictions from a) CAB (ours), b) Trajectron++, and c) Trajectron++ without the context input. The thickness of trajectories represent their likelihood. Trajectron++ and its blind variant have very similar predictions and they both forecast trajectories that leave the driveable area. CAB is more consistent with the map. Sidewalks are in blue, crosswalks in yellow and driveable areas in red.
å›¾ 1. æ¥è‡ª a) CAB(æˆ‘ä»¬çš„)ã€b) Trajectron++ å’Œ c) Trajectron++ çš„é¢„æµ‹ï¼Œæ²¡æœ‰ä¸Šä¸‹æ–‡è¾“å…¥ã€‚ è½¨è¿¹çš„ç²—ç»†è¡¨ç¤ºå®ƒä»¬çš„å¯èƒ½æ€§ã€‚ Trajectron++ åŠå…¶ç›²å˜ä½“å…·æœ‰éå¸¸ç›¸ä¼¼çš„é¢„æµ‹ï¼Œå®ƒä»¬éƒ½é¢„æµ‹ç¦»å¼€å¯é©¾é©¶åŒºåŸŸçš„è½¨è¿¹ã€‚ CAB ä¸åœ°å›¾æ›´ä¸€è‡´ã€‚ äººè¡Œé“ä¸ºè“è‰²ï¼Œäººè¡Œæ¨ªé“ä¸ºé»„è‰²ï¼Œå¯è¡Œé©¶åŒºåŸŸä¸ºçº¢è‰²ã€‚

In this paper, we propose to equip a motion forecasting model with a novel learning mechanism that encourages predictions to rely more on the scene information, i.e. a bird-eye-view map of the surroundings and the relationships with neighboring agents. Specifically, we introduce blind predictions, i.e. predictions obtained with past motions of the agent only, without any contextual information. In contrast, the main model has access to both these inputs but is encouraged to produce motion forecasts that are different from the blind predictions, thus promoting the use of contextual information. Our model is called â€˜CABâ€™ as it raises Context Awareness by leveraging Blind predictions. It is built on the Conditional Variational AutoEncoder (CVAE) framework, widely used in motion forecasting; in practice, it is instantiated with the Trajectron++ [42] trajectory forecasting backbone. Specifically, CAB acts on the probabilistic latent representation of the CVAE and encourages the latent distribution for the motion forecasts to be different to the latent distribution for the blind predictions. Additionally, we introduce Reweight, and RUBiZ, two alternative de-biasing strategies that are not specific to probabilistic forecasting models as they rely on loss and gradients reweighting respectively.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å»ºè®®ä¸ºè¿åŠ¨é¢„æµ‹æ¨¡å‹é…å¤‡ä¸€ç§æ–°é¢–çš„å­¦ä¹ æœºåˆ¶ï¼Œé¼“åŠ±é¢„æµ‹æ›´å¤šåœ°ä¾èµ–åœºæ™¯ä¿¡æ¯ï¼Œå³å‘¨å›´ç¯å¢ƒçš„é¸Ÿç°å›¾ä»¥åŠä¸é‚»è¿‘æ™ºèƒ½ä½“çš„å…³ç³»ã€‚ å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¼•å…¥äº†ç›²ç›®é¢„æµ‹ï¼Œå³ä»…é€šè¿‡æ™ºèƒ½ä½“è¿‡å»çš„åŠ¨ä½œè·å¾—çš„é¢„æµ‹ï¼Œæ²¡æœ‰ä»»ä½•ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸»æ¨¡å‹å¯ä»¥è®¿é—®è¿™ä¸¤ç§è¾“å…¥ï¼Œä½†è¢«é¼“åŠ±äº§ç”Ÿä¸ç›²ç›®é¢„æµ‹ä¸åŒçš„è¿åŠ¨é¢„æµ‹ï¼Œä»è€Œä¿ƒè¿›ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä½¿ç”¨ã€‚ æˆ‘ä»¬çš„æ¨¡å‹è¢«ç§°ä¸ºâ€œCABâ€ï¼Œå› ä¸ºå®ƒé€šè¿‡åˆ©ç”¨ç›²ç›®é¢„æµ‹æ¥æé«˜ä¸Šä¸‹æ–‡æ„è¯†ã€‚ å®ƒå»ºç«‹åœ¨å¹¿æ³›ç”¨äºè¿åŠ¨é¢„æµ‹çš„æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨(CVAE)æ¡†æ¶ä¹‹ä¸Š;  åœ¨å®è·µä¸­ï¼Œå®ƒæ˜¯ç”¨ Trajectron++ [42] è½¨è¿¹é¢„æµ‹ä¸»å¹²å®ä¾‹åŒ–çš„ã€‚ å…·ä½“æ¥è¯´ï¼ŒCAB ä½œç”¨äº CVAE çš„æ¦‚ç‡æ½œåœ¨è¡¨ç¤ºï¼Œå¹¶é¼“åŠ±è¿åŠ¨é¢„æµ‹çš„æ½œåœ¨åˆ†å¸ƒä¸ç›²ç›®é¢„æµ‹çš„æ½œåœ¨åˆ†å¸ƒä¸åŒã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº† Reweight å’Œ RUBiZï¼Œè¿™ä¸¤ç§æ›¿ä»£çš„å»åç­–ç•¥å¹¶ä¸ç‰¹å®šäºæ¦‚ç‡é¢„æµ‹æ¨¡å‹ï¼Œå› ä¸ºå®ƒä»¬åˆ†åˆ«ä¾èµ–äºæŸå¤±å’Œæ¢¯åº¦é‡æ–°åŠ æƒã€‚<!--æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨(CVAE)-->

In motion forecast algorithms deployed in real robotic systems, when successive forecasts are done, it is desirable to have both a fast convergence towards the ground-truth, as well as a high consistency of consecutive predictions. Accordingly, we introduce two novel metrics: convergence-torange and dispersion. These metrics aim at providing more refined measurements of how early models are able to anticipate their trajectory and how stable through time their successive predictions are.

åœ¨å®é™…æœºå™¨äººç³»ç»Ÿä¸­éƒ¨ç½²çš„è¿åŠ¨é¢„æµ‹ç®—æ³•ä¸­ï¼Œå½“è¿›è¡Œè¿ç»­é¢„æµ‹æ—¶ï¼Œå¸Œæœ›æ—¢èƒ½å¿«é€Ÿæ”¶æ•›åˆ°çœŸå®æƒ…å†µï¼Œåˆèƒ½ä¿æŒè¿ç»­é¢„æµ‹çš„é«˜åº¦ä¸€è‡´æ€§ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸¤ä¸ªæ–°çš„æŒ‡æ ‡ï¼šconvergence-to range å’Œ dispersionã€‚ è¿™äº›æŒ‡æ ‡æ—¨åœ¨æä¾›æ›´ç²¾ç»†çš„æµ‹é‡ï¼Œä»¥è¡¡é‡æ—©æœŸæ¨¡å‹å¦‚ä½•èƒ½å¤Ÿé¢„æµ‹å®ƒä»¬çš„è½¨è¿¹ï¼Œä»¥åŠå®ƒä»¬çš„è¿ç»­é¢„æµ‹åœ¨æ—¶é—´ä¸Šçš„ç¨³å®šæ€§ã€‚

Overall, our main contributions are as follows.
1. We target the problem of incorporating contextual information into motion forecasting architectures, as we find that state-of-the-art models overly rely on motion.
2. We present CAB, an end-to-end learning scheme that leverages blind predictions to promote the use of context.
3. In addition to standard evaluation practices, we propose two novel metrics, namely dispersion and convergence-to-range, that respectively measure the temporal stability of successive predictions and their spatial convergence speed.

æ€»çš„æ¥è¯´ï¼Œæˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ã€‚
1. æˆ‘ä»¬çš„ç›®æ ‡æ˜¯å°†ä¸Šä¸‹æ–‡ä¿¡æ¯æ•´åˆåˆ°è¿åŠ¨é¢„æµ‹æ¶æ„ä¸­ï¼Œå› ä¸ºæˆ‘ä»¬å‘ç°æœ€å…ˆè¿›çš„æ¨¡å‹è¿‡åº¦ä¾èµ–è¿åŠ¨ã€‚
2. æˆ‘ä»¬ä»‹ç»äº† CABï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å­¦ä¹ æ–¹æ¡ˆï¼Œå®ƒåˆ©ç”¨ç›²ç›®é¢„æµ‹æ¥ä¿ƒè¿›ä¸Šä¸‹æ–‡çš„ä½¿ç”¨ã€‚
3. é™¤äº†æ ‡å‡†è¯„ä¼°å®è·µå¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸¤ä¸ªæ–°æŒ‡æ ‡ï¼Œå³åˆ†æ•£åº¦å’Œæ”¶æ•›åˆ°èŒƒå›´ï¼Œåˆ†åˆ«è¡¡é‡è¿ç»­é¢„æµ‹çš„æ—¶é—´ç¨³å®šæ€§åŠå…¶ç©ºé—´æ”¶æ•›é€Ÿåº¦ã€‚

To validate the design of our approach, we conduct experiments on nuScenes [6], a public self-driving car dataset focused on urban driving. We show that we outperform previous works [42, 52], as well as the alternative debiasing strategies that we propose, inspired by the recent literature in Visual Question Answering (VQA) and Natural Language Inference (NLI) [5, 31]. Besides, we use Shapley values to measure the contribution of each modality on the predictions: this allows us to measure how well a model can leverage the context input. Lastly, we conduct evaluations on a subset of the most difficult examples of nuScenes where we find that our approach is better suited to anticipate high-level behavior changes.

ä¸ºäº†éªŒè¯æˆ‘ä»¬çš„æ–¹æ³•è®¾è®¡ï¼Œæˆ‘ä»¬åœ¨ nuScenes [6] ä¸Šè¿›è¡Œäº†å®éªŒï¼ŒnuScenes [6] æ˜¯ä¸€ä¸ªä¸“æ³¨äºåŸå¸‚é©¾é©¶çš„å…¬å…±è‡ªåŠ¨é©¾é©¶æ±½è½¦æ•°æ®é›†ã€‚ æˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬ä¼˜äºä»¥å‰çš„ä½œå“ [42, 52]ï¼Œä»¥åŠæˆ‘ä»¬æå‡ºçš„æ›¿ä»£å»åç­–ç•¥ï¼Œçµæ„Ÿæ¥è‡ªè§†è§‰é—®ç­” (VQA) å’Œè‡ªç„¶è¯­è¨€æ¨ç† (NLI) [5, 31] æœ€è¿‘çš„æ–‡çŒ®ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬ä½¿ç”¨ Shapley å€¼æ¥è¡¡é‡æ¯ç§æ¨¡æ€å¯¹é¢„æµ‹çš„è´¡çŒ®ï¼šè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿè¡¡é‡æ¨¡å‹å¯¹ä¸Šä¸‹æ–‡è¾“å…¥çš„åˆ©ç”¨ç¨‹åº¦ã€‚ æœ€åï¼Œæˆ‘ä»¬å¯¹ nuScenes æœ€å›°éš¾æ ·æœ¬çš„å­é›†è¿›è¡Œäº†è¯„ä¼°ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„æ–¹æ³•æ›´é€‚åˆé¢„æµ‹é«˜çº§è¡Œä¸ºå˜åŒ–ã€‚<!--å»åç­–ç•¥?-->

## 2. Related Work
Motion forecasting models aim to predict the future trajectories of road agents. This prediction is achieved using information from their current motion such as velocity, acceleration or previous trajectory, and some contextual elements about the scene. This context can take various forms, ranging from raw LiDAR point clouds [7, 8, 26, 29, 36, 39, 40] and RGB camera stream [26, 30, 41, 45] to more semantic representations including High-Definition maps [3,8,16,17,21,29,37,42,43,51], or detections of other agents and their motion information [12, 35, 37, 42]. Recent trajectory prediction models are designed to produce multiple forecasts, attempting to capture the multiplicity of possible futures [12, 24, 46]. Various learning setups are explored to train these models: regression in the trajectory space [9,12,14,20], spatio-temporal occupancy map prediction [3, 16, 17, 43, 49], or probabilistic methods with either implicit modelling using Generative Adversarial Networks (GANs) [18, 19, 41, 51], or explicit modelling with Conditional Variational Auto-Encoder (CVAE) [21,26,40,42,44]. Our work is based on this CVAE family of methods, which not only has provided strong results in motion forecasting but also structurally defines a separation between high-level decision and low-level execution of this decision [9].

è¿åŠ¨é¢„æµ‹æ¨¡å‹æ—¨åœ¨é¢„æµ‹é“è·¯æ™ºèƒ½ä½“çš„æœªæ¥è½¨è¿¹ã€‚ è¿™ç§é¢„æµ‹æ˜¯ä½¿ç”¨æ¥è‡ªä»–ä»¬å½“å‰è¿åŠ¨çš„ä¿¡æ¯æ¥å®ç°çš„ï¼Œä¾‹å¦‚é€Ÿåº¦ã€åŠ é€Ÿåº¦æˆ–å…ˆå‰çš„è½¨è¿¹ï¼Œä»¥åŠå…³äºåœºæ™¯çš„ä¸€äº›ä¸Šä¸‹æ–‡å…ƒç´ ã€‚ è¿™ç§ä¸Šä¸‹æ–‡å¯ä»¥é‡‡ç”¨å„ç§å½¢å¼ï¼Œä»åŸå§‹ LiDAR ç‚¹äº‘ [7ã€8ã€26ã€29ã€36ã€39ã€40] å’Œ RGB ç›¸æœºæµ [26ã€30ã€41ã€45] åˆ°æ›´å¤šè¯­ä¹‰è¡¨ç¤ºï¼ŒåŒ…æ‹¬é«˜æ¸…åœ°å›¾ [3,8,16,17,21,29,37,42,43,51]ï¼Œæˆ–æ£€æµ‹å…¶ä»–æ™ºèƒ½ä½“åŠå…¶è¿åŠ¨ä¿¡æ¯ [12,35,37,42]ã€‚ æœ€è¿‘çš„è½¨è¿¹é¢„æµ‹æ¨¡å‹æ—¨åœ¨äº§ç”Ÿå¤šä¸ªé¢„æµ‹ï¼Œè¯•å›¾æ•æ‰å¯èƒ½æœªæ¥çš„å¤šæ ·æ€§ [12ã€24ã€46]ã€‚ æ¢ç´¢äº†å„ç§å­¦ä¹ è®¾ç½®æ¥è®­ç»ƒè¿™äº›æ¨¡å‹ï¼šè½¨è¿¹ç©ºé—´å›å½’ [9,12,14,20]ï¼Œæ—¶ç©ºå ç”¨å›¾é¢„æµ‹ [3,16,17,43,49]ï¼Œæˆ–ä½¿ç”¨éšå¼å»ºæ¨¡çš„æ¦‚ç‡æ–¹æ³• ä½¿ç”¨ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ (GAN) [18ã€19ã€41ã€51]ï¼Œæˆ–ä½¿ç”¨æ¡ä»¶å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ (CVAE) [21,26,40,42,44] è¿›è¡Œæ˜¾å¼å»ºæ¨¡ã€‚ æˆ‘ä»¬çš„å·¥ä½œåŸºäºè¿™ä¸ª CVAE æ–¹æ³•ç³»åˆ—ï¼Œå®ƒä¸ä»…åœ¨è¿åŠ¨é¢„æµ‹æ–¹é¢æä¾›äº†å¼ºæœ‰åŠ›çš„ç»“æœï¼Œè€Œä¸”åœ¨ç»“æ„ä¸Šå®šä¹‰äº†é«˜å±‚å†³ç­–å’Œå†³ç­–çš„ä½å±‚æ‰§è¡Œä¹‹é—´çš„åˆ†ç¦» [9]ã€‚

The difficulty of efficiently leveraging contextual information in deep forecasting methods is verified in motion planning models that suffer from â€˜causal confusionâ€™ on the state variable leading to catastrophic motion drift [11, 13, 15, 25]. Moreover, modelsâ€™ proclivity to make reasoning shortcuts and to overlook an informative input modality is also encountered in other fields that deal with inputs of different natures, such as medical image processing [48], Visual Question Answering (VQA), or Natural Language Inference (NLI). In VQA, for instance, researchers report that models tend to be strongly biased towards the linguistic inputs and mostly ignore the visual input [1, 2, 5, 34, 38]. For example, the answer to the question â€œWhat color is the banana in the imageâ€ will be â€œYellowâ€ 90% of the time, and models will ignore the image. To alleviate this issue, some recent works propose to explicitly capture linguistic biases within a question-only branch and attempt to reduce the impact of these linguistic biases in the general model, for example through adversarial regularization [38], or with a gradient reweighting strategy during training [5]. We make a parallel between the current motion for trajectory forecasting and the linguistic input in VQA. Also, drawing inspiration from recent de-biasing strategies used in VQA [5, 31], we propose novel methods for motion forecasting. To the best of our knowledge, biases and statistical shortcut on the agentâ€™s dynamics have not yet been studied in the context of learning-based motion forecasting.

åœ¨æ·±åº¦é¢„æµ‹æ–¹æ³•ä¸­æœ‰æ•ˆåˆ©ç”¨ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å›°éš¾åœ¨è¿åŠ¨è§„åˆ’æ¨¡å‹ä¸­å¾—åˆ°éªŒè¯ï¼Œè¿™äº›æ¨¡å‹åœ¨çŠ¶æ€å˜é‡ä¸Šå­˜åœ¨â€œå› æœæ··æ·†â€ï¼Œå¯¼è‡´ç¾éš¾æ€§è¿åŠ¨æ¼‚ç§» [11ã€13ã€15ã€25]ã€‚ æ­¤å¤–ï¼Œåœ¨å¤„ç†ä¸åŒæ€§è´¨çš„è¾“å…¥çš„å…¶ä»–é¢†åŸŸï¼Œå¦‚åŒ»å­¦å›¾åƒå¤„ç† [48]ã€è§†è§‰é—®ç­”(VQA)æˆ–è‡ªç„¶è¯­è¨€æ¨ç†(NLI)ï¼Œæ¨¡å‹ä¹Ÿæœ‰ä½¿ç”¨æ¨ç†æ·å¾„å’Œå¿½ç•¥ä¿¡æ¯è¾“å…¥æ¨¡å¼çš„å€¾å‘ã€‚ ä¾‹å¦‚ï¼Œåœ¨ VQA ä¸­ï¼Œç ”ç©¶äººå‘˜æŠ¥å‘Šè¯´ï¼Œæ¨¡å‹å¾€å¾€å¼ºçƒˆåå‘è¯­è¨€è¾“å…¥ï¼Œè€Œå¤§å¤šå¿½ç•¥è§†è§‰è¾“å…¥ [1ã€2ã€5ã€34ã€38]ã€‚ ä¾‹å¦‚ï¼Œâ€œå›¾åƒä¸­çš„é¦™è•‰æ˜¯ä»€ä¹ˆé¢œè‰²â€è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆåœ¨ 90% çš„æƒ…å†µä¸‹éƒ½æ˜¯â€œé»„è‰²â€ï¼Œæ¨¡å‹ä¼šå¿½ç•¥å›¾åƒã€‚ ä¸ºäº†ç¼“è§£è¿™ä¸ªé—®é¢˜ï¼Œæœ€è¿‘çš„ä¸€äº›å·¥ä½œå»ºè®®åœ¨ä»…é—®é¢˜åˆ†æ”¯ä¸­æ˜ç¡®æ•è·è¯­è¨€åå·®ï¼Œå¹¶å°è¯•å‡å°‘è¿™äº›è¯­è¨€åå·®åœ¨ä¸€èˆ¬æ¨¡å‹ä¸­çš„å½±å“ï¼Œä¾‹å¦‚é€šè¿‡å¯¹æŠ—æ€§æ­£åˆ™åŒ– [38]ï¼Œæˆ–ä½¿ç”¨æ¢¯åº¦é‡æ–°åŠ æƒç­–ç•¥ åœ¨è®­ç»ƒæœŸé—´[5]ã€‚ æˆ‘ä»¬åœ¨è½¨è¿¹é¢„æµ‹çš„å½“å‰è¿åŠ¨å’Œ VQA ä¸­çš„è¯­è¨€è¾“å…¥ä¹‹é—´è¿›è¡Œäº†æ¯”è¾ƒã€‚ æ­¤å¤–ï¼Œä»æœ€è¿‘åœ¨ VQA [5, 31] ä¸­ä½¿ç”¨çš„å»åç­–ç•¥ä¸­æ±²å–çµæ„Ÿï¼Œæˆ‘ä»¬æå‡ºäº†ç”¨äºè¿åŠ¨é¢„æµ‹çš„æ–°æ–¹æ³•ã€‚ æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨åŸºäºå­¦ä¹ çš„è¿åŠ¨é¢„æµ‹çš„èƒŒæ™¯ä¸‹ï¼Œå°šæœªç ”ç©¶æ™ºèƒ½ä½“åŠ¨åŠ›å­¦çš„åå·®å’Œç»Ÿè®¡æ·å¾„ã€‚

## 3. Model
The goal is to predict a distribution over possible future trajectories y = [y1, . . . , yT ] of a moving agent of interest in a scene, where yt âˆˆ R2 is the position of the agent t steps in the future in a bird-eye view, and T is the prediction horizon. To do so, we consider a sequence of sensor measurements X containing motion information (e.g. position, velocity, acceleration) over the H previous steps. Besides, the context C provides information about the static (e.g. driveable area, crosswalks, etc.) and dynamic (e.g. other agentsâ€™ motion) surroundings of the agent. In this framework, a prediction model provides an estimate of p(y|X , C) for any given input pair (X , C).

### 3.1. Conditional VAE framework for motion forecasting
Following recent works in trajectory forecasting [21, 26, 40, 42, 46, 52], we use the CVAE framework to train our model for future motion prediction. A CVAE provides an estimate pÎ˜(y|X , C) of the distribution of possible trajectories by introducing a latent variable z âˆˆ Z that accounts for the possible high-level decisions taken by the agent: pÎ˜(y|X , C) = Z zâˆˆZ pÎ¸ (z|X , C) pÏ†(y|X , C, z), (1) where Î˜ = {Î¸, Ï†}.

Input Blind input

CVAE CVAE block gradients Output

Figure 2. Overview of the learning scheme of CAB. CAB employs a CVAE backbone which produces distributions pÎ¸ (z|X , C) and pÎ˜(y|X , C) over the latent variable z and the future trajectory.

During training, a blind input X , CËœ is forwarded into the CVAE and the resulting distribution over z is used to encourage the prediction of the model to be different from the context-agnostic distribution p(y|X ), thanks to the LCAB-KL loss. Note that the two depicted CVAEs are identical. The original context C is overlayed onto the prediction for visualization purposes.

To train the CVAE, we need to estimate the latent variable z corresponding to a given trajectory y. To that end, we introduce the additional distribution qÏˆ(z|X , C, y).

Distributions pÎ¸ (z|X , C), pÏ†(y|X , C, z) and qÏˆ(z|X , C, y) are parameterized by neural networks, where Î¸, Ï† and Ïˆ are their respective weights. These networks are jointly trained to minimize:

Lcvae = 1N NXi=1 âˆ’ log pÎ˜(yi |Xi, Ci) + Î±DKL[qÏˆ (z|Xi, Ci, yi) k pÎ¸ (z|Xi, Ci)], (2) where the summation ranges over the N training samples indexed by i, and DKL is the Kullback-Leibler divergence.

### 3.2. CAB
Using this setup, ideally, the networks would learn to extract relevant information from both motion and context to produce the most likely distribution over possible outputs y. However, because of the very strong correlation between

X and y in driving datasets, they tend, in practice, to learn to focus essentially on X and to ignore C when estimating p(y|X , C). In the worse cases, models can collapse into estimating simply p(y|X ). Yet, C contains crucial information such as road boundaries or pedestrians. Our goal is then to encourage taking C into account by introducing a regularization term LCAB to the CVAE objective:

L = Lcvae + LCAB. (3)

The idea of LCAB is to encourage the prediction of the model to be different from p (y|X ). However, in practice, we do not have access to this distribution. Instead, we introduce a blind-mode for the CVAE model by simply replacing the context input C by a null context CËœ. We obtain pÎ˜(y|X , CËœ), an explicitly flawed model whose predictions can then be used to steer the learning of the main model pÎ˜(y|X , C) away from focusing exclusively on X .

To do so, we would want LCAB to increase

DKL[pÎ˜(y|X , C)k pÎ˜(y|X , CËœ)]. Unfortunately, this term is intractable in the general case, and computing a robust Monte-Carlo estimate requires sampling a very large number of trajectories, which would significantly slow down the training. Therefore, we simplify the problem by setting this divergence constraint on the distributions over z instead of the distributions over y. We thus minimize

LCAB-KL = âˆ’DKL[pÎ¸ (z|X , C) k pÎ¸(z|X , CËœ)] (4) instead. Following the intuition proposed in [9], the distributions over z model intent uncertainties, whereas distributions over y merge intent and control uncertainties. In this case, forcing pÎ¸(z|X , C) and pÎ¸(z|X , CËœ) to have a high DKL explicitly sets this constraint on high-level decisions.

Moreover, to make sure that pÎ˜(y|X , CËœ) is a reasonable approximation for p(y|X ), we also optimize parameters Î˜ for an additional term LËœ cvae, which consists in the loss described in Equation 2 where each Ci is replaced by CËœ.

The final LCAB objective is then

LCAB = Î»KLLCAB-KL + Î»LËœ cvae, (5) where Î» and Î»KL are hyper-parameters.

To ensure that the blind distribution focuses solely on approximating p(y|X ), LCAB-KL is only back-propagated along pÎ¸(z|X , C) and not along pÎ¸(z|X , CËœ). We underline that LCAB does not introduce extra parameters.

### 3.3. Instanciation of CAB with Trajectron++
To show the efficiency of CAB, we use Trajectron++ [42], a popular model for trajectory prediction based on a variant of a CVAE and whose code is freely available.

We first discuss how the loss of Trajectron++ deviates from standard CVAE, and then present its implementation.

Information Maximizing Categorical CVAE Trajectron++ deviates from the standard CVAE setup in two notable ways. Firstly, following [50], they include in the CVAE objective Lcvae a mutual information term

Iq(X , C, z) between the inputs (X , C) and the latent factor z. Secondly, in Trajectron++, the latent variable z is set as categorical. The output distribution defined in Equation 1 is then modeled as a Gaussian mixture with |Z| modes. These deviations are easily integrated to CAB by adding the same mutual information term and also setting z as categorical.

As with Gaussian distributions that are often used in the context of VAEs, the DKL between two categorical distributions has a differentiable closed-form expression.

Data and implementation in Trajectron++ The dynamic history of the agent is a multi-dimensional temporal signal X = [xâ€“H, ..., xâ€“1, x0], where each vector xj âˆˆ R8 contains position, velocities, acceleration, heading and angular velocity. This sequence is encoded into a vector x = fx (X ), where fx is designed as a recurrent neural network. The visual context C is represented by two quantities that provide external information about the scene. The first is a bird-eye view image M âˆˆ {0, 1}hÃ—wÃ—l , constructed from a high-definition map, where each element M[h, w, l] encodes the presence or the absence of the semantic class l at the position (h, w). Classes correspond to semantic types such as â€œdriveable areaâ€, â€œpedestrian crossingâ€ or â€œwalkwayâ€. This tensor M is processed by a convolutional neural network to provide m = fm(M) âˆˆ Rdm. The second quantity is a vector g âˆˆ Rdg that encodes the dynamic state of neighboring agents. We define the context vector c as the concatenation of m and g.

As discussed here-above, distributions pÎ¸ (z|Xi, Ci) and qÏˆ (z|Xi, Ci, yi) from Equation 2 are set as categorical distributions, parameterized by the outputs of neural networks fÎ¸(x, c) and fÏˆ(x, c, y) respectively.

Then, for each z âˆˆ Z, we have pÏ†(y|x, c, z) = N (Âµz, Î£z), where (Âµz, Î£z) = fÏ† (x, c, z). These Gaussian densities are weighted by the probabilities of the corresponding z, and summed to provide the trajectory distribution: pÎ˜(y|x, c) = X zâˆˆZ pÎ¸(z|x, c)pÏ†(y|x, c, z). (6)

Interestingly, fÏ† is constructed as a composition of two functions. The first is a neural network whose output is a distribution over control values for each prediction timestep. The second is a dynamic integration module that models temporal coherence by transforming these control distributions into 2-D position distributions. This design ensures that output trajectories are dynamically feasible. For more details, please refer to [42].

### 3.4. Alternative de-biasing strategies
We also propose two alternative de-biaising strategies.

Like CAB, they leverage blind predictions to encourage the model to use the context. However, unlike CAB that plays on the specificity of motion forecasting by acting on distribution of the latent representation, these variations are inspired by recent models from the VQA and NLI fields. 
* Reweight is inspired by the de-biased focal loss proposed in [31]. The importance of training examples is dynamically modulated during the learning phase to focus more on examples poorly handled by the blind model. Formally, the model optimizes the following objective:

Lrw =Lcvae + LËœ cvae âˆ’ NXi=1 Ïƒ(âˆ’ log pÎ˜(yi |Xi, CËœ)) log pÎ˜(yi |Xi, Ci), (7) where Ïƒ represents the sigmoid function. Intuitively, samples that can be well predicted from the blind model, i.e. low value of Ïƒ(âˆ’ log pÎ˜(yi |Xi, CËœi)), will see their contribution lowered and reciprocally, the ones that require contextual information to make accurate forecast, i.e. high value of Ïƒ(âˆ’ log pÎ˜(yi |Xi, CËœi)), have an increased weight. Similarly to CAB, we prevent the gradients to flow back into the blind branch from the loss weight term. 
* RUBiZ adjusts gradients instead of sample importance. It does so by modulating the predictions of the main model during training to resemble more to predictions of a blind model. RUBiZ is inspired by RUBi [5], a VQA model designed to mitigate language bias. Originally designed for the classification setup, we adapt this de-biasing strategy to operate over the latent factor z of our model, hence the name

RUBiZ. In practice, given l and Ëœl the logits of pÎ¸(z|X , C) and pÎ¸(z|X , CËœ), a new distribution over the latent variable is obtained as p rubiz Î¸ (z|X , C, CËœ) = softmax(Ïƒ(l)âˆ—Ïƒ(Ëœl)). This distribution, when used by the decoder, shifts the output of the main prediction towards a blind prediction. Consequently, situations where scene information is essential and past trajectory is not enough have increased gradient, whereas easy examples that are well predicted by the blind model have less importance in the global objective.

## 4. Experiments
### 4.1. nuScenes Dataset
Our models are trained and evaluated on the driving dataset nuScenes [6]. It contains a thousand 20-second urban scenes recorded in Boston and Singapore. Each scene includes data from several cameras, lidars, and radars, a high-definition map of the scene, as well as annotations for surrounding agents provided at 2 Hz. These annotations are processed to build a trajectory prediction dataset for surrounding agents, and especially for vehicles. Models are trained and evaluated on the official train/val/test splits from the nuScenes Prediction challenge, respectively containing 32186 / 8560 / 9041 instances, each corresponding to a specific agent at a certain time step for which we are given a 2-second history (H = 4) and are expected to predict up to 6 seconds in the future (T = 12).

### 4.2. Baselines and details
Physics-based baselines We consider four simple physicsbased models, and a Physics oracle, as introduced in [37], that are purely based on motion cues and ignore contextual elements. The four physics-based models use the current velocity, acceleration, and yaw rate and forecast assuming constant speed/acceleration and yaw/yaw rate. The trajectory predicted by the Physics oracle model is constructed by selecting the best trajectory, in terms of average point-wise

Euclidean distance, from the pool of trajectories predicted by the four aforementioned physics-based models. This

Physics Oracle serves as a coarse upper bound on the best achievable results from a blind model that would be purely based on motion dynamics and ignores the scene structure.

Learning-based forecasting methods We compare our debiased models against recently published motion prediction models. CoverNet [37] forwards a rasterized representation of the scene and the vehicle state (velocity, acceleration, yaw rate) into a CNN and learns to predict the future motion as a class, which corresponds to a pre-defined trajectory. We re-train the â€œfixed  = 2â€ variant, for which the code is available, to compare it with our models. Trajectron++ [42] is our baseline, which corresponds to removing LCAB in CAB. HalentNet [52] casts the Trajectron++ model as the generator of a Generative Adversarial

Network (GAN) [18]. A discriminator is trained to distinguish real trajectories from generated ones and to recognize which z was chosen to sample a trajectory. It also introduces â€˜hallucinatedâ€™ predictions in the training, which correspond to predictions with several confounded values of z. To measure the usefulness of the contextual elements in these models, we also consider the â€˜Trajectron++ (nocontext)â€™ and â€˜HalentNet (no-context)â€™ variants that simply discard the map and social interactions from the input of the respective underlying models. Trajectron++ and HalentNet are not evaluated for different temporal horizons on the nuScenes prediction challenge splits and we thus re-train them given their respective codebases.

Implementation details We use the ADAM optimizer [23], with a learning rate of 0.0003. The value of hyperparameters Î» = 1.0 and Î»KL = 5.0 are found on the validation set.

ADE-ML FDE-ML OffR-ML ADE-f FDE-f OffR-f

Model @1s @2s @3s @4s @5s @6s @1s @2s @3s @4s @5s @6s @6s @6s @6s @6s

Constant vel. and yaw 0.46 0.94 1.61 2.44 3.45 4.61 0.64 1.74 3.37 5.53 8.16 11.21 0.14 -

Physics Oracle 0.43 0.82 1.33 1.98 2.76 3.70 0.59 1.45 2.69 4.35 6.47 9.09 0.12 -

Covernet, fixed  = 2 0.81 1.41 2.11 2.93 3.88 4.93 1.07 2.35 3.92 5.90 8.30 10.84 0.11 -

Trajectron++ (no-context) 0.13 0.39 0.87 1.59 2.56 3.80 0.15 0.86 2.23 4.32 7.22 10.94 0.27 4.46 12.32 0.36

Trajectron++ 0.13 0.39 0.86 1.55 2.47 3.65 0.15 0.87 2.16 4.15 6.92 10.45 0.23 4.15 11.44 0.29

HalentNet (no-context) 0.12 0.38 0.82 1.43 2.21 3.17 0.13 0.85 2.04 3.72 5.92 8.64 0.27 4.13 10.95 0.29

HalentNet 0.14 0.41 0.87 1.51 2.32 3.29 0.17 0.88 2.14 3.91 6.15 8.83 0.28 3.98 10.61 0.25

Reweight 0.13 0.38 0.81 1.42 2.20 3.14 0.15 0.83 2.00 3.69 5.90 8.58 0.17 3.71 9.74 0.19

RUBiZ 0.18 0.42 0.82 1.40 2.14 3.04 0.23 0.84 1.95 3.55 5.65 8.21 0.11 3.68 9.45 0.17

CAB 0.12 0.34 0.73 1.29 2.01 2.90 0.14 0.73 1.81 3.39 5.47 8.02 0.13 3.41 9.03 0.20

Table 1. Trajectory forecasting on the nuScenes Prediction challenge [6]. Reported metrics are the Average/Final Displacement Error (ADE/FDE), and the Off-road Rate (OffR). Each metric is computed for both the most-likely trajectory (-ML) and the full distribution (-f).

### 4.3. Results and standard evaluations
We compare our debiased models to the baselines by measuring the widely used metrics of displacement and offroad rate. All models are trained to predict 6 seconds in the future, and their performance is evaluated for varying temporal horizons (T âˆˆ {2, 4, 6, 8, 10, 12}). Average Displacement Error (ADE) and Final Displacement Error (FDE) measure the distance between the predicted and the groundtruth trajectory, either as an average between each corresponding pair of points (ADE), or as the distance between final points (FDE). To compute these metrics with CAB, we sample the most likely trajectory yML by first selecting the most likely latent factor zML = arg maxzâˆˆZ pÎ¸(z|x, c), and then computing the mode of the corresponding Gaussian yML = arg maxy pÏ†(y|x, c, zML). To evaluate the quality of the whole distribution and not just the most-likely trajectory, similarly to [42, 52], we compute metrics â€˜ADE-fâ€™ and â€˜FDE-fâ€™. They are respectively the average and final displacement error averaged for 2000 trajectories randomly sampled in the full distribution predicted by the network yfull âˆ¼ pÎ˜(y|x, c). Finally, the â€˜off-road rateâ€™ (OffR) is the rate of future trajectories that leave the driveable area.

In Table 1, we compare the performance of our models CAB, Reweight and RUBiZ with baselines from the recent literature. To begin with, we remark that for the

Trajectron++ model, the use of context brings close to no improvement for predictions up to 4 seconds and a very small one for 5and 6-second horizons. Even more surprisingly, the HalentNet (no-context) model which does not use any information from the surroundings, shows better

ADE-ML and FDE-ML than the regular context-aware HalentNet model. This supports our claim that the contextual elements are overlooked by these models and that predictions are mostly done by relying on motion cues. Moreover, we emphasize that the Physics oracle â€” which is purely based on motion dynamics â€” obtains very strong performances (3.70 ADE-ML@6s, 9.09 FDE-ML@6s) as it can choose the closest trajectory to the ground truth from a variety of dynamics. Its scores approximate upper bounds on the forecasting abilities of purely motion-based models and we observe that learning-based methods hardly outperform this Physics-oracle on long temporal horizons.

On the other hand, we remark that all three of our debiaising strategies significantly outperform the Physics oracle and previous models on almost all the metrics, both when looking at the most-likely trajectory as well as the full future distribution. This validates the idea, shared in our methods, to enforce the modelâ€™s prediction to have a high divergence with a blind prediction. Indeed, despite optimizing very different objective functions, our Reweight and

RUBiZ and CAB share the idea of a motion-only encoding.

More precisely, at a 6-second horizon, the sample reweighting strategy gives a relative improvement of 16% w.r.t. Trajectron++. The more refined RUBiZ strategy of gradient reweighting gives a relative improvement of 19% w.r.t. Trajectron++. CAB achieves a 22% relative improvement over

Trajectron++. This indicates that guiding the modelâ€™s latent variable constitutes a better use of blind predictions than simple example or gradient weightings.

### 4.4. Further analyses: stability, convergence, Shapley values
We hypothesize that properly leveraging contextual information has a strong impact on the ability to anticipate the agentâ€™s intents. Intuitively, for an agent arriving at an intersection, a model without context will begin predicting a stopping trajectory only from the moment when this agent starts to stop, whereas a model with a proper understanding of contextual information will be able to foresee this behavior change ahead in time. Furthermore, improving this anticipation ability should also help the temporal stability of the predictions, as unanticipated changes of trajectory will be less likely. Unfortunately, ADE and FDE metrics do not explicitly measure the rate of convergence towards the ground-truth, nor the stability of successive predictions.

Consequently, we introduce two new metrics focusing on the stability and convergence rate of successive forecasts. Instead of classically looking at the T-step forecast

Figure 3. Visualizations of predicted trajectories and Shapley values. The thickness of lines represent the probability of each trajectory.

Dispersion D â†“

Convergence-to-range C(Ï„ ) â†‘

Model Ï„ = 20cm Ï„ = 1m Ï„ = 5m

Cst. accel., yaw 6.55 0.44 1.49 3.30

Cst. accel., yaw rate 6.03 0.47 1.58 3.46

Cst. speed, yaw rate 3.42 0.54 1.82 4.15

Cst. vel., yaw 3.33 0.53 1.82 4.16

Physics Oracle 2.99 0.55 1.91 4.38

Covernet, fixed  = 2 4.06 0.15 0.93 3.50

Trajectron++ (no-context) 3.65 1.00 2.11 4.17

Trajectron++ 3.55 0.98 2.11 4.22

Halentnet (no-context) 2.85 1.03 2.28 4.59

Halentnet 3.23 0.90 2.07 4.26

Reweight 3.02 1.03 2.27 4.50

RUBiZ 2.73 0.94 2.31 4.60

CAB 2.61 1.12 2.45 4.74

Table 2. Study of the temporal stability of trajectory prediction, with the Dispersion (D) and Convergence-to-range (C(Ï„ )) metrics. Predictions are made at a 6-second horizon. yt = [y1t , . . . , ytT ] made at a specific time t, we take a dual viewpoint by considering the consecutive predictions [ytË†âˆ’T T , . . . , y1tË†âˆ’1] made for the same ground-truth point y gt Ë†t .

When the agent approaches the timestamp tË†, as t grows, predictions yttË†âˆ’t will get closer to the ground-truth y gt Ë†t . In addition to low ADE/FDE scores, it is desirable to have both (1) a high consistency of consecutive predictions, as well as (2) a fast convergence towards the ground-truth y gt Ë†t . Therefore, for a given annotated point at tË†, we define the dispersion DtË† as the standard deviation of the points predicted by the model for this specific ground-truth point

DtË† = STD ( k yttË†âˆ’t âˆ’ yÂ¯tË†k )tâˆˆJ 1,TK where yÂ¯tË† is the barycenter of {yttË†âˆ’t}tâˆˆJ 1,TK . The global dispersion score D is obtained by averaging these values over all the points in the dataset. Moreover, we propose the convergence-to-range-Ï„ metric C(Ï„ )tË† as the time from which all subsequent predictions fall within a margin Ï„ of the ground-truth y gt Ë†t , where Ï„ is a user-defined threshold:

CtË†(Ï„ )=max  T0 âˆˆ J 1, TK | âˆ€t â‰¤ T0 , k yttË†âˆ’tâˆ’y gt Ë†t k 2 â‰¤ Ï„	 . (8)

In Table 2, we report evaluations of the stability and spatial convergence metrics. First, we observe that previous learning-based forecasting models have more limited anticipation capacities than the simpler physics-based models, in terms of both convergence speed (metric C(Ï„ )) and convergence stability (metric D). Consistently to the results of Table 1, we remark that our de-biased strategies, and especially our main model CAB, lead to better anticipation scores as they converge faster towards the ground truth.

In Figure 3, we visualize trajectories generated by

CAB and the baselines Trajectron++ and Trajectron++ (nocontext). We also analyze the contribution brought by each input of the model. To do so, we estimate the Shapley values [28] which correspond to the signed contribution of individual input features on a scalar output, the distance to the final predicted point in our case. We remark that the Shapley value of the state signal is overwhelmingly higher than the ones attributed to the map and the neighbors for Trajectron++. This means that the decisions are largely made from the agentâ€™s dynamics. This can further be seen as sev-

Model 1% 2% 3% All

Trajectron++ (no-context) 15.80 15.58 14.97 10.94

Trajectron ++ 13.12 12.69 12.25 10.45

HalentNet 14.12 12.83 12.09 8.83

Reweight 14.00 13.30 12.58 8.58

RUBiZ 13.42 12.49 11.64 8.21

CAB 12.13 11.88 11.59 8.02

Table 3. Final Displacement Error FDE@6s on challenging situations, as defined by Makansi et al. [32]. Results for columns â€˜i%â€™ are averaged over the top i% hardest situations as measured by the the mismatch between the prediction from a Kalman filter and the ground-truth. eral predicted trajectories are highly unlikely futures as they collide with the other agents and/or leave the driveable area.

Instead, the Shapley values for CAB give much more importance to both the map and the neighboring agents, which helps to generate likely and acceptable futures.

### 4.5. Evaluation on hard situations
We verify that the performance boost observed in Table 1 does not come at the expense of a performance drop on difficult yet critical situations. Accordingly, we use recently proposed evaluations [32] as they remark that uncritical cases dominate the prediction and that complex scenarios cases are at the long tail of the dataset distribution. In practice, situations are ranked based on how well the forecast made by a Kalman filter fits the ground-truth trajectory.

In Table 3, we report such stratified evaluations, on the 1%, 2%, and 3% hardest situations. Our first observation is that the overall performance (â€˜Allâ€™) does not necessarily correlate with the capacity to anticipate hard situations.

Indeed, while HalentNet significantly outperfoms Trajectron++ on average, it falls short on the most challenging cases. Besides, CAB achieves better results than Trajectron++ on the hardest situations (top 1%, 2%, and 3%).

Lastly, while the gap between Trajectron++ and CAB is only 0.66 point for the 3% of hard examples, it increases for the top 1% of hardest examples up to 0.99.

In Figure 4, we display some qualitative results we obtain on challenging situations selected among the 1% hardest examples. On the left, we observe that the turn is not correctly predicted by Trajectron++ as it estimates several possible futures that leave the driveable area. On the right, the agent of interest has to stop because of stopped agents in front of it and this behavior is well forecasted by CAB, unlike Trajectron++ which extrapolates the past and provides multiple futures colliding into other agents. Overall, the better use of the context in CAB not only helps on average situations but also on difficult and potentially critical ones.

Figure 4. Visualizations on challenging situations, as defined by Makansi et al. [32]. By better leveraging the context, CAB generates more accurate predictions while Trajectron++ leaves the driveable area or collides into other agents.

## 5. Conclusion
We showed that modern motion forecasting models struggle to use contextual scene information. To address this, we introduced blind predictions that we leveraged with novel de-biaising strategies. This results into three motion forecasting models designed to focus more on context. We show that doing so helps reducing statistical biases from which learning-based approaches suffer. In particular,

CAB, which is specifically built for probabilistic forecasting models, makes significant improvements in traditional distance-based metrics. Finally, after introducing new stability and convergence metrics, we show that CAB shows better anticipation properties than concurrent methods.

Acknowledgments: We thank Thibault Buhet, Auguste

Lehuger and Ivan Novikov for insightful comments. This work was supported by ANR grant VISA DEEP (ANR-20-

CHIA-0022) and MultiTrans (ANR-21-CE23-0032).

## References

1. Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. Analyzing the behavior of visual question answering models. In EMNLP, 2016. 3
2. Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Donâ€™t just assume; look and answer: Overcoming priors for visual question answering. In CVPR, 2018. 3
3. Mayank Bansal, Alex Krizhevsky, and Abhijit S. Ogale. Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst. In Robotics: Science and Systems XV, 2019. 1, 2
4. Thibault Buhet, Â´Emilie Wirbel, and Xavier Perrotton. PLOP: probabilistic polynomial objects trajectory planning for autonomous driving. In CoRL, Proceedings of Machine Learning Research, 2020. 1
5. RÂ´emi Cad`ene, Corentin Dancette, Hedi Ben-younes, Matthieu Cord, and Devi Parikh. Rubi: Reducing unimodal biases for visual question answering. In NeurIPS, 2019. 2, 3, 5
6. Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020. 1, 2, 5, 6
7. Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urtasun. Spagnn: Spatially-aware graph neural networks for relational behavior forecasting from sensor data. In ICRA, 2020. 1, 2
8. Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet: Learning to predict intention from raw sensor data. In CoRL, 2018. 1, 2
9. Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In CoRL, Proceedings of Machine Learning Research, 2019. 1, 2, 4
10. Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argoverse: 3d tracking and forecasting with rich maps. In CVPR, 2019. 1
11. Felipe Codevilla, Eder Santana, Antonio M. LÂ´opez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In ICCV, 2019. 1, 3
12. Henggang Cui, Vladan Radosavljevic, Fang-Chieh Chou, Tsung-Han Lin, Thi Nguyen, Tzu-Kuo Huang, Jeff Schneider, and Nemanja Djuric. Multimodal trajectory predictions for autonomous driving using deep convolutional networks. In ICRA, 2019. 1, 2
13. Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In NeurIPS, 2019. 3
14. Nachiket Deo and Mohan M. Trivedi. Convolutional social pooling for vehicle trajectory prediction. In CVPR Workshops, 2018. 2
15. Laurent George, Thibault Buhet, Â´Emilie Wirbel, Gaetan LeGall, and Xavier Perrotton. Imitation learning for end to end vehicle longitudinal control with forward camera. CoRR, abs/1812.05841, 2018. 1, 3
16. Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde. GOHOME: graphoriented heatmap output for future motion estimation. CoRR, abs/2109.01827, 2021. 2
17. Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde. HOME: heatmap output for future motion estimation. In IITSC, 2021. 2
18. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. 2, 5
19. Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social GAN: socially acceptable trajectories with generative adversarial networks. In CVPR, 2018. 2
20. Joey Hong, Benjamin Sapp, and James Philbin. Rules of the road: Predicting driving behavior with a convolutional model of semantic interactions. In CVPR, 2019. 2
21. Boris Ivanovic and Marco Pavone. The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs. In ICCV, 2019. 1, 2, 3
22. Byeoungdo Kim, Chang Mook Kang, Jaekyum Kim, SeungHi Lee, Chung Choo Chung, and Jun Won Choi. Probabilistic vehicle trajectory prediction over occupancy grid map via recurrent neural network. In ITSC, 2017. 1
23. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5
24. Kris M. Kitani, Brian D. Ziebart, James Andrew Bagnell, and Martial Hebert. Activity forecasting. In ECCV, 2012. 2
25. Yann LeCun, Urs Muller, Jan Ben, Eric Cosatto, and Beat Flepp. Off-road obstacle avoidance through end-to-end learning. In NIPS, 2005. 1, 3
26. Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B. Choy, Philip H. S. Torr, and Manmohan Chandraker. DESIRE: distant future prediction in dynamic scenes with interacting agents. In CVPR, 2017. 2, 3
27. StÂ´ephanie Lef`evre, Dizan Vasquez, and Christian Laugier. A survey on motion prediction and risk assessment for intelligent vehicles. ROBOMECH journal, 2014. 1
28. Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In NIPS, 2017. 7
29. Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net. In CVPR, 2018. 1, 2
30. Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wenping Wang, and Dinesh Manocha. Trafficpredict: Trajectory prediction for heterogeneous traffic-agents. In AAAI, 2019. 2
31. Rabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. End-to-end bias mitigation by modelling biases in corpora. In ACL, 2020. 2, 3, 5
32. Osama Makansi, Â¨OzgÂ¨un CÂ¸ icÂ¸ek, Yassine Marrakchi, and Thomas Brox. On exposing the challenging long tail in future prediction of traffic actors. In ICCV, 2021. 8
33. Osama Makansi, Julius Von KÂ¨ugelgen, Francesco Locatello, Peter Vincent Gehler, Dominik Janzing, Thomas Brox, and Bernhard SchÂ¨olkopf. You mostly walk alone: Analyzing feature attribution in trajectory prediction. In ICLR, 2022. 1
34. Varun Manjunatha, Nirat Saini, and Larry S. Davis. Explicit bias discovery in visual question answering models. In CVPR, 2019. 3
35. Kaouther Messaoud, Nachiket Deo, Mohan M. Trivedi, and Fawzi Nashashibi. Multi-head attention with joint agent-map representation for trajectory prediction in autonomous driving. CoRR, abs/2005.02545, 2020. 1, 2
36. Gregory P. Meyer, Jake Charland, Shreyash Pandey, Ankit Laddha, Shivam Gautam, Carlos Vallespi-Gonzalez, and Carl K. Wellington. Laserflow: Efficient and probabilistic object detection and motion forecasting. IEEE Robotics Autom. Lett., 2021. 1, 2
37. Tung Phan-Minh, Elena Corina Grigore, Freddy A. Boulton, Oscar Beijbom, and Eric M. Wolff. Covernet: Multimodal behavior prediction using trajectory sets. In CVPR, 2020. 1, 2, 5
38. Sainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. Overcoming language priors in visual question answering with adversarial regularization. In NeurIPS, 2018. 3
39. Nicholas Rhinehart, Kris M. Kitani, and Paul Vernaza. r2p2: A reparameterized pushforward policy for diverse, precise generative path forecasting. In ECCV, 2018. 2
40. Nicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. PRECOG: prediction conditioned on goals in visual multi-agent settings. In ICCV, 2019. 2, 3
41. Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid Rezatofighi, and Silvio Savarese. Sophie: An attentive GAN for predicting paths compliant to social and physical constraints. In CVPR, 2019. 2
42. Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In ECCV, 2020. 1, 2, 3, 4, 5, 6
43. Maximilian SchÂ¨afer, Kun Zhao, Markus BÂ¨uhren, and Anton Kummert. Context-aware scene prediction network (caspnet). CoRR, abs/2201.06933, 2022. 2
44. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In NIPS, 2015. 2
45. Shashank Srikanth, Junaid Ahmed Ansari, Karnik Ram R., Sarthak Sharma, J. Krishna Murthy, and K. Madhava Krishna. INFER: intermediate representations for future prediction. In IROS, 2019. 2
46. Yichuan Charlie Tang and Ruslan Salakhutdinov. Multiple futures prediction. In NeurIPS, 2019. 2, 3
47. Â´Eloi Zablocki, Hedi Ben-Younes, Patrick PÂ´erez, and Matthieu Cord. Explainability of vision-based autonomous driving systems: Review and challenges. CoRR, abs/2101.05307, 2021. 1
48. John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and Eric K. Oermann. Confounding variables can degrade generalization performance of radiological deep learning models. CoRR, abs/1807.00431, 2018. 3
49. Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. In CVPR, 2019. 1, 2
50. Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in variational autoencoders. In AAAI, 2019. 4
51. Tianyang Zhao, Yifei Xu, Mathew Monfort, Wongun Choi, Chris L. Baker, Yibiao Zhao, Yizhou Wang, and Ying Nian Wu. Multi-agent tensor fusion for contextual trajectory prediction. In CVPR, 2019. 2
52. Deyao Zhu, Mohamed Zahran, Li Erran Li, and Mohamed Elhoseiny. Halentnet: Multimodal trajectory forecasting with hallucinative intents. In ICLR, 2021. 2, 3, 5, 6
