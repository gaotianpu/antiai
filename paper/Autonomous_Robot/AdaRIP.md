# Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?
è‡ªåŠ¨é©¾é©¶æ±½è½¦èƒ½å¦è¯†åˆ«ã€æ¢å¤å¹¶é€‚åº”åˆ†å¸ƒå˜åŒ–ï¼Ÿ https://arxiv.org/abs/2006.14911

## Abstract
Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called robust imitative planning (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the modelâ€™s uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term adaptive robust imitative planning (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes prediction challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess control, we introduce an autonomous car novel-scene benchmark,CARNOVEL, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts, where our methods outperform all the baselines.

è®­ç»ƒåˆ†å¸ƒå¤– (OOD) åœºæ™¯æ˜¯å­¦ä¹ æ™ºèƒ½ä½“åœ¨éƒ¨ç½²æ—¶é¢ä¸´çš„å¸¸è§æŒ‘æˆ˜ï¼Œé€šå¸¸ä¼šå¯¼è‡´ä»»æ„æ¨è®ºå’Œä¸æ˜æ™ºçš„å†³ç­–ã€‚ åŸåˆ™ä¸Šï¼Œæ£€æµ‹å’Œé€‚åº” OOD åœºæ™¯å¯ä»¥å‡è½»å®ƒä»¬çš„ä¸åˆ©å½±å“ã€‚ åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å½“å‰æ–°é¢–é©¾é©¶åœºæ™¯æ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ç§è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è§„åˆ’æ–¹æ³•ï¼Œç§°ä¸ºç¨³å¥æ¨¡ä»¿è§„åˆ’ (RIP)ã€‚ æˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ£€æµ‹åˆ°ä¸€äº›åˆ†å¸ƒå˜åŒ–å¹¶ä»ä¸­æ¢å¤ï¼Œå‡å°‘ OOD åœºæ™¯ä¸­çš„è¿‡åº¦è‡ªä¿¡å’Œç¾éš¾æ€§å¤–æ¨ã€‚ å¦‚æœæ¨¡å‹çš„ä¸ç¡®å®šæ€§å¤ªå¤§è€Œæ— æ³•å»ºè®®å®‰å…¨çš„è¡ŒåŠ¨æ–¹æ¡ˆï¼Œåˆ™è¯¥æ¨¡å‹å¯ä»¥å‘ä¸“å®¶å¸æœºæŸ¥è¯¢åé¦ˆï¼Œä»è€Œå®ç°æ ·æœ¬æœ‰æ•ˆçš„åœ¨çº¿é€‚åº”ï¼Œè¿™æ˜¯æˆ‘ä»¬ç§°ä¸ºè‡ªé€‚åº”ç¨³å¥æ¨¡ä»¿è§„åˆ’ (AdaRIP) æ–¹æ³•çš„ä¸€ç§å˜ä½“ã€‚ æˆ‘ä»¬çš„æ–¹æ³•åœ¨ nuScenes é¢„æµ‹æŒ‘æˆ˜ä¸­ä¼˜äºå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œä½†ç”±äºç›®å‰æ²¡æœ‰è¯„ä¼° OOD æ£€æµ‹å’Œé€‚åº”çš„åŸºå‡†æ¥è¯„ä¼°æ§åˆ¶ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè‡ªåŠ¨é©¾é©¶æ±½è½¦æ–°åœºæ™¯åŸºå‡† CARNOVEL æ¥è¯„ä¼° é©±åŠ¨æ™ºèƒ½ä½“æ‰§è¡Œä¸€ç»„å…·æœ‰åˆ†å¸ƒå˜åŒ–çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿™äº›ä»»åŠ¡ä¸­ä¼˜äºæ‰€æœ‰åŸºçº¿ã€‚

## 1. Introduction
Autonomous agents hold the promise of systematizing decision-making to reduce catastrophes due to human mistakes. Recent advances in machine learning (ML) enable the deployment of such agents in challenging, real-world, safety-critical domains, such as autonomous driving (AD) in urban areas. However, it has been repeatedly demonstrated that the reliability of ML models degrades radically when they are exposed to novel settings (i.e., under a shift away from the distribution of observations seen during their training) due to their failure to generalise, leading to catastrophic outcomes (Sugiyama & Kawanabe, 2012; Amodei et al., 2016; Snoek et al., 2019). The diminishing performance of ML models to out-of-training distribution (OOD) regimes is concerning in life-critical applications, such as AD (Quionero-Candela et al., 2009; Leike et al., 2017).

è‡ªä¸»æ™ºèƒ½ä½“æœ‰æœ›å°†å†³ç­–ç³»ç»ŸåŒ–ï¼Œä»¥å‡å°‘å› äººä¸ºé”™è¯¯é€ æˆçš„ç¾éš¾ã€‚ æœºå™¨å­¦ä¹  (ML) çš„æœ€æ–°è¿›å±•ä½¿æ­¤ç±»æ™ºèƒ½ä½“èƒ½å¤Ÿéƒ¨ç½²åœ¨å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€ç°å®ä¸–ç•Œçš„ã€å®‰å…¨å…³é”®é¢†åŸŸï¼Œä¾‹å¦‚åŸå¸‚åœ°åŒºçš„è‡ªåŠ¨é©¾é©¶ (AD)ã€‚ ç„¶è€Œï¼Œå·²ç»åå¤è¯æ˜ï¼Œå½“ ML æ¨¡å‹æš´éœ²äºæ–°ç¯å¢ƒæ—¶(å³åç¦»è®­ç»ƒæœŸé—´è§‚å¯Ÿåˆ°çš„åˆ†å¸ƒ)ï¼Œç”±äºå®ƒä»¬æ— æ³•æ³›åŒ–ï¼Œå¯é æ€§ä¼šæ€¥å‰§ä¸‹é™ï¼Œä»è€Œå¯¼è‡´ç¾éš¾æ€§çš„åæœ (Sugiyama å’Œ Kawanabeï¼Œ2012 ;Amodei et al., 2016 ;Snoek et al., 2019)ã€‚ ML æ¨¡å‹å¯¹è®­ç»ƒå¤–åˆ†å¸ƒ (OOD) æœºåˆ¶çš„æ€§èƒ½ä¸‹é™åœ¨ç”Ÿå‘½å…³é”®åº”ç”¨ç¨‹åºä¸­å¼•èµ·å…³æ³¨ï¼Œä¾‹å¦‚ AD(Quionero-Candela et al., 2009 ;Leike et al., 2017)ã€‚

Figure 1. Didactic example: (a) in a novel, out-of-training distribution (OOD) driving scenario, candidate plans/trajectories y1, y2, y3 are (b) evaluated (row-wise) by an ensemble of expertlikelihood models q1, q2, q3. Under models q1 and q2 the best plans are the catastrophic trajectories y1 and y2 respectively. Our epistemic uncertainty-aware robust (RIP) planning method aggregates the evaluations of the ensemble and proposes the safe plan y3 . RIP considers the disagreement between the models and avoid overconfident but catastrophic extrapolations in OOD tasks. 
å›¾ 1. æ•™å­¦æ ·æœ¬ï¼š(a) åœ¨æ–°é¢–çš„è®­ç»ƒå¤–åˆ†å¸ƒ (OOD) é©¾é©¶åœºæ™¯ä¸­ï¼Œå€™é€‰è®¡åˆ’/è½¨è¿¹ y1ã€y2ã€y3 (b) ç”±ä¸€ç»„ä¸“å®¶ä¼¼ç„¶æ¨¡å‹è¿›è¡Œè¯„ä¼°(é€è¡Œ) é—®é¢˜ 1ã€é—®é¢˜ 2ã€é—®é¢˜ 3ã€‚ åœ¨æ¨¡å‹ q1 å’Œ q2 ä¸‹ï¼Œæœ€ä½³è®¡åˆ’åˆ†åˆ«æ˜¯ç¾éš¾æ€§è½¨è¿¹ y1 å’Œ y2ã€‚ æˆ‘ä»¬çš„è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥ç¨³å¥ (RIP) è§„åˆ’æ–¹æ³•æ±‡æ€»äº†æ•´ä½“çš„è¯„ä¼°å¹¶æå‡ºäº†å®‰å…¨è®¡åˆ’ y3ã€‚ RIP è€ƒè™‘æ¨¡å‹ä¹‹é—´çš„åˆ†æ­§ï¼Œé¿å…åœ¨ OOD ä»»åŠ¡ä¸­è¿‡åº¦è‡ªä¿¡ä½†ç¾éš¾æ€§çš„å¤–æ¨ã€‚

Although there are relatively simple strategies (e.g., stay within the lane boundaries, avoid other cars and pedestrians) that generalise, perception-based, end-to-end approaches, while flexible, they are also susceptible to spurious correlations. Therefore, they can pick up non-causal features that lead to confusion in OOD scenes (de Haan et al., 2019).

å°½ç®¡æœ‰ç›¸å¯¹ç®€å•çš„ç­–ç•¥(ä¾‹å¦‚ï¼Œç•™åœ¨è½¦é“è¾¹ç•Œå†…ï¼Œé¿å¼€å…¶ä»–æ±½è½¦å’Œè¡Œäºº)å¯ä»¥æ¦‚æ‹¬ã€åŸºäºæ„ŸçŸ¥çš„ç«¯åˆ°ç«¯æ–¹æ³•ï¼Œè™½ç„¶çµæ´»ï¼Œä½†å®ƒä»¬ä¹Ÿå®¹æ˜“å—åˆ°è™šå‡ç›¸å…³çš„å½±å“ã€‚ å› æ­¤ï¼Œä»–ä»¬å¯ä»¥æ‹¾å–å¯¼è‡´ OOD åœºæ™¯æ··æ·†çš„éå› æœç‰¹å¾(de Haan et al., 2019)ã€‚

Due to the complexity of the real-world and its everchanging dynamics, the deployed agents inevitably face novel situations and should be able to cope with them, to at least (a) identify and ideally (b) recover from them, without failing catastrophically. These desiderata are not captured by the existing benchmarks (Ros et al., 2019; Codevilla et al., 2019) and as a consequence, are not satisfied by the current state-of-the-art methods (Chen et al., 2019; Tang et al., 2019; Rhinehart et al., 2020), which are prone to fail in unpredictable ways when they experience OOD scenarios (depicted in Figure 1 and empirically verified in Section 4).

ç”±äºç°å®ä¸–ç•Œçš„å¤æ‚æ€§å’Œä¸æ–­å˜åŒ–çš„åŠ¨æ€ï¼Œéƒ¨ç½²çš„æ™ºèƒ½ä½“ä¸å¯é¿å…åœ°ä¼šé¢ä¸´æ–°æƒ…å†µï¼Œå¹¶ä¸”åº”è¯¥èƒ½å¤Ÿåº”å¯¹è¿™äº›æƒ…å†µï¼Œè‡³å°‘ (a) è¯†åˆ«å¹¶ç†æƒ³åœ° (b) ä»ä¸­æ¢å¤ï¼Œè€Œä¸ä¼šå‡ºç°ç¾éš¾æ€§çš„å¤±è´¥ã€‚ ç°æœ‰åŸºå‡†æ²¡æœ‰æ•æ‰åˆ°è¿™äº›è¿«åˆ‡éœ€æ±‚(Ros et al., 2019 ;Codevilla et al., 2019)ï¼Œå› æ­¤ï¼Œå½“å‰æœ€å…ˆè¿›çš„æ–¹æ³•æ— æ³•æ»¡è¶³è¿™äº›è¦æ±‚(Chen et al., 2019) ; Tang et al., 2019 ;Rhinehart et al., 2020)ï¼Œå½“å®ƒä»¬é‡åˆ° OOD åœºæ™¯æ—¶ï¼Œå®ƒä»¬å¾ˆå®¹æ˜“ä»¥ä¸å¯é¢„æµ‹çš„æ–¹å¼å¤±è´¥(å¦‚å›¾ 1 æ‰€ç¤ºï¼Œå¹¶åœ¨ç¬¬ 4 èŠ‚ä¸­è¿›è¡Œäº†ç»éªŒéªŒè¯)ã€‚

Figure 2. The robust imitative planning (RIP) framework. (a) Expert demonstrations. We assume access to observations x and expert state y pairs, collected either in simulation (Dosovitskiy et al., 2017) or in real-world (Caesar et al., 2019; Sun et al., 2019; Kesten et al., 2019). (b) Learning algorithm (cf. Section 3.1). We capture epistemic model uncertainty by training an ensemble of density estimators {q(y|x; Î¸k)}Kk=1, via maximum likelihood. Other approximate Bayesian deep learning methods (Gal & Ghahramani, 2016) are also tested. (c) Planning paradigm (cf. Section 3.3). The epistemic uncertainty is taken into account at planning via the aggregation operator âŠ• (e.g., mink), and the optimal plan yâˆ— is calculated online with gradient-based optimization through the learned likelihood models. 
å›¾ 2. ç¨³å¥çš„æ¨¡æ‹Ÿè§„åˆ’ (RIP) æ¡†æ¶ã€‚ (ä¸€)ä¸“å®¶è®ºè¯ã€‚ æˆ‘ä»¬å‡è®¾å¯ä»¥è®¿é—®åœ¨æ¨¡æ‹Ÿ(Dosovitskiy et al., 2017)æˆ–ç°å®ä¸–ç•Œ(Caesar et al., 2019 ;Sun et al., 2019 ;Kesten et al., 2019)ä¸­æ”¶é›†çš„è§‚å¯Ÿç»“æœ x å’Œä¸“å®¶çŠ¶æ€ y å¯¹ ). (b) å­¦ä¹ ç®—æ³•(å‚è§ç¬¬ 3.1 èŠ‚)ã€‚ æˆ‘ä»¬é€šè¿‡æœ€å¤§ä¼¼ç„¶è®­ç»ƒä¸€ç»„å¯†åº¦ä¼°è®¡é‡ {q(y|x; Î¸k)}Kk=1 æ¥æ•æ‰è®¤çŸ¥æ¨¡å‹çš„ä¸ç¡®å®šæ€§ã€‚ è¿˜æµ‹è¯•äº†å…¶ä»–è¿‘ä¼¼è´å¶æ–¯æ·±åº¦å­¦ä¹ æ–¹æ³• (Gal & Ghahramani, 2016)ã€‚ (c) è§„åˆ’èŒƒä¾‹(å‚è§ç¬¬ 3.3 èŠ‚)ã€‚ é€šè¿‡èšåˆè¿ç®—ç¬¦âŠ•(ä¾‹å¦‚ï¼Œmink)åœ¨è§„åˆ’æ—¶è€ƒè™‘äº†è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡å­¦ä¹ çš„ä¼¼ç„¶æ¨¡å‹é€šè¿‡åŸºäºæ¢¯åº¦çš„ä¼˜åŒ–åœ¨çº¿è®¡ç®—æœ€ä¼˜è®¡åˆ’ y*ã€‚

In this paper, we demonstrate the practical importance of OOD detection in AD and its importance for safety. The key contributions are summarised as follows:
1. Epistemic uncertainty-aware planning: We present an epistemic uncertainty-aware planning method, called robust imitative planning (RIP) for detecting and recovering from distribution shifts. Simple quantification of epistemic uncertainty with deep ensembles enables detection of distribution shifts. By employing Bayesian decision theory and robust control objectives, we show how we can act conservatively in unfamiliar states which often allows us to recover from distribution shifts (didactic example depicted in Figure 1).
2. Uncertainty-driven online adaptation: Our adaptive, online method, called adaptive robust imitative planning (AdaRIP), uses RIPâ€™s epistemic uncertainty estimates to efficiently query the expert for feedback which is used to adapt on-the-fly, without compromising safety. Therefore, AdaRIP could be deployed in the real world: it can reason about what it does not know and in these cases ask for human guidance to guarantee current safety and enhance future performance.
3. Autonomous car novel-scene benchmark: We introduce an autonomous car novel-scene benchmark, called CARNOVEL, to assess the robustness of AD methods to a suite of out-of-distribution tasks. In particular, we evaluate them in terms of their ability to: (a) detect OOD events, measured by the correlation of infractions and model uncertainty; (b) recover from distribution shifts, quantified by the percentage of successful manoeuvres in novel scenes and (c) efficiently adapt to OOD scenarios, provided online supervision.

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº† OOD æ£€æµ‹åœ¨ AD ä¸­çš„å®é™…é‡è¦æ€§åŠå…¶å¯¹å®‰å…¨çš„é‡è¦æ€§ã€‚ ä¸»è¦è´¡çŒ®æ€»ç»“å¦‚ä¸‹ï¼š
1. è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è§„åˆ’ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è§„åˆ’æ–¹æ³•ï¼Œç§°ä¸ºç¨³å¥æ¨¡ä»¿è§„åˆ’ (RIP)ï¼Œç”¨äºæ£€æµ‹åˆ†å¸ƒå˜åŒ–å¹¶ä»ä¸­æ¢å¤ã€‚ ä½¿ç”¨æ·±åº¦é›†åˆå¯¹è®¤çŸ¥ä¸ç¡®å®šæ€§è¿›è¡Œç®€å•é‡åŒ–å¯ä»¥æ£€æµ‹åˆ†å¸ƒå˜åŒ–ã€‚ é€šè¿‡é‡‡ç”¨è´å¶æ–¯å†³ç­–ç†è®ºå’Œç¨³å¥æ§åˆ¶ç›®æ ‡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬å¦‚ä½•åœ¨ä¸ç†Ÿæ‚‰çš„çŠ¶æ€ä¸‹é‡‡å–ä¿å®ˆè¡ŒåŠ¨ï¼Œè¿™é€šå¸¸ä½¿æˆ‘ä»¬èƒ½å¤Ÿä»åˆ†å¸ƒå˜åŒ–ä¸­æ¢å¤è¿‡æ¥(å›¾ 1 ä¸­æè¿°çš„æ•™å­¦æ ·æœ¬)ã€‚
2. ä¸ç¡®å®šæ€§é©±åŠ¨çš„åœ¨çº¿é€‚åº”ï¼šæˆ‘ä»¬çš„è‡ªé€‚åº”åœ¨çº¿æ–¹æ³•ç§°ä¸ºè‡ªé€‚åº”ç¨³å¥æ¨¡ä»¿è§„åˆ’ (AdaRIP)ï¼Œå®ƒä½¿ç”¨ RIP çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡æ¥æœ‰æ•ˆåœ°æŸ¥è¯¢ä¸“å®¶çš„åé¦ˆï¼Œç”¨äºå³æ—¶é€‚åº”ï¼Œè€Œä¸ä¼šå½±å“å®‰å…¨æ€§ã€‚ å› æ­¤ï¼ŒAdaRIP å¯ä»¥éƒ¨ç½²åœ¨ç°å®ä¸–ç•Œä¸­ï¼šå®ƒå¯ä»¥æ¨ç†å®ƒä¸çŸ¥é“çš„äº‹æƒ…ï¼Œå¹¶åœ¨è¿™äº›æƒ…å†µä¸‹è¯·æ±‚äººç±»æŒ‡å¯¼ä»¥ä¿è¯å½“å‰çš„å®‰å…¨å¹¶æé«˜æœªæ¥çš„æ€§èƒ½ã€‚
3. è‡ªåŠ¨é©¾é©¶æ±½è½¦æ–°é¢–åœºæ™¯åŸºå‡†ï¼šæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ç§°ä¸º CARNOVEL çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦æ–°é¢–åœºæ™¯åŸºå‡†ï¼Œä»¥è¯„ä¼° AD æ–¹æ³•å¯¹ä¸€ç»„åˆ†å¸ƒå¤–ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚ ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬æ ¹æ®å®ƒä»¬çš„èƒ½åŠ›æ¥è¯„ä¼°å®ƒä»¬ï¼š(a) æ£€æµ‹ OOD äº‹ä»¶ï¼Œé€šè¿‡è¿è§„å’Œæ¨¡å‹ä¸ç¡®å®šæ€§çš„ç›¸å…³æ€§æ¥è¡¡é‡;  (b) ä»åˆ†å¸ƒå˜åŒ–ä¸­æ¢å¤ï¼Œé€šè¿‡æ–°åœºæ™¯ä¸­æˆåŠŸæœºåŠ¨çš„ç™¾åˆ†æ¯”é‡åŒ–ï¼Œä»¥åŠ (c) æœ‰æ•ˆé€‚åº” OOD åœºæ™¯ï¼Œæä¾›åœ¨çº¿ç›‘ç£ã€‚

## 2. Problem Setting and Notation
We consider sequential decision-making in safety-critical domains. A method is considered safety when it is accurate, with respect to some metric (cf. Sections 4, 6), and certain.

Assumption 1 (Expert demonstrations). We assume access to a dataset, D = {(xi, yi)}Ni=1, of time-profiled expert trajectories (i.e., plans), y, paired with high-dimensional observations, x, of the corresponding scenes. The trajectories are drawn from the expert policy, y âˆ¼ Ï€expert(Â·|x).

Our goal is to approximate the (i.e., near-optimal) unknown expert policy, Ï€expert, using imitation learning (Widrow &

Smith, 1964; Pomerleau, 1989, IL), based only on the demonstrations, D. For simplicity, we also make the following assumptions, common in the autonomous driving and robotics literature (Rhinehart et al., 2020; Du et al., 2019).

Assumption 2 (Inverse dynamics). We assume access to an inverse dynamics model (Bellman, 2015, PID controller, I), which performs the low-level control â€“ inverse planning â€“ at (i.e., steering, braking and throttling), provided the current and next states (i.e., positions), st and st+1, respectively.

Therefore, we can operate directly on state-only trajectories, y = (s1, . . . , sT ), where the actions are determined by the local planner, at = I(st, st+1), âˆ€t = 1, . . . , T âˆ’ 1.

Assumption 3 (Global planner). We assume access to a global navigation system that we can use to specify highlevel goal locations G or/and commands C (e.g., turn left/right at the intersection, take the second exit).

Assumption 4 (Perfect localization). We consider the provided locations (e.g., goal, ego-vehicle positions) as accurate, i.e., filtered by a localization system.

These are benign assumptions for many applications in robotics. If required, these quantities can also be learned from data, and are typically easier to learn than Ï€expert.




## 3. Robust Imitative Planning
We seek an imitation learning method that (a) provides a distribution over expert plans; (b) quantifies epistemic uncertainty to allow for detection of OOD observations and (c) enables robustness to distribution shift with an explicit mechanism for recovery. Our method is shown in Figure 2.

First, we present the model used for imitating the expert.

### 3.1. Bayesian Imitative Model
We perform context-conditioned density estimation of the distribution over future expert trajectories (i.e., plans), using a probabilistic â€œimitativeâ€ model q(y|x; Î¸), trained via maximum likelihood estimation (MLE): Î¸MLE = arg max Î¸ E(x,y)âˆ¼D [log q(y|x; Î¸)] . (1)

Contrary to existing methods in AD (Rhinehart et al., 2020;

Chen et al., 2019), we place a prior distribution p(Î¸) over possible model parameters Î¸, which induces a distribution over the density models q(y|x; Î¸). After observing data D, the distribution over density models has a posterior p(Î¸|D).

Practical implementation. We use an autoregressive neural density estimator (Rhinehart et al., 2018), depicted in

Figure 2b, as the imitative model, parametrised by learnable parameters Î¸. The likelihood of a plan y in context x to come from an expert (i.e., imitation prior) is given by: q(y|x; Î¸) =

TYt=1 p(st|y<\t, x; Î¸) = TYt=1

N (st; Âµ(y<\t, x; Î¸), Î£(y<\t, x; Î¸)), (2) where Âµ(Â·; Î¸) and Î£(Â·; Î¸) are two heads of a recurrent neural network, with shared torso. We decompose the imitation prior as a telescopic product (cf. Eqn. (2)), where conditional densities are assumed normally distributed, and the distribution parameters are learned (cf. Eqn. (1)). Despite the unimodality of normal distributions, the autoregression (i.e., sequential sampling of normal distributions where the future samples depend on the past) allows to model multimodal distributions (Uria et al., 2016). Although more expressive alternatives exist, such as the mixture of density networks (Bishop, 1994) and normalising flows (Rezende &

Mohamed, 2015), we empirically find Eqn. (2) sufficient.

The estimation of the posterior of the model parameters, p(Î¸|D), with exact inference is intractable for non-trivial models (Neal, 2012). We use ensembles of deep imitative models as a simple approximation to the posterior p(Î¸|D).

We consider an ensemble of K components, using Î¸k to refer to the parameters of our k-th model qk, trained with via maximum likelihood (cf. Eqn. (1) and Figure 2b). However, any (approximate) inference method to recover the posterior p(Î¸|D) would be applicable. To that end, we also try Monte

Carlo dropout (Gal & Ghahramani, 2016).

### 3.2. Detecting Distribution Shifts
The log-likelihood of a plan log q(y|x; Î¸) (i.e., imitation prior) is a proxy of the quality of a plan y in context x under model Î¸. We detect distribution shifts by looking at the disagreement of the qualities of a plan under models coming from the posterior, p(Î¸|D). We use the variance of the imitation prior with respect to the model posterior, i.e., u(y) , Varp(Î¸|D) [log q(y|x; Î¸)] (3) to quantify the model disagreement: Plans at in-distribution scenes have low variance, but high variance in OOD scenes.

We can efficiently calculate Eqn. (3) when we use ensembles, or Monte Carlo, sampling-based methods for p(Î¸|D).

Having to commit to a decision, just the detection of distribution shifts via the quantification of epistemic uncertainty is insufficient for recovery. Next, we introduce an epistemic uncertainty-aware planning objective that allows for robustness to distribution shifts.

### 3.3. Planning Under Epistemic Uncertainty
We formulate planning to a goal location G under epistemic uncertainty, i.e., posterior over model parameters p(Î¸|D), as the optimization (Barber, 2012) of the generic objective, which we term robust imitative planning (RIP): yG

RIP , arg max y aggregation operator z }| { âŠ• Î¸âˆˆsupp p(Î¸|D) log p(y|G, x; Î¸) | {z} imitation posterior = arg max y âŠ• Î¸âˆˆsupp p(Î¸|D) logq(y|x; Î¸) | {z} imitation prior + log p(G|y) | {z} goal likelihood , (4) where âŠ• is an operator (defined below) applied on the posterior p(Î¸|D) and the goal-likelihood is given, for example, by a Gaussian centred at the final goal location sGT and a pre-specified tolerance  , p(G|y) = N (yT ; yGT , 2I).

Intuitively, we choose the plan yG

RIP that maximises the likelihood to have come from an expert demonstrator (i.e., â€œimitation priorâ€) and is â€œcloseâ€ to the goal G. The model posterior p(Î¸|D) represents our belief (uncertainty) about the true expert model, having observed data D and from prior p(Î¸) and the aggregation operator âŠ• determines our level of awareness to uncertainty under a unified framework.

For example, a deep imitative model (Rhinehart et al., 2020) is a particular instance of the more general family of objectives described by Eqn. (4), where the operator âŠ• selects a


? single Î¸k from the posterior (point estimate). However, this approach is oblivious to the epistemic uncertainty and prone to fail in unfamiliar scenes (cf. Section 4).

In contrast, we focus our attention on two aggregation operators due to their favourable properties, which take epistemic uncertainty into account: (a) one inspired by robust control (Wald, 1939) which encourages pessimism in the face of uncertainty and (b) one from Bayesian decision theory, which marginalises the epistemic uncertainty. Table 1 summarises the different operators considered in our experiments. Next, we motivate the used operators.

#### 3.3.1. WORST CASE MODEL (RIP-WCM)
In the face of (epistemic) uncertainty, robust control (Wald, 1939) suggests to act pessimistically â€“ reason about the worst case scenario and optimise it. All models with nonzero posterior probability p(Î¸|D) are likely and hence our robust imitative planning with respect to the worst case model (RIP-WCM) objective acts with respect to the most pessimistic model, i.e., sRIP-WCM , arg max y min Î¸âˆˆsupp p(Î¸|D) log q(y|x; Î¸). (5)

The solution of the arg maxy minÎ¸ optimization problem in Eqn. (5) is generally not tractable, but our deep ensemble approximation enables us to solve it by evaluating the minimum over a finite number of K models. The maximization over plans, y, is solved with online gradient-based adaptive optimization, specifically ADAM (Kingma & Ba, 2014).

An alternative online planning method with a trajectory library (Liu & Atkeson, 2009) (c.f. Appendix D) is used too but its performance in OOD scenes is noticeably worse than online gradient descent.

Alternative, â€œsofterâ€ robust operators can be used instead of the minimum, including the Conditional Value at Risk (Embrechts et al., 2013; Rajeswaran et al., 2016, CVaR) that employs quantiles. CVaR may be more useful in cases of full support model posterior, where there may be a pessimistic but trivial model, for example, due to misspecification of the prior, p(Î¸), or due to the approximate inference procedure. Mean-variance optimization (Kahn et al., 2017;

Kenton et al., 2019) can be also used, aiming to directly minimise the distribution shift metric, as defined in Eqn. (3).

Next, we present a different aggregator for epistemic uncertainty that is not as pessimistic as RIP-WCM and, as found empirically, works sufficiently well too.

#### 3.3.2. MODEL AVERAGING (RIP-MA)
In the face of (epistemic) uncertainty, Bayesian decision theory (Barber, 2012) uses the predictive posterior (i.e., model averaging), which weights each modelâ€™s contribution according to its posterior probability, i.e., sRIP-MA , arg max y Z p(Î¸|D)log q(y|x; Î¸)dÎ¸ . (6)

Despite the intractability of the exact integration, the ensemble approximation used allows us to efficiently estimate and optimise the objective. We call this method robust imitative planning with model averaging (RIP-MA), where the more likely modelsâ€™ impacts are up-weighted according to the predictive posterior.

From a multi-objective optimization point of view, we can interpret the log-likelihood, log q(y|x; Î¸), as the utility of a task Î¸, with importance p(Î¸|D), given by the posterior density. Then RIP-MA in Eqn. (6) gives the Pareto efficient solution (Barber, 2012) for the tasks Î¸ âˆˆ supp p(Î¸|D) .

Table 1. Robust imitative planning (RIP) unified framework. The different aggregation operators applied on the posterior distribution p(Î¸|D), approximated with the deep ensemble (Lakshminarayanan et al., 2017) components Î¸k.

Methods Operator âŠ• Interpretation

Imitative Models log qk=1 Sample

Best Case (RIP-BCM) maxk log qk Max

Robust Imitative Planning (ours)

Model Average (RIP-MA) P k log qk Geometric Mean

Worst Case (RIP-WCM) mink log qk Min (a) nuScenes (b) CARNOVEL

Figure 3. RIPâ€™s (ours) robustness to OOD scenarios, compared to (Codevilla et al., 2018, CIL) and (Rhinehart et al., 2020, DIM).

## 4. Benchmarking Robustness to Novelty
We designed our experiments to answer the following questions: Q1. Can autonomous driving, imitation-learning, epistemic-uncertainty unaware methods detect distribution shifts? Q2. How robust are these methods under distribution shifts, i.e., can they recover? Q3. Does RIPâ€™s epistemic uncertainty quantification enable identification of novel scenes? Q4. Does RIPâ€™s explicit mechanism for recovery from distribution shifts lead to improved performance?

To that end, we conduct experiments both on real data, in




Table 2. We evaluate different autonomous driving prediction methods in terms of their robustness to distribution scene, in the nuScenes

ICRA 2020 challenge (Phan-Minh et al., 2019). We use the provided trainâ€“valâ€“test splits and report performance on the test (i.e., out-of-sample) scenarios. A â€œâ™£â€ indicates methods that use LIDAR observation, as in (Rhinehart et al., 2019), and a â€œâ™¦â€ methods that use bird-view privileged information, as in (Phan-Minh et al., 2019). A â€œF â€ indicates that we used the results from the original paper, otherwise we used our implementation. Standard errors are in gray (via bootstrap sampling). The outperforming method is in bold.

Boston Singapore minADE1 â†“ minADE5 â†“ minFDE1 â†“ minADE1 â†“ minADE5 â†“ minFDE1 â†“

Methods (2073 scenes, 50 samples, open-loop planning) (1189 scenes, 50 samples, open-loop planning)

MTPâ™¦F (Cui et al., 2019) 4.13 3.24 9.23 4.13 3.24 9.23

MultiPathâ™¦F (Chai et al., 2019) 3.89 3.34 9.19 3.89 3.34 9.19

CoverNetâ™¦F (Phan-Minh et al., 2019) 3.87 2.41 9.26 3.87 2.41 9.26

DIMâ™£ (Rhinehart et al., 2020) 3.64Â±0.05 2.48Â±0.02 8.22Â±0.13 3.82Â±0.04 2.95Â±0.01 8.91Â±0.08

RIP-BCMâ™£ (baseline, cf. Table 1) 3.53Â±0.04 2.37Â±0.01 7.92Â±0.09 3.57Â±0.02 2.70Â±0.01 8.39Â±0.03

RIP-MAâ™£ (ours, cf. Section 3.3.2) 3.39Â±0.03 2.33Â±0.01 7.62Â±0.07 3.48Â±0.01 2.69Â±0.02 8.19Â±0.02

RIP-WCMâ™£ (ours, cf. Section 3.3.1) 3.29Â±0.03 2.28Â±0.00 7.45Â±0.05 3.43Â±0.01 2.66Â±0.01 8.09Â±0.04

Section 4.1, and on simulated scenarios, in Section 4.2, comparing our method (RIP) against current state-of-the-art driving methods.

### 4.1. nuScenes
We first compare our robust planning objectives (cf. Eqn. (5â€“ 6)) against existing state-of-the-art imitation learning methods in a prediction task (Phan-Minh et al., 2019), based on nuScenes (Caesar et al., 2019), the public, real-world, large-scale dataset for autonomous driving. Since we do not have control over the scenes split, we cannot guarantee that the evaluation is under distribution shifts, but only test out-of-sample performance, addressing question Q4.

#### 4.1.1. METRICS
For fair comparison with the baselines, we use the metrics from the ICRA 2020 nuScenes prediction challenge.

Displacement error. The quality of a plan, y, with respect to the ground truth prediction, yâˆ— is measured by the average displacement error, i.e.,

ADE(y) , 1T TXt=1 k st âˆ’ sâˆ—t k , (7) where y = (s1, . . . , sT ). Stochastic models, such as our imitative model, q(y|x; Î¸), can be evaluated based on their samples, using the minimum (over k samples) ADE (i.e., minADEk), i.e., minADEk(q) , min {yi}ki=1âˆ¼q(y|x)

ADE(yi). (8)

In prior work, Phan-Minh et al. (2019) studied minADEk for k > 1 in order to assess the quality of the generated samples from a model, q. Although we report minADEk for k = {1, 5}, we are mostly interested in the decision-making (planning) task, where the driving agent commits to a single plan, k = 1. We also study the final displacement error (FDE), or equivalently minFDE1, i.e., minFDE1(y) , k sT âˆ’ sâˆ—T k . (9)

#### 4.1.2. BASELINES
We compare our contribution to state-of-the-art methods in the nuScenes dataset: the Multiple-Trajectory Prediction (Cui et al., 2019, MTP), MultiPath (Chai et al., 2019) and CoverNet (Phan-Minh et al., 2019), all of which score a (fixed) set of trajectories, i.e., trajectory library (Liu &

Atkeson, 2009). Moreover, we implement the Deep Imitative Model (Rhinehart et al., 2020, DIM) and an optimistic variant of RIP, termed RIP-BCM and described in Table 1.

#### 4.1.3. OFFLINE FORECASTING EXPERIMENTS
We use the provided train-val-test splits from (Phan-Minh et al., 2019), for towns Boston and Singapore. For all methods we use N = 50 trajectories, and in case of both

DIM and RIP, we only optimise the â€œimitation priorâ€ (cf.

Eqn. 4), since goals are not provided, running N planning procedures with different random initializations. The performance of the baselines and our methods are reported on

Table 2. We can affirmatively answer Q4 since RIP consistently outperforms the current state-of-the-art methods in out-of-sample evaluation. Moreover, Q2 can be partially answered, since the epistemic-uncertainty-unaware baselines underperformed compared to RIP.

Nonetheless, since we do not have full control over train and test splits at the ICRA 2020 challenge and hence we cannot introduce distribution shifts, we are not able to address questions Q1 and Q3 with the nuScenes benchmark. To that end, we now introduce a control benchmark based on the

CARLA driving simulator (Dosovitskiy et al., 2017).




Table 3. We evaluate different autonomous driving methods in terms of their robustness to distribution shifts, in our new benchmark,CARNOVEL. All methods are trained on CARLA Town01 using imitation learning on expert demonstrations from the autopilot (Dosovitskiy et al., 2017). A â€œâ€ â€ indicates methods that use first-person camera view, as in (Chen et al., 2019), a â€œâ™£â€ methods that use LIDAR
 observation, as in (Rhinehart et al., 2020) and a â€œâ™¦â€ methods that use the ground truth game engine state, as in (Chen et al., 2019). A â€œF â€ indicates that we used the reference implementation from the original paper, otherwise we used our implementation. For all the scenes we chose pairs of start-destination locations and ran 10 trials with randomised initial simulator state for each pair. Standard errors are in gray (via bootstrap sampling). The outperforming method is in bold. The complete CARNOVEL benchmark results are in Appendix B.

AbnormalTurns Hills Roundabouts

Success â†‘ Infra/km â†“ Success â†‘ Infra/km â†“ Success â†‘ Infra/km â†“

Methods (7 Ã— 10 scenes, %) (Ã—1eâˆ’3) (4 Ã— 10 scenes, %) (Ã—1eâˆ’3) (5 Ã— 10 scenes, %) (Ã—1eâˆ’3)

CILâ™£F (Codevilla et al., 2018) 65.71Â±07.37 7.04Â±5.07 60.00Â±29.34 4.74Â±3.02 20.00Â±00.00 4.60Â±3.23

LbCâ€ F (Chen et al., 2019) 00.00Â±00.00 5.81Â±0.58 50.00Â±00.00 1.61Â±0.15 08.00Â±10.95 3.70Â±0.72

LbC-GTâ™¦F (Chen et al., 2019) 02.86Â±06.39 3.68Â±0.34 05.00Â±11.18 3.36Â±0.26 00.00Â±00.00 6.47Â±0.99

DIMâ™£ (Rhinehart et al., 2020) 74.28Â±11.26 5.56Â±4.06 70.00Â±10.54 6.87Â±4.09 20.00Â±09.42 6.19Â±4.73

RIP-BCMâ™£ (baseline, cf. Table 1) 68.57Â±09.03 7.93Â±3.73 75.00Â±00.00 5.49Â±4.03 06.00Â±09.66 6.78Â±7.05

RIP-MAâ™£ (ours, cf. Section 3.3.2) 84.28Â±14.20 7.86Â±5.70 97.50Â±07.90 0.26Â±0.54 38.00Â±06.32 5.48Â±5.56

RIP-WCMâ™£ (ours, cf. Section 3.3.1) 87.14Â±14.20 4.91Â±3.60 87.50Â±13.17 1.83Â±1.73 42.00Â±06.32 4.32Â±1.91

### 4.2. CARNOVEL
In order to access the robustness of AD methods to novel,

OOD driving scenarios, we introduce a benchmark, called

CARNOVEL. In particular, CARNOVEL is built on the
CARLA simulator (Dosovitskiy et al., 2017). Offline expert demonstrations1 from Town01 are provided for training.

Then, the driving agents are evaluated on a suite of OOD navigation tasks, including but not limited to roundabouts, challenging non-right-angled turns and hills, none of which are experienced during training. The CARNOVEL tasks are summarised in Appendix A. Next, we introduce metrics that quantify and help us answer questions Q1, Q3.

#### 4.2.1. METRICS
Since we are studying navigation tasks, agents should be able to reach safely pre-specified destinations. As done also in previous work (Codevilla et al., 2018; Rhinehart et al., 2020; Chen et al., 2019), the infractions per kilometre metric (i.e., violations of rules of the road and accidents per driven kilometre) measures how safely the agent navigates.

The success rate measures the percentage of successful navigations to the destination, without any infraction. However, these standard metrics do not directly reflect the methodsâ€™ performance under distribution shifts. As a result, we introduce two new metrics for quantifying the performance in out-of-training distribution tasks:

Detection score. The correlation of infractions and modelâ€™s uncertainty termed detection score is used to measure a methodâ€™s ability to predict the OOD scenes that lead to catastrophic events. As discussed by Michelmore et al. 1 using the CARLA rule-based autopilot (Dosovitskiy et al., 2017) without actuator noise. (2018), we look at time windows of 4 seconds (Taoka, 1989;

Coley et al., 2009). A method that can detect potential infractions should have high detection score.

Recovery score. The percentage of successful manoeuvres in novel scenes â€” where the uncertainty-unaware methods fail â€” is used to quantify a methodâ€™s ability to recover from distribution shifts. We refer to this metric as recovery score.

A method that is oblivious to novelty should have 0 recovery score, but positive otherwise.

#### 4.2.2. BASELINES
We compare RIP against the current state-of-the-art imitation learning methods in the CARLA benchmark (Codevilla et al., 2018; Rhinehart et al., 2020; Chen et al., 2019). Apart from DIM and RIP-BCM, discussed in Section 4.1.2, we also benchmark:

Conditional imitation learning (Codevilla et al., 2018,

CIL) is a discriminative behavioural cloning method that conditions its predictions on contextual information (e.g., LIDAR) and high-level commands (e.g., turn left, go straight).

Learning by cheating (Chen et al., 2019, LbC) is a method that builds on CIL and uses (cross-modal) distillation of privileged information (e.g., game state, rich, annotated bird-eye-view observations) to a sensorimotor agent. For reference, we also evaluate the agent who has uses privileged information directly (i.e., teacher), which we term LbC-GT.

#### 4.2.3. ONLINE PLANNING EXPERIMENTS
All the methods are trained on offline expert demonstrations from CARLA Town01. We perform 10 trials per

CARNOVEL task with randomised initial simulator state and the results are reported on Table 3 and Appendix B.


? 0 1 2 3

Number of Demos 0 50 100 (a) AbnormalTurns4-v0 0 1 2 3

Number of Demos 0 50 100 (b) BusyTown2-v0 0 1 2 3

Number of Demos 0 50 100 (c) Hills1-v0 0 1 2 3

Number of Demos 0 50 100 (d) Roundabouts1-v0

Figure 4. Adaptation scores of AdaRIP (cf. Section 5) on CARNOVEL tasks that RIP-WCM and RIP-MA (cf. Section 3) do worst. We observe that as the number of online expert demonstrations increases, the success rate improves thanks to online model adaptation.

Our robust imitative planning (i.e., RIP-WCM and RIPMA) consistently outperforms the current state-of-the-art imitation learning-based methods in novel, OOD driving scenarios. In alignment with the experimental results from nuScenes (cf. Section 4.1), we address questions Q4 and

Q2, reaching the conclusion that RIPâ€™s epistemic uncertainty explicit mechanism for recovery improves its performance under distribution shifts, compared to epistemic uncertaintyunaware methods. As a result, RIPâ€™s recovery score (cf.

Section 4.2.1) is higher than the baselines.

Towards distribution shift detection and answering questions

Q1 and Q3, we collect 50 scenes for each method that led to a crash, record the uncertainty 4 seconds (Taoka, 1989) before the accident and assert if the uncertainties can be used for detection. RIPâ€™s (ours) predictive variance (cf.

Eqn. (3)) serves as a useful detector, while DIMâ€™s (Rhinehart et al., 2020) negative log-likelihood was unable to detect catastrophes. The results are illustrated on Figure 5.

Despite RIPâ€™s improvement over current state-of-the-art methods with 97.5% success rate and 0.26 infractions per driven kilometre (cf. Table 3), the safety-critical nature of the task mandates higher performance. Towards this goal, we introduce an online adaptation variant of RIP. 0 20 40 60 80 100



Figure 5. Uncertainty estimators as indicators of catastrophes on

CARNOVEL. We collect 50 scenes for each model that led to a
 crash, record the uncertainty 4 seconds (Taoka, 1989) before the accident and assert if the uncertainties can be used for detection.

RIPâ€™s (ours) predictive variance (in blue, cf. Eqn. (3)) serves as a useful detector, while DIMâ€™s (Rhinehart et al., 2020) negative loglikelihood (in orange) cannot be used for detecting catastrophes.

## 5. Adaptive Robust Imitative Planning
We empirically observe that the quantification of epistemic uncertainty and its use in the RIP objectives is not always sufficient to recover from shifts away from the training distribution (cf. Section 4.2.3). However, we can use uncertainty estimates to ask the human driver to take back control or default to a safe policy, avoiding potential infractions. In the former case, the human driverâ€™s behaviors can be recorded and used to reduce RIPâ€™s epistemic uncertainty via online adaptation. The epistemic uncertainty is reducible and hence it can be eliminated, provided enough demonstrations.

We propose an adaptive variant of RIP, called AdaRIP, which uses the epistemic uncertainty estimates to decide when to query the human driver for feedback, which is used to update its parameters online, adapting to arbitrary new driving scenarios. AdaRIP relies on external, online feedback from an expert demonstrator2 , similar to DAgger (Ross et al., 2011) and its variants (Zhang & Cho, 2016; Cronrath et al., 2018).

However, unlike this prior work, AdaRIP uses an epistemic uncertainty-aware acquisition mechanism. AdaRIPâ€™s pseudocode is given in Algorithm 1.

The uncertainty (i.e., variance) threshold, Ï„ , is calibrated on a validation dataset, such that it matches a pre-specified level of false negatives, using a similar analysis to Figure 5.

## 6. Benchmarking Adaptation
The goal of this section is to provide experimental evidence for answering the following questions: Q5. Can RIPâ€™s epistemic-uncertainty estimation be used for efficiently querying an expert for online feedback (i.e., demonstrations)? Q6. Does AdaRIPâ€™s online adaptation mechanism improve success rate?

We evaluate AdaRIP on CARNOVEL tasks, where the

CARLA autopilot (Dosovitskiy et al., 2017) is queried for demonstrations online when the predictive variance (cf.

Eqn. (3)) exceeds a threshold, chosen according to RIPâ€™s detection score, (cf. Figure 5). We measure performance 2AdaRIP is also compatible with other feedback mechanisms, such as expert preferences (Christiano et al., 2017) or explicit reward functions (de Haan et al., 2019).

Success, % Binary Accuracy


? novel (OOD) in-distribution (a) Data distribution (b) Domain Randomization

ENC (c) Domain adaptation (d) Online adaptation

Figure 6. Common approaches to distribution shift, as in (a) there are novel (OOD) points that are outside the support of the training data: (b) domain randomization (e.g., Sadeghi & Levine (2016)) covers the data distribution by exhaustively sampling configurations from a simulator; (c) domain adaptation (e.g., McAllister et al. (2019)) projects (or encodes) the (OOD) points to the in-distribution space and (d) online adaptation (e.g., Ross et al. (2011)) progressively expands the in-distribution space by incorporating online, external feedback. according to the:

Adaptation score. The improvement in success rate as a function of number of online expert demonstrations is used to measure a methodâ€™s ability to adapt efficiently online. We refer to this metric as adaptation score. A method that can adapt online should have a positive adaptation score.

AdaRIPâ€™s performance on the most challenging CARNOVEL tasks is summarised in Figure 4, where, as expected, the success rate improves as the number of online demonstrations increases. Qualitative examples are illustrated in Appendix C.

Although AdaRIP can adapt to any distribution shift, it is prone to catastrophic forgetting and sample-inefficiency, as many online methods (French, 1999). In this paper, we only demonstrate AdaRIPâ€™s efficacy to adapt under distribution shifts and do not address either of these limitations.

Future work lies in providing a practical, sample-efficient algorithm to be used in conjunction with the AdaRIP framework. Methods for efficient (e.g., few-shot or zero-shot) and safe adaptation (Finn et al., 2017; Zhou et al., 2019) are orthogonal to AdaRIP and hence any improvement in these fields could be directly used for AdaRIP.

## 7. Related Work
### Imitation learning. 
Learning from expert demonstrations (i.e., imitation learning (Widrow & Smith, 1964; Pomerleau, 1989, IL)) is an attractive framework for sequential decisionmaking in safety-critical domains such as autonomous driving, where trial and error learning has little to no safety guarantees during training. A plethora of expert driving demonstrations has been used for IL (Caesar et al., 2019; Sun et al., 2019; Kesten et al., 2019) since a model mimicking expert demonstrations can simply learn to stay in â€œsafeâ€, expert-like parts of the state space and no explicit reward function need be specified.

æ¨¡ä»¿å­¦ä¹ ã€‚ ä»ä¸“å®¶ç¤ºèŒƒä¸­å­¦ä¹ (å³æ¨¡ä»¿å­¦ä¹ (Widrow & Smithï¼Œ1964 ;Pomerleauï¼Œ1989ï¼Œä¼Šåˆ©è¯ºä¼Šå·))æ˜¯åœ¨è‡ªåŠ¨é©¾é©¶ç­‰å®‰å…¨å…³é”®é¢†åŸŸè¿›è¡Œé¡ºåºå†³ç­–çš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„æ¡†æ¶ï¼Œåœ¨è¿™äº›é¢†åŸŸï¼Œè¯•é”™å­¦ä¹ å‡ ä¹æ²¡æœ‰å®‰å…¨æ€§ è®­ç»ƒæœŸé—´çš„ä¿è¯ã€‚ å¤§é‡ä¸“å®¶é©¾é©¶æ¼”ç¤ºå·²ç”¨äº IL(Caesar et al., 2019 ;Sun et al., 2019 ;Kesten et al., 2019)ï¼Œå› ä¸ºæ¨¡ä»¿ä¸“å®¶æ¼”ç¤ºçš„æ¨¡å‹å¯ä»¥ç®€å•åœ°å­¦ä¼šä¿æŒâ€œå®‰å…¨â€ï¼Œä¸“å®¶ -ç±»ä¼¼äºçŠ¶æ€ç©ºé—´çš„éƒ¨åˆ†ï¼Œä¸éœ€è¦æŒ‡å®šæ˜ç¡®çš„å¥–åŠ±å‡½æ•°ã€‚

On the one hand, behavioural cloning approaches (Liang et al., 2018; Sauer et al., 2018; Li et al., 2018; Codevilla et al., 2018; 2019; Chen et al., 2019) fit commandconditioned discriminative sequential models to expert demonstrations, which are used in deployment to produce expert-like trajectories. On the other hand, Rhinehart et al. (2020) proposed command-unconditioned expert trajectory density models which are used for planning trajectories that both satisfy the goal constraints and are likely under the expert model. However, both of these approaches fit pointestimates to their parameters, thus do not quantify their model (epistemic) uncertainty, as explained next. This is especially problematic when estimating what an expert would or would not do in unfamiliar, OOD scenes. In contrast, our methods, RIP and AdaRIP, does quantify epistemic uncertainty in order to both improve planning performance and triage situations in which an expert should intervene.

ä¸€æ–¹é¢ï¼Œè¡Œä¸ºå…‹éš†æ–¹æ³•(Liang et al., 2018 ;Sauer et al., 2018 ;Li et al., 2018 ;Codevilla et al., 2018 ;2019 ;Chen et al., 2019)é€‚åˆå‘½ä»¤æ¡ä»¶åˆ¤åˆ«é¡ºåº æ¨¡å‹åˆ°ä¸“å®¶æ¼”ç¤ºï¼Œç”¨äºéƒ¨ç½²ä»¥äº§ç”Ÿç±»ä¼¼ä¸“å®¶çš„è½¨è¿¹ã€‚ å¦ä¸€æ–¹é¢ï¼ŒRhinehart et al. (2020) æå‡ºäº†å‘½ä»¤æ— æ¡ä»¶ä¸“å®¶è½¨è¿¹å¯†åº¦æ¨¡å‹ï¼Œç”¨äºè§„åˆ’æ—¢æ»¡è¶³ç›®æ ‡çº¦æŸåˆå¯èƒ½åœ¨ä¸“å®¶æ¨¡å‹ä¸‹çš„è½¨è¿¹ã€‚ ç„¶è€Œï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½å°†ç‚¹ä¼°è®¡æ‹Ÿåˆåˆ°å®ƒä»¬çš„å‚æ•°ï¼Œå› æ­¤ä¸é‡åŒ–å®ƒä»¬çš„æ¨¡å‹(è®¤çŸ¥)ä¸ç¡®å®šæ€§ï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚ åœ¨ä¼°è®¡ä¸“å®¶åœ¨ä¸ç†Ÿæ‚‰çš„ OOD åœºæ™¯ä¸­ä¼šåšä»€ä¹ˆæˆ–ä¸ä¼šåšä»€ä¹ˆæ—¶ï¼Œè¿™å°¤å…¶æˆé—®é¢˜ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³• RIP å’Œ AdaRIP ç¡®å®é‡åŒ–äº†è®¤çŸ¥ä¸ç¡®å®šæ€§ï¼Œä»¥æé«˜è§„åˆ’ç»©æ•ˆå’Œä¸“å®¶åº”è¯¥å¹²é¢„çš„åˆ†ç±»æƒ…å†µã€‚

### Novelty detection & epistemic uncertainty. 
A principled means to capture epistemic uncertainty is with Bayesian inference to compute the predictive distribution. However, evaluating the posterior p(Î¸|D) with exact inference is intractable for non-trivial models (Neal, 2012). Approximate inference methods (Graves, 2011; Blundell et al., 2015; Gal & Ghahramani, 2016; HernÂ´andez-Lobato & Adams, 2015) have been introduced that can efficiently capture epistemic uncertainty. One approximation for epistemic uncertainty in deep models is model ensembles (Lakshminarayanan et al., 2017; Chua et al., 2018). Prior work by Kahn et al. (2017) and Kenton et al. (2019) use ensembles of deep models to detect and avoid catastrophic actions in navigation tasks, although they can not recover from or adapt to distribution shifts. Our epistemic uncertainty-aware planning objective, RIP, instead, managed to recover from some distribution shifts, as shown experimentally in Section 4.

æ–°é¢–æ€§æ£€æµ‹å’Œè®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚ æ•è·è®¤çŸ¥ä¸ç¡®å®šæ€§çš„ä¸€ç§åŸåˆ™æ€§æ–¹æ³•æ˜¯ä½¿ç”¨è´å¶æ–¯æ¨ç†æ¥è®¡ç®—é¢„æµ‹åˆ†å¸ƒã€‚ ç„¶è€Œï¼Œå¯¹äºéå¹³å‡¡æ¨¡å‹ï¼Œä½¿ç”¨ç²¾ç¡®æ¨ç†è¯„ä¼°åéªŒ p(Î¸|D) æ˜¯å¾ˆæ£˜æ‰‹çš„ (Neal, 2012)ã€‚ è¿‘ä¼¼æ¨ç†æ–¹æ³• (Graves, 2011; Blundell et al., 2015; Gal & Ghahramani, 2016; HernÂ´andez-Lobato & Adams, 2015) å·²ç»è¢«å¼•å…¥ï¼Œå¯ä»¥æœ‰æ•ˆåœ°æ•æ‰è®¤çŸ¥ä¸ç¡®å®šæ€§ã€‚ æ·±åº¦æ¨¡å‹ä¸­è®¤çŸ¥ä¸ç¡®å®šæ€§çš„ä¸€ç§è¿‘ä¼¼æ˜¯æ¨¡å‹é›†æˆ(Lakshminarayanan et al., 2017 ;Chua et al., 2018)ã€‚ Kahn et al ä¹‹å‰çš„å·¥ä½œã€‚ (2017) å’Œ Kenton et al. (2019) ä½¿ç”¨æ·±åº¦æ¨¡å‹çš„é›†åˆæ¥æ£€æµ‹å’Œé¿å…å¯¼èˆªä»»åŠ¡ä¸­çš„ç¾éš¾æ€§è¡Œä¸ºï¼Œå°½ç®¡å®ƒä»¬æ— æ³•ä»åˆ†å¸ƒå˜åŒ–ä¸­æ¢å¤æˆ–é€‚åº”åˆ†å¸ƒå˜åŒ–ã€‚ ç›¸åï¼Œæˆ‘ä»¬çš„è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è§„åˆ’ç›®æ ‡ RIP è®¾æ³•ä»ä¸€äº›åˆ†å¸ƒå˜åŒ–ä¸­æ¢å¤ï¼Œå¦‚ç¬¬ 4 èŠ‚ä¸­çš„å®éªŒæ‰€ç¤ºã€‚

### Coping with distribution shift. 
Strategies to cope with distribution shift include (a) domain randomization; (b) domain adaptation and (c) online adaptation. Domain randomization assumes access to a simulator and exhaustively searches for configurations that cover all the data distribution support in order to eliminate OOD scenes, as illustrated in Figure 6b. This approach has been successfully used in simple robotic tasks (Sadeghi & Levine, 2016; OpenAI et al., 2018; Akkaya et al., 2019) but it is impractical for use in large, real-world tasks, such as AD. Domain adaptation and bisimulation (Castro & Precup, 2010), depicted in Figure 6c, tackle OOD points by projecting them back to in-distribution points, that are â€œcloseâ€ to training points according to some metric. Despite its success in simple visual tasks (McAllister et al., 2019), it has no guarantees under arbitrary distribution shifts. In contrast, online learning methods (Cesa-Bianchi & Lugosi, 2006; Ross et al., 2011; Zhang & Cho, 2016; Cronrath et al., 2018) have no-regret guarantees and, provided frequent expert supervision, they asymptotically cover the whole data distributionâ€™s support, adaptive to any distribution shift, as shown in Figure 6d. In order to continually cope with distribution shift, a learner must receive interactive feedback (Ross et al., 2011), however, the frequency of this costly feedback should be minimised. Our epistemic-uncertainty-aware method, Robust Imitative Planning can cope with some OOD events, thereby reducing the systemâ€™s dependency on expert feedback, and can use this uncertainty to decide when it cannot copeâ€“when the expert must intervene.

åº”å¯¹åˆ†é…è½¬å˜ã€‚ åº”å¯¹åˆ†å¸ƒè½¬å˜çš„ç­–ç•¥åŒ…æ‹¬(a)åŸŸéšæœºåŒ–;  (b) åŸŸé€‚åº”å’Œ (c) åœ¨çº¿é€‚åº”ã€‚ åŸŸéšæœºåŒ–å‡å®šè®¿é—®æ¨¡æ‹Ÿå™¨å¹¶è¯¦å°½åœ°æœç´¢æ¶µç›–æ‰€æœ‰æ•°æ®åˆ†å¸ƒæ”¯æŒçš„é…ç½®ï¼Œä»¥æ¶ˆé™¤ OOD åœºæ™¯ï¼Œå¦‚å›¾ 6b æ‰€ç¤ºã€‚ è¿™ç§æ–¹æ³•å·²æˆåŠŸç”¨äºç®€å•çš„æœºå™¨äººä»»åŠ¡(Sadeghi & Levineï¼Œ2016 ;OpenAI et al., 2018 ;Akkaya et al., 2019)ï¼Œä½†ç”¨äºå¤§å‹ç°å®ä¸–ç•Œä»»åŠ¡(ä¾‹å¦‚ AD)æ˜¯ä¸åˆ‡å®é™…çš„ã€‚ åŸŸé€‚åº”å’ŒåŒå‘æ¨¡æ‹Ÿ(Castro & Precupï¼Œ2010)ï¼Œå¦‚å›¾ 6c æ‰€ç¤ºï¼Œé€šè¿‡å°† OOD ç‚¹æŠ•å½±å›åˆ†å¸ƒç‚¹æ¥è§£å†³è¿™äº›ç‚¹ï¼Œè¿™äº›ç‚¹æ ¹æ®æŸäº›æŒ‡æ ‡â€œæ¥è¿‘â€è®­ç»ƒç‚¹ã€‚ å°½ç®¡å®ƒåœ¨ç®€å•çš„è§†è§‰ä»»åŠ¡ä¸­å–å¾—äº†æˆåŠŸ(McAllister et al., 2019)ï¼Œä½†å®ƒåœ¨ä»»æ„åˆ†å¸ƒå˜åŒ–ä¸‹æ— æ³•ä¿è¯ã€‚ ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨çº¿å­¦ä¹ æ–¹æ³• (Cesa-Bianchi & Lugosi, 2006; Ross et al., 2011; Zhang & Cho, 2016; Cronrath et al., 2018) å…·æœ‰æ— æ‚”ä¿è¯ï¼Œå¹¶ä¸”æä¾›é¢‘ç¹çš„ä¸“å®¶ç›‘ç£ï¼Œå®ƒä»¬æ¸è¿‘è¦†ç›– æ•´ä¸ªæ•°æ®åˆ†å¸ƒçš„æ”¯æŒï¼Œé€‚åº”ä»»ä½•åˆ†å¸ƒå˜åŒ–ï¼Œå¦‚å›¾6dæ‰€ç¤ºã€‚ ä¸ºäº†æŒç»­åº”å¯¹åˆ†å¸ƒè½¬å˜ï¼Œå­¦ä¹ è€…å¿…é¡»æ¥æ”¶äº¤äº’å¼åé¦ˆ(Ross et al., 2011)ï¼Œä½†æ˜¯ï¼Œåº”è¯¥å°½é‡å‡å°‘è¿™ç§ä»£ä»·é«˜æ˜‚çš„åé¦ˆçš„é¢‘ç‡ã€‚ æˆ‘ä»¬çš„è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥æ–¹æ³• Robust Imitative Planning å¯ä»¥åº”å¯¹ä¸€äº› OOD äº‹ä»¶ï¼Œä»è€Œå‡å°‘ç³»ç»Ÿå¯¹ä¸“å®¶åé¦ˆçš„ä¾èµ–ï¼Œå¹¶å¯ä»¥åˆ©ç”¨è¿™ç§ä¸ç¡®å®šæ€§æ¥å†³å®šä½•æ—¶æ— æ³•åº”å¯¹â€”â€”ä½•æ—¶ä¸“å®¶å¿…é¡»å¹²é¢„ã€‚

Algorithm 1: Adaptive Robust Imitative Planning
ç®—æ³• 1ï¼šè‡ªé€‚åº”ç¨³å¥æ¨¡ä»¿è§„åˆ’

### Current benchmarks. 
We are interested in the control problem, where AD agents get deployed in reactive environments and make sequential decisions. The CARLA Challenge (Ros et al., 2019; Dosovitskiy et al., 2017; Codevilla et al., 2019) is an open-source benchmark for control in AD. It is based on 10 traffic scenarios from the NHTSA pre-crash typology (National Highway Traffic Safety Administration, 2007) to inject challenging driving situations into traffic patterns encountered by AD agents. The methods are only assessed in terms of their generalization to weather conditions, the initial state of the simulation (e.g., the start and goal locations, and the random seed of other agents.) and the traffic density (i.e., empty town, regular traffic and dense traffic).

å½“å‰åŸºå‡†ã€‚ æˆ‘ä»¬å¯¹æ§åˆ¶é—®é¢˜æ„Ÿå…´è¶£ï¼Œå…¶ä¸­ AD æ™ºèƒ½ä½“éƒ¨ç½²åœ¨ååº”æ€§ç¯å¢ƒä¸­å¹¶åšå‡ºé¡ºåºå†³ç­–ã€‚ CARLA æŒ‘æˆ˜èµ›(Ros et al., 2019 ;Dosovitskiy et al., 2017 ;Codevilla et al., 2019)æ˜¯ AD æ§åˆ¶çš„å¼€æºåŸºå‡†ã€‚ å®ƒåŸºäº NHTSA é¢„ç¢°æ’ç±»å‹å­¦(å›½å®¶å…¬è·¯äº¤é€šå®‰å…¨ç®¡ç†å±€ï¼Œ2007)çš„ 10 ç§äº¤é€šåœºæ™¯ï¼Œå°†å…·æœ‰æŒ‘æˆ˜æ€§çš„é©¾é©¶æƒ…å†µæ³¨å…¥ AD æ™ºèƒ½ä½“é‡åˆ°çš„äº¤é€šæ¨¡å¼ä¸­ã€‚ è¿™äº›æ–¹æ³•ä»…æ ¹æ®å®ƒä»¬å¯¹å¤©æ°”æ¡ä»¶çš„æ³›åŒ–ã€æ¨¡æ‹Ÿçš„åˆå§‹çŠ¶æ€(ä¾‹å¦‚ï¼Œå¼€å§‹å’Œç›®æ ‡ä½ç½®ï¼Œä»¥åŠå…¶ä»–æ™ºèƒ½ä½“çš„éšæœºç§å­)å’Œäº¤é€šå¯†åº¦(å³ç©ºåŸã€æ­£å¸¸äº¤é€š)è¿›è¡Œè¯„ä¼° å’Œå¯†é›†çš„äº¤é€š)ã€‚

Despite these challenging scenarios selected in the CARLA Challenge, the agents are allowed to train on the same scenarios in which they evaluated, and so the robustness to distributional shift is not assessed. Consequently, both Chen et al. (2019) and Rhinehart et al. (2020) manage to solve the CARLA Challenge with almost 100% success rate, when trained in Town01 and tested in Town02. However, both methods score almost 0% when evaluated in Roundabouts due to the presence of OOD road morphologies, as discussed in Section 4.2.3.

å°½ç®¡åœ¨ CARLA æŒ‘æˆ˜èµ›ä¸­é€‰æ‹©äº†è¿™äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ï¼Œä½†å…è®¸æ™ºèƒ½ä½“åœ¨ä»–ä»¬è¯„ä¼°çš„ç›¸åŒåœºæ™¯ä¸­è¿›è¡Œè®­ç»ƒï¼Œå› æ­¤ä¸è¯„ä¼°å¯¹åˆ†å¸ƒè½¬å˜çš„ç¨³å¥æ€§ã€‚ å› æ­¤ï¼Œé™ˆet al. (2019) å’Œ Rhinehart et al. (2020) åœ¨ Town01 ä¸­è®­ç»ƒå¹¶åœ¨ Town02 ä¸­æµ‹è¯•æ—¶ï¼Œä»¥å‡ ä¹ 100% çš„æˆåŠŸç‡æˆåŠŸè§£å†³äº† CARLA æŒ‘æˆ˜ã€‚ ç„¶è€Œï¼Œç”±äºå­˜åœ¨ OOD é“è·¯å½¢æ€ï¼Œè¿™ä¸¤ç§æ–¹æ³•åœ¨ Roundabouts ä¸­è¿›è¡Œè¯„ä¼°æ—¶å¾—åˆ†å‡ ä¹ä¸º 0%ï¼Œå¦‚ç¬¬ 4.2.3 èŠ‚æ‰€è¿°ã€‚

## 8. Summary and Conclusions
To summarise, in this paper, we studied autonomous driving agents in out-of-training distribution tasks (i.e. under distribution shifts). We introduced an epistemic uncertaintyaware planning method, called robust imitative planning (RIP), which can detect and recover from distribution shifts, as shown experimentally in a real prediction task, nuScenes, and a driving simulator, CARLA. We presented an adaptive variant (AdaRIP) which uses RIPâ€™s epistemic uncertainty estimates to efficiently query the expert for online feedback and adapt its model parameters online.

æ€»è€Œè¨€ä¹‹ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è®­ç»ƒå¤–åˆ†é…ä»»åŠ¡(å³åˆ†é…è½¬ç§»)ä¸­çš„è‡ªåŠ¨é©¾é©¶æ™ºèƒ½ä½“ã€‚ æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§è®¤çŸ¥ä¸ç¡®å®šæ€§æ„ŸçŸ¥è§„åˆ’æ–¹æ³•ï¼Œç§°ä¸ºç¨³å¥æ¨¡ä»¿è§„åˆ’ (RIP)ï¼Œå®ƒå¯ä»¥æ£€æµ‹åˆ†å¸ƒå˜åŒ–å¹¶ä»ä¸­æ¢å¤ï¼Œå¦‚çœŸå®é¢„æµ‹ä»»åŠ¡ nuScenes å’Œé©¾é©¶æ¨¡æ‹Ÿå™¨ CARLA ä¸­çš„å®éªŒæ‰€ç¤ºã€‚ æˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªé€‚åº”å˜ä½“ (AdaRIP)ï¼Œå®ƒä½¿ç”¨ RIP çš„è®¤çŸ¥ä¸ç¡®å®šæ€§ä¼°è®¡æ¥æœ‰æ•ˆåœ°æŸ¥è¯¢ä¸“å®¶çš„åœ¨çº¿åé¦ˆå¹¶åœ¨çº¿è°ƒæ•´å…¶æ¨¡å‹å‚æ•°ã€‚

We also introduced and open-sourced an autonomous car novel-scene benchmark, termed CARNOVEL, to assess the robustness of driving agents to a suite of OOD tasks.

æˆ‘ä»¬è¿˜å¼•å…¥å¹¶å¼€æºäº†ä¸€ä¸ªåä¸º CARNOVEL çš„è‡ªåŠ¨é©¾é©¶æ±½è½¦æ–°åœºæ™¯åŸºå‡†ï¼Œä»¥è¯„ä¼°é©¾é©¶æ™ºèƒ½ä½“å¯¹ä¸€ç»„ OOD ä»»åŠ¡çš„ç¨³å¥æ€§ã€‚

## Acknowledgements
This work was supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems (grant reference EP/L015897/1). This project has received funding from the Office of Naval Research, the DARPA Assured Autonomy Program, and ARL DCIST CRA W911NF-17-2- 0181, Microsoft Azure and Intel AI Labs.

è¿™é¡¹å·¥ä½œå¾—åˆ°äº†è‹±å›½ EPSRC CDT åœ¨è‡ªä¸»æ™ºèƒ½æœºå™¨å’Œç³»ç»Ÿæ–¹é¢çš„æ”¯æŒ(æˆæƒå‚è€ƒ EP/L015897/1)ã€‚ è¯¥é¡¹ç›®å·²è·å¾—æµ·å†›ç ”ç©¶åŠå…¬å®¤ã€DARPA ä¿è¯è‡ªæ²»è®¡åˆ’å’Œ ARL DCIST CRA W911NF-17-2-0181ã€Microsoft Azure å’Œè‹±ç‰¹å°”äººå·¥æ™ºèƒ½å®éªŒå®¤çš„èµ„åŠ©ã€‚

## References
* Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M.,McGrew, B., Petron, A., Paino, A., Plappert, M., Powell,G., Ribas, R., et al. Solving Rubikâ€™s cube with a robothand. arXiv preprint arXiv:1910.07113, 2019.
* Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulï¿¾man, J., and ManÂ´e, D. Concrete problems in AI safety.
* arXiv preprint arXiv:1606.06565, 2016.
* Barber, D. Bayesian reasoning and machine learning. Camï¿¾bridge University Press, 2012.
* Bellman, R. E. Adaptive control processes: a guided tour.
* Princeton university press, 2015.
* Bishop, C. M. Mixture density networks. 1994.
* Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,D. Weight uncertainty in neural networks. arXiv preprintarXiv:1505.05424, 2015.
* Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E.,Xu, Q., Krishnan, A., Pan, Y., Baldan, G., and Beijbom, O.
* nuscenes: A multimodal dataset for autonomous driving.
* arXiv preprint arXiv:1903.11027, 2019.
* Castro, P. S. and Precup, D. Using bisimulation for polï¿¾icy transfer in MDPs. In AAAI Conference on ArtificialIntelligence, 2010.
* Cesa-Bianchi, N. and Lugosi, G. Prediction, learning, andgames. Cambridge University Press, 2006.
* Chai, Y., Sapp, B., Bansal, M., and Anguelov, D. Multipath:Multiple probabilistic anchor trajectory hypotheses forbehavior prediction. arXiv preprint arXiv:1910.05449,2019.
* Chen, D., Zhou, B., Koltun, V., and KrÂ¨ahenbÂ¨uhl, P. Learningby cheating. arXiv preprint arXiv:1912.12294, 2019.
* Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg,S., and Amodei, D. Deep reinforcement learning fromhuman preferences. In Advances in Neural InformationProcessing Systems, pp. 4299â€“4307, 2017.
* Chua, K., Calandra, R., McAllister, R., and Levine, S. Deepreinforcement learning in a handful of trials using probaï¿¾bilistic dynamics models. In Neural Information Processï¿¾ing Systems (NeurIPS), pp. 4754â€“4765, 2018.
* Codevilla, F., Miiller, M., LÂ´opez, A., Koltun, V., and Dosoï¿¾vitskiy, A. End-to-end driving via conditional imitationlearning. In International Conference on Robotics andAutomation (ICRA), pp. 1â€“9. IEEE, 2018.
* Codevilla, F., Santana, E., LÂ´opez, A. M., and Gaidon, A.
* Exploring the limitations of behavior cloning for auï¿¾tonomous driving. In International Conference on Comï¿¾puter Vision (ICCV), pp. 9329â€“9338, 2019.
* Coley, G., Wesley, A., Reed, N., and Parry, I. Driver reactiontimes to familiar, but unexpected events. TRL PublishedProject Report, 2009.
* Cronrath, C., Jorge, E., Moberg, J., Jirstrand, M.,and Lennartson, B. BAgger: A Bayesian alï¿¾gorithm for safe and query-efficient imitationlearning. https://personalrobotics.cs.
* washington.edu/workshops/mlmp2018/assets/docs/24_CameraReadySubmission_180928_BAgger.pdf, 2018.
* Cui, H., Radosavljevic, V., Chou, F.-C., Lin, T.-H., Nguyen,T., Huang, T.-K., Schneider, J., and Djuric, N. Mulï¿¾timodal trajectory predictions for autonomous drivingusing deep convolutional networks. In 2019 Internaï¿¾tional Conference on Robotics and Automation (ICRA),pp. 2090â€“2096. IEEE, 2019.
* de Haan, P., Jayaraman, D., and Levine, S. Causal confusionin imitation learning. In Neural Information ProcessingSystems (NeurIPS), pp. 11693â€“11704, 2019.
* Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., andKoltun, V. CARLA: An open urban driving simulator.
* arXiv preprint arXiv:1711.03938, 2017.
* Du, Y., Lin, T., and Mordatch, I. Model based planning withenergy based models. arXiv preprint arXiv:1909.06878,2019.
* Embrechts, P., KlÂ¨uppelberg, C., and Mikosch, T. Modellingextremal events: for insurance and finance, volume 33.
* Springer Science & Business Media, 2013.
* Finn, C., Abbeel, P., and Levine, S. Model-agnostic metaï¿¾learning for fast adaptation of deep networks. In Interï¿¾national Conference on Machine Learning (ICML), pp.
* 1126â€“1135, 2017.
* French, R. M. Catastrophic forgetting in connectionist netï¿¾works. Trends in cognitive sciences, 3(4):128â€“135, 1999.
* Gal, Y. and Ghahramani, Z. Dropout as a Bayesian approxï¿¾imation: Representing model uncertainty in deep learnï¿¾ing. In International Conference on Machine Learning(ICML), pp. 1050â€“1059, 2016.
* Graves, A. Practical variational inference for neuralnetworks. In Neural Information Processing Systems(NeurIPS), pp. 2348â€“2356, 2011.
* HernÂ´andez-Lobato, J. M. and Adams, R. Probabilistic backï¿¾propagation for scalable learning of Bayesian neural netï¿¾works. In International Conference on Machine Learning(ICML), pp. 1861â€“1869, 2015.
* Kahn, G., Villaflor, A., Pong, V., Abbeel, P., and Levine, S.
* Uncertainty-aware reinforcement learning for collisionavoidance. arXiv preprint arXiv:1702.01182, 2017.
* Kenton, Z., Filos, A., Evans, O., and Gal, Y. Generalizingfrom a few environments in safety-critical reinforcementlearning. arXiv preprint arXiv:1907.01475, 2019.
* Kesten, R., Usman, M., Houston, J., Pandya, T., Nadhamuni,K., Ferreira, A., Yuan, M., Low, B., Jain, A., Ondruska,P., Omari, S., Shah, S., Kulkarni, A., Kazakova, A., Tao,C., Platinsky, L., Jiang, W., and Shet, V. Lyft level 5 avdataset 2019, 2019. URL https://level5.lyft.
* com/dataset/.
* Kingma, D. P. and Ba, J. Adam: A method for stochasticoptimization. arXiv preprint arXiv:1412.6980, 2014.
* Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simpleand scalable predictive uncertainty estimation using deepensembles. In Neural Information Processing Systems(NeurIPS), pp. 6402â€“6413, 2017.
* Leike, J., Martic, M., Krakovna, V., Ortega, P. A., Everitt,T., Lefrancq, A., Orseau, L., and Legg, S. AI safetygridworlds. arXiv preprint arXiv:1711.09883, 2017.
* Li, Z., Motoyoshi, T., Sasaki, K., Ogata, T., and Sugano, S.
* Rethinking self-driving: Multi-task knowledge for bettergeneralization and accident explanation ability. arXivpreprint arXiv:1809.11100, 2018.
* Liang, X., Wang, T., Yang, L., and Xing, E. Cirl: Controlï¿¾lable imitative reinforcement learning for vision-basedself-driving. In European Conference on Computer Vision(ECCV), pp. 584â€“599, 2018.
* Liu, C. and Atkeson, C. G. Standing balance control usinga trajectory library. In 2009 IEEE/RSJ International Conï¿¾ference on Intelligent Robots and Systems, pp. 3031â€“3036.
* IEEE, 2009.
* McAllister, R., Kahn, G., Clune, J., and Levine, S. Robustï¿¾ness to out-of-distribution inputs via task-aware generaï¿¾tive uncertainty. In International Conference on Roboticsand Automation (ICRA), pp. 2083â€“2089. IEEE, 2019.
* Michelmore, R., Kwiatkowska, M., and Gal, Y. Evaluatï¿¾ing uncertainty quantification in end-to-end autonomousdriving control. arXiv preprint arXiv:1811.06817, 2018.
* National Highway Traffic Safety Administration. Pre-crashscenario typology for crash avoidance research, 2007.
* URL https://www.nhtsa.gov/sites/nhtsa.
* dot.gov/files/pre-crash_scenario_typology-final_pdf_version_5-2-07.
* pdf.
* Neal, R. M. Bayesian learning for neural networks, volume118. Springer Science & Business Media, 2012.
* OpenAI, M. A., Baker, B., Chociej, M., JÂ´ozefowicz, R., Mcï¿¾Grew, B., Pachocki, J., Petron, A., Plappert, M., Powell,G., Ray, A., et al. Learning dexterous in-hand manipulaï¿¾tion. arXiv preprint arXiv:1808.00177, 2018.
* Phan-Minh, T., Grigore, E. C., Boulton, F. A., Beijbom,O., and Wolff, E. M. Covernet: Multimodal behavï¿¾ior prediction using trajectory sets. arXiv preprintarXiv:1911.10298, 2019.
* Pomerleau, D. A. Alvinn: An autonomous land vehiclein a neural network. In Neural Information ProcessingSystems (NeurIPS), pp. 305â€“313, 1989.
* Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., andLawrence, N. D. Dataset shift in machine learning. MITPress, 2009.
* Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine,S. Epopt: Learning robust neural network policies usï¿¾ing model ensembles. arXiv preprint arXiv:1610.01283,2016.
* Rezende, D. J. and Mohamed, S. Variational inference withnormalizing flows. arXiv preprint arXiv:1505.05770,2015.
* Rhinehart, N., Kitani, K. M., and Vernaza, P. R2P2: Areparameterized pushforward policy for diverse, precisegenerative path forecasting. In European Conference onComputer Vision (ECCV), pp. 772â€“788, 2018.
* Rhinehart, N., McAllister, R., Kitani, K., and Levine, S.
* PRECOG: Prediction conditioned on goals in visualmulti-agent settings. International Conference on Comï¿¾puter Vision, 2019.
* Rhinehart, N., McAllister, R., and Levine, S. Deep imitativemodels for flexible inference, planning, and control. InInternational Conference on Learning Representations(ICLR), April 2020.
* Ros, G., Koltun, V., Codevilla, F., and Lopez, M. A. CARLAchallenge, 2019. URL https://carlachallenge.
* org.
* Ross, S., Gordon, G., and Bagnell, D. A reduction ofimitation learning and structured prediction to no-regretonline learning. In Artificial Intelligence and Statistics(AISTATS), pp. 627â€“635, 2011.
* Sadeghi, F. and Levine, S. Cad2rl: Real single-imageflight without a single real image. arXiv preprintarXiv:1611.04201, 2016.
* Sauer, A., Savinov, N., and Geiger, A. Conditional afforï¿¾dance learning for driving in urban environments. arXivpreprint arXiv:1806.06498, 2018.
* Snoek, J., Ovadia, Y., Fertig, E., Lakshminarayanan, B.,Nowozin, S., Sculley, D., Dillon, J., Ren, J., and Nado,Z. Can you trust your modelâ€™s uncertainty? evaluatingpredictive uncertainty under dataset shift. In Neural Inforï¿¾mation Processing Systems (NeurIPS), pp. 13969â€“13980,2019.
* Sugiyama, M. and Kawanabe, M. Machine learning in nonï¿¾stationary environments: Introduction to covariate shiftadaptation. MIT press, 2012.
* Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patï¿¾naik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine,B., et al. Scalability in perception for autonomousdriving: An open dataset benchmark. arXiv preprintarXiv:1912.04838, 2019.
* Tang, Y. C., Zhang, J., and Salakhutdinov, R. Worst casespolicy gradients. arXiv preprint arXiv:1911.03618, 2019.
* Taoka, G. T. Brake reaction times of unalerted drivers. ITEjournal, 59(3):19â€“21, 1989.
* Uria, B., CË†otÂ´e, M.-A., Gregor, K., Murray, I., andLarochelle, H. Neural autoregressive distribution estiï¿¾mation. The Journal of Machine Learning Research, 17(1):7184â€“7220, 2016.
* Wald, A. Contributions to the theory of statistical estimationand testing hypotheses. The Annals of MathematicalStatistics, 10(4):299â€“326, 1939.
* Widrow, B. and Smith, F. W. Pattern-recognizing controlsystems, 1964.
* Zhang, J. and Cho, K. Query-efficient imitation learnï¿¾ing for end-to-end autonomous driving. arXiv preprintarXiv:1605.06450, 2016.
* Zhou, A., Jang, E., Kappler, D., Herzog, A., Khansari, M.,Wohlhart, P., Bai, Y., Kalakrishnan, M., Levine, S., andFinn, C. Watch, try, learn: Meta-learning from demonï¿¾strations and reward. arXiv preprint arXiv:1906.03352,2019.


## Appendix
### A. CARNOVEL: Suite of Tasks Under Distribution Shift
 (a) AbnormalTurns0-v0 (b) AbnormalTurns1-v0 (c) AbnormalTurns2-v0 (d) AbnormalTurns3-v0 (e) AbnormalTurns4-v0 (f) AbnormalTurns5-v0 (g) AbnormalTurns6-v0 (h) BusyTown0-v0 (i) BusyTown1-v0 (j) BusyTown2-v0 (k) BusyTown3-v0 (l) BusyTown4-v0 (m) BusyTown5-v0 (n) BusyTown6-v0 (o) BusyTown7-v0  (p) BusyTown8-v0 (q) BusyTown9-v0 (r) BusyTown10-v0 (s) Hills0-v0 (t) Hills1-v0 (u) Hills2-v0 (v) Hills3-v0 (w) Roundabouts0-v0 (x) Roundabouts1-v0 (y) Roundabouts2-v0 (z) Roundabouts3-v0 (aa) Roundabouts4-v0 

### B. Experimental Results on CARNOVEL
Table 4. We evaluate different autonomous driving methods in terms of their robustness to distribution shifts, in our new benchmark,

CARNOVEL. All methods are trained on CARLA Town01 using imitation learning on expert demonstrations from the autopilot (Dosovitskiy et al., 2017). A â€œâ€ â€ indicates methods that use first-person camera view, as in (Chen et al., 2019), a â€œâ™£â€ methods that use LIDAR
 observation, as in (Rhinehart et al., 2020) and a â€œâ™¦â€ methods that use the ground truth game engine state, as in (Chen et al., 2019). A â€œF â€ indicates that we used the reference implementation from the original paper, otherwise we used our implementation. For all the scenes we chose pairs of start-destination locations and ran 10 trials with randomized initial simulator state for each pair. Standard errors are in gray (via bootstrap sampling). The outperforming method is in bold.

AbnormalTurns BusyTown

Success â†‘ Infra/km â†“ Distance â†‘ Success â†‘ Infra/km â†“ Distance â†‘

Methods (7 Ã— 10 scenes, %) (Ã—1eâˆ’3) (m) (11 Ã— 10 scenes, %) (Ã—1eâˆ’3) (m)

CILâ™£F (Codevilla et al., 2018) 65.71Â±07.37 07.04Â±05.07 128Â±020 05.45Â±06.35 11.49Â±03.66 217Â±033

LbCâ€ F (Chen et al., 2019) 00.00Â±00.00 05.81Â±00.58 208Â±004 20.00Â±13.48 03.96Â±00.15 374Â±016

LbC-GTâ™¦F (Chen et al., 2019) 02.86Â±06.39 03.68Â±00.34 217Â±033 65.45Â±07.60 02.59Â±00.02 400Â±006

DIMâ™£ (Rhinehart et al., 2020) 74.28Â±11.26 05.56Â±04.06 108Â±017 47.13Â±14.54 08.47Â±05.22 175Â±026

RIP-BCMâ™£ (baseline, cf. Table 1) 68.57Â±09.03 07.93Â±03.73 096Â±017 50.90Â±20.64 03.74Â±05.52 175Â±031

RIP-MAâ™£ (ours, cf. Section 3.3.2) 84.28Â±14.20 07.86Â±05.70 102Â±015 64.54Â±23.25 05.86Â±03.99 170Â±033

RIP-WCMâ™£ (ours, cf. Section 3.3.1) 87.14Â±14.20 04.91Â±03.60 102Â±021 62.72Â±05.16 03.17Â±02.04 167Â±021

Hills Roundabouts

Success â†‘ Infra/km â†“ Distance â†‘ Success â†‘ Infra/km â†“ Distance â†‘

Methods (4 Ã— 10 scenes, %) (Ã—1eâˆ’3) (m) (5 Ã— 10 scenes, %) (Ã—1eâˆ’3) (m)

CILâ™£F (Codevilla et al., 2018) 60.00Â±29.34 04.74Â±03.02 219Â±034 20.00Â±00.00 03.60Â±03.23 269Â±021

LbCâ€ F (Chen et al., 2019) 50.00Â±00.00 01.61Â±00.15 541Â±101 08.00Â±10.95 03.70Â±00.72 323Â±043

LbC-GTâ™¦F (Chen et al., 2019) 05.00Â±11.18 03.36Â±00.26 312Â±020 00.00Â±00.00 06.47Â±00.99 123Â±018

DIMâ™£ (Rhinehart et al., 2020) 70.00Â±10.54 06.87Â±04.09 195Â±012 20.00Â±09.42 06.19Â±04.73 240Â±044

RIP-BCMâ™£ (baseline, cf. Table 1) 75.00Â±00.00 05.49Â±04.03 191Â±013 06.00Â±09.66 06.78Â±07.05 251Â±027

RIP-MAâ™£ (ours, cf. Section 3.3.2) 97.50Â±07.90 00.26Â±00.54 196Â±013 38.00Â±06.32 05.48Â±05.56 271Â±047

RIP-WCMâ™£ (ours, cf. Section 3.3.1) 87.50Â±13.17 01.83Â±01.73 191Â±006 42.00Â±06.32 04.32Â±01.91 217Â±030 

### C. AdaRIP Examples
 (Normalized) Uncertainty (a) RIP (b) AdaRIP

Figure 8. Examples where the non-adaptive method (a) fails to recover from a distribution shift, despite it being able to detect it. The adaptive method (b) queries the human driver when uncertain (dark red), then uses the online demonstrations for updating its model, resulting into confident (light red, white) and safe trajectories. 

### D. Online Planning with a Trajectory Library
In the absence of scalable global optimizers, we search the trajectory space in Eqn. (4) by restricting the search space to a trajectory library (Liu & Atkeson, 2009), TY, a finite set of fixed trajectories. In this work, we perform K-means clustering of the expert planâ€™s from the training distribution and keep 64 of the centroids, as illustrated in Figure 9. Therefore we efficiently solve a search problem over a discrete space rather than an optimization problem of continuous variables. The modified objective is: yG

RIP â‰ˆ arg max yâˆˆTY âŠ• Î¸âˆˆsupp p(Î¸|D) log p(y|G, x; Î¸) (10)

Solving for Eqn. (10) results in Ã—20 improvement in runtime compared to the gradient descent alternative. Although in in-distribution scenes solving Eqn. (10) over Eqn. (4) does not deteriorate perfomance, in out-of-distribution scenes the trajectory library, TY, is not useful. Therefore in the experiments (c.f. Section 4.2.3) we used online gradient-descent.

Future work lies in developing a hybrid optimization method that takes advantage of the speedup the trajectory library provides without a decrease in performance in out-of-distribution scenarios. (a) Trajectories (b) K = 64 (c) K = 128 (d) K = 1024

Figure 9. Our trajectory library from CARLAâ€™s autopilot demonstrations, 4 seconds.   
